{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading data from Twitter\n",
    "\n",
    "\n",
    "tweepy\n",
    "\n",
    "Tweepy is a library that will help you to connect to Twitter API.\n",
    "\n",
    "### Creating a Twitter app\n",
    "\n",
    "Before anything, you must have a developer access to Twitter and create an app that will be used to connect to Twitter API. Doing this used to be simpler, but now, Twitter is reviewing each developer access request. You have to submit a request here: https://developer.twitter.com/.\n",
    "After you have been accepted, it’s time to create an app. You can consult Twitter documentation to find how you can do it. Once you have finished, take note of your credentials (and don’t forget to protect them). You will need:\n",
    "•The API key\n",
    "•The API secret key\n",
    "•The access token\n",
    "•The access token secret\n",
    "\n",
    "All of this can be found on your app details, under the “Keys and tokens” option.\n",
    "\n",
    "\n",
    "\n",
    "https://medium.com/@wilamelima/mining-twitter-for-sentiment-analysis-using-python-a74679b85546\n",
    "\n",
    "http://docs.tweepy.org/en/3.7.0/api.html\n",
    "\n",
    "\n",
    "### Installing tweepy\n",
    "\n",
    "pip install tweepy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tweepy\n",
    "import jsonpickle\n",
    "\n",
    "# Consume:\n",
    "CONSUMER_KEY    = 'USEYOUR'\n",
    "CONSUMER_SECRET = 'USEYOUR'\n",
    "\n",
    "# Access:\n",
    "ACCESS_TOKEN  = 'USEYOUR'\n",
    "ACCESS_SECRET = 'USEYOUR'\n",
    "\n",
    "# Setup access API\n",
    "def connect_to_twitter_OAuth():\n",
    "    auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "    auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET)\n",
    "    \n",
    "    api = tweepy.API(auth)\n",
    "    return api\n",
    " \n",
    "# Creates API object\n",
    "api = connect_to_twitter_OAuth()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scrape by ashtag\n",
    "\n",
    "So, let’s start by building a function that will:\n",
    "\n",
    "Create a json file that will hold all the tweets\n",
    "Access Twitter API, query it and return the tweets\n",
    "Save the tweets into the file we just created\n",
    "\n",
    "The function will accept as parameters:\n",
    "\n",
    "filepath: where the file should be saved and it’s name\n",
    "\n",
    "api: the api object we created earlier\n",
    "\n",
    "query: the query that will be used by Twitter to retrieve the tweets\n",
    "\n",
    "max_tweets: your developer account has a limit of how many requests you can do each 15 minutes.\n",
    "\n",
    "The function return the number of rweets downloaded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_save_tweets(filepath, api, query='', id='', max_tweets=1000, lang='en', mode ='w'):\n",
    "\n",
    "    tweetCount = 0\n",
    "\n",
    "    #Open file and save tweets\n",
    "    with open(filepath, mode) as f:\n",
    "        \n",
    "        # if id passed search id's timeline\n",
    "        if id:\n",
    "             try:    \n",
    "                for tweet in tweepy.Cursor(api.user_timeline,id=id,lang=lang).items(max_tweets):\n",
    "                    #Convert to JSON format\n",
    "                    f.write(jsonpickle.encode(tweet._json, unpicklable=False) + '\\n')\n",
    "                    tweetCount += 1\n",
    "                    \n",
    "             except: return(0)   \n",
    "            \n",
    "        # else try query\n",
    "        else:\n",
    "             try:                \n",
    "                for tweet in tweepy.Cursor(api.search,q=query,lang=lang).items(max_tweets): \n",
    "                    #Convert to JSON format\n",
    "                    f.write(jsonpickle.encode(tweet._json, unpicklable=False) + '\\n')\n",
    "                    tweetCount += 1\n",
    "                    \n",
    "             except: return(0)                      \n",
    "        \n",
    "    f.close()    \n",
    "    return (tweetCount) #Display how many tweets we have collected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We download 1000 tweets for astag #innovation \n",
    "\n",
    "They will be taken among the most recents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '#innovation'  \n",
    "filename = 'tweets.json'\n",
    "\n",
    "# Get those tweets\n",
    "nrec=get_save_tweets(filename, api, query)\n",
    "\n",
    "print(\"Downloaded {0} tweets\".format(nrec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other case: we want to get 10 tweets from a list of companies we previously prepared:\n",
    "\n",
    "the file  twitter_co.csv contains data for 85 companies size 1001-10000 employees and twitter url\n",
    "\n",
    "twitter id can be extracted by stripping the leading https://www.twitter.com/ from url.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "filename = 'comptweets.json'\n",
    "\n",
    "comptwitter = pd.read_csv('twitter_co.csv')\n",
    "\n",
    "twitter_urls = comptwitter['twitter_url']\n",
    "\n",
    "for twitter_url in twitter_urls:\n",
    "    twitter_id= twitter_url.replace(\"https://www.twitter.com/\",\"\")\n",
    "# Get those tweets\n",
    "    nrec = get_save_tweets(filename, api, id = twitter_id,max_tweets=10, mode = 'a') \n",
    "    print(\"Downloaded {0} tweets for account {1}\".format(nrec, twitter_id))                       \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reading and cleaning tweets\n",
    "\n",
    "Last step we create a function that makes some basic cleanings\n",
    "\n",
    "and loads into a dataframe the tweets we saved previously\n",
    "\n",
    "tweets_to_df()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweets_to_df(path):\n",
    "    \n",
    "    tweets = list(open(path, 'rt'))\n",
    "    \n",
    "    text = []\n",
    "    weekday = []\n",
    "    month = []\n",
    "    day = []\n",
    "    hour = []\n",
    "    hashtag = []\n",
    "    url = []\n",
    "    favorite = []\n",
    "    reply = []\n",
    "    retweet = []\n",
    "    follower = []\n",
    "    following = []\n",
    "    user = []\n",
    "    screen_name = []\n",
    "\n",
    "    for t in tweets:\n",
    "        t = jsonpickle.decode(t)\n",
    "        \n",
    "        # Text\n",
    "        text.append(t['text'])\n",
    "        \n",
    "        # Decompose date\n",
    "        date = t['created_at']\n",
    "        weekday.append(date.split(' ')[0])\n",
    "        month.append(date.split(' ')[1])\n",
    "        day.append(date.split(' ')[2])\n",
    "        \n",
    "        time = date.split(' ')[3].split(':')\n",
    "        hour.append(time[0]) \n",
    "        \n",
    "        # Has hashtag\n",
    "        if len(t['entities']['hashtags']) == 0:\n",
    "            hashtag.append(0)\n",
    "        else:\n",
    "            hashtag.append(1)\n",
    "            \n",
    "        # Has url\n",
    "        if len(t['entities']['urls']) == 0:\n",
    "            url.append(0)\n",
    "        else:\n",
    "            url.append(1)\n",
    "            \n",
    "        # Number of favs\n",
    "        favorite.append(t['favorite_count'])\n",
    "        \n",
    "        # Is reply?\n",
    "        if t['in_reply_to_status_id'] == None:\n",
    "            reply.append(0)\n",
    "        else:\n",
    "            reply.append(1)       \n",
    "        \n",
    "        # Retweets count\n",
    "        retweet.append(t['retweet_count'])\n",
    "        \n",
    "        # Followers number\n",
    "        follower.append(t['user']['followers_count'])\n",
    "        \n",
    "        # Following number\n",
    "        following.append(t['user']['friends_count'])\n",
    "        \n",
    "        # Add user\n",
    "        user.append(t['user']['name'])\n",
    "\n",
    "        # Add screen name\n",
    "        screen_name.append(t['user']['screen_name'])\n",
    "        \n",
    "    d = {'text': text,\n",
    "         'date':date,\n",
    "         'weekday': weekday,\n",
    "         'month' : month,\n",
    "         'day': day,\n",
    "         'hour' : hour,\n",
    "         'has_hashtag': hashtag,\n",
    "         'has_url': url,\n",
    "         'fav_count': favorite,\n",
    "         'is_reply': reply,\n",
    "         'retweet_count': retweet,\n",
    "         'followers': follower,\n",
    "         'following' : following,\n",
    "         'user': user,\n",
    "         'screen_name' : screen_name\n",
    "        }\n",
    "    \n",
    "    return pd.DataFrame(data = d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets_df = tweets_to_df('comptweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
