{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## downloading data from twitter\n",
    "\n",
    "\n",
    "tweepy\n",
    "\n",
    "Tweepy is a library that will help you to connect to Twitter API.\n",
    "\n",
    "https://medium.com/@wilamelima/mining-twitter-for-sentiment-analysis-using-python-a74679b85546\n",
    "\n",
    "http://docs.tweepy.org/en/3.7.0/api.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tweepy\n",
    "import jsonpickle\n",
    "\n",
    "# Consume:\n",
    "CONSUMER_KEY    = 'YR3k94YxVmGDwJx9DdnTWMYOC'\n",
    "CONSUMER_SECRET = 'Wv1DCH9WaXLBwKgM8mpKU1DftQKQYcKc4MPAAbNJiHEUfDc5c1'\n",
    "\n",
    "# Access:\n",
    "ACCESS_TOKEN  = '1113785099248635904-2pY06cIPChWqdlIcqXvwlWMbzkjQQD'\n",
    "ACCESS_SECRET = 'IhjXZNehvjoDLDIsHrKVPA5XaL507cceQVcUzfQOa6e5l'\n",
    "\n",
    "# Setup access API\n",
    "def connect_to_twitter_OAuth():\n",
    "    auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "    auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET)\n",
    "    \n",
    "    api = tweepy.API(auth)\n",
    "    return api\n",
    " \n",
    "# Create API object\n",
    "api = connect_to_twitter_OAuth()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scrape by ashtag\n",
    "\n",
    "So, let’s start by building a function that will:\n",
    "\n",
    "Create a json file that will hold all the tweets\n",
    "Access Twitter API, query it and return the tweets\n",
    "Save the tweets into the file we just created\n",
    "\n",
    "The function will accept as parameters:\n",
    "\n",
    "filepath: where the file should be saved and it’s name\n",
    "\n",
    "api: the api object we created earlier\n",
    "\n",
    "query: the query that will be used by Twitter to retrieve the tweets\n",
    "\n",
    "max_tweets: your developer account has a limit of how many requests you can do each 15 minutes.\n",
    "\n",
    "The function return the number of rweets downloaded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_save_tweets(filepath, api, query='', id='', max_tweets=1000, lang='en', mode ='w'):\n",
    "\n",
    "    tweetCount = 0\n",
    "\n",
    "    #Open file and save tweets\n",
    "    with open(filepath, mode) as f:\n",
    "        \n",
    "        # if id passed search id's timeline\n",
    "        if id:\n",
    "             try:    \n",
    "                for tweet in tweepy.Cursor(api.user_timeline,id=id,lang=lang).items(max_tweets):\n",
    "                    #Convert to JSON format\n",
    "                    f.write(jsonpickle.encode(tweet._json, unpicklable=False) + '\\n')\n",
    "                    tweetCount += 1\n",
    "                    \n",
    "             except: return(0)   \n",
    "            \n",
    "        # else try query\n",
    "        else:\n",
    "             try:                \n",
    "                for tweet in tweepy.Cursor(api.search,q=query,lang=lang).items(max_tweets): \n",
    "                    #Convert to JSON format\n",
    "                    f.write(jsonpickle.encode(tweet._json, unpicklable=False) + '\\n')\n",
    "                    tweetCount += 1\n",
    "                    \n",
    "             except: return(0)                      \n",
    "        \n",
    "    f.close()    \n",
    "    return (tweetCount) #Display how many tweets we have collected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We download 1000 tweets for astag #innovation \n",
    "\n",
    "They will be taken among the most recents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 563 tweets\n"
     ]
    }
   ],
   "source": [
    "query = '#innovation'  \n",
    "filename = 'tweets.json'\n",
    "\n",
    "# Get those tweets\n",
    "nrec=get_save_tweets(filename, api, query)\n",
    "\n",
    "print(\"Downloaded {0} tweets\".format(nrec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other case: we want to get 10 tweets from a list of companies we previously prepared:\n",
    "\n",
    "the file  twitter_co.csv contains data for 85 companies size 1001-10000 employees and twitter url\n",
    "\n",
    "twitter id can be extracted by stripping the leading https://www.twitter.com/ from url.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 10 tweets for account intel\n",
      "Downloaded 10 tweets for account toyota\n",
      "Downloaded 10 tweets for account cisco\n",
      "Downloaded 10 tweets for account nhk_pr\n",
      "Downloaded 10 tweets for account cleanharbors_hr\n",
      "Downloaded 10 tweets for account stericycle_inc\n",
      "Downloaded 10 tweets for account officedepot\n",
      "Downloaded 10 tweets for account mednax\n",
      "Downloaded 10 tweets for account dicks\n",
      "Downloaded 10 tweets for account ciena\n",
      "Downloaded 4 tweets for account nttdataamericas\n",
      "Downloaded 10 tweets for account gehealthcare\n",
      "Downloaded 10 tweets for account ebay\n",
      "Downloaded 10 tweets for account medtronic\n",
      "Downloaded 10 tweets for account yahoo\n",
      "Downloaded 10 tweets for account finisar\n",
      "Downloaded 10 tweets for account pitneybowes\n",
      "Downloaded 10 tweets for account bnbuzz\n",
      "Downloaded 10 tweets for account bw\n",
      "Downloaded 10 tweets for account staples\n",
      "Downloaded 10 tweets for account oracle\n",
      "Downloaded 10 tweets for account fiserv\n",
      "Downloaded 10 tweets for account cainc\n",
      "Downloaded 10 tweets for account att\n",
      "Downloaded 10 tweets for account firstdata\n",
      "Downloaded 10 tweets for account half_com\n",
      "Downloaded 0 tweets for account experianmkt\n",
      "Downloaded 10 tweets for account blackrock\n",
      "Downloaded 10 tweets for account toysrus\n",
      "Downloaded 0 tweets for account softlayer\n",
      "Downloaded 10 tweets for account generalelectric\n",
      "Downloaded 10 tweets for account opensolutionscc\n",
      "Downloaded 10 tweets for account lexisnexis\n",
      "Downloaded 10 tweets for account webex\n",
      "Downloaded 10 tweets for account magento\n",
      "Downloaded 10 tweets for account tmobile\n",
      "Downloaded 10 tweets for account dsm\n",
      "Downloaded 10 tweets for account infor\n",
      "Downloaded 10 tweets for account nookbn\n",
      "Downloaded 10 tweets for account dimensiondata\n",
      "Downloaded 10 tweets for account teconnectivity\n",
      "Downloaded 0 tweets for account dowchemical\n",
      "Downloaded 10 tweets for account broadcom\n",
      "Downloaded 10 tweets for account firstsource\n",
      "Downloaded 10 tweets for account yahoomusic\n",
      "Downloaded 10 tweets for account hp\n",
      "Downloaded 10 tweets for account google\n",
      "Downloaded 10 tweets for account centurylink\n",
      "Downloaded 10 tweets for account expedia\n",
      "Downloaded 10 tweets for account zerochaos\n",
      "Downloaded 10 tweets for account ibm\n",
      "Downloaded 0 tweets for account emcsupport\n",
      "Downloaded 10 tweets for account petsmart\n",
      "Downloaded 10 tweets for account enterprise\n",
      "Downloaded 10 tweets for account ibm\n",
      "Downloaded 10 tweets for account paypal\n",
      "Downloaded 10 tweets for account accenture\n",
      "Downloaded 10 tweets for account epinions\n",
      "Downloaded 10 tweets for account servicemaster\n",
      "Downloaded 0 tweets for account qualcommatheros\n",
      "Downloaded 10 tweets for account xuanwulab\n",
      "Downloaded 0 tweets for account apaclawson\n",
      "Downloaded 10 tweets for account skire\n",
      "Downloaded 2 tweets for account hpautonomy\n",
      "Downloaded 10 tweets for account amazon\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "filename = 'comptweets.json'\n",
    "\n",
    "comptwitter = pd.read_csv('twitter_co.csv')\n",
    "\n",
    "twitter_urls = comptwitter['twitter_url']\n",
    "\n",
    "for twitter_url in twitter_urls:\n",
    "    twitter_id= twitter_url.replace(\"https://www.twitter.com/\",\"\")\n",
    "# Get those tweets\n",
    "    nrec = get_save_tweets(filename, api, id = twitter_id,max_tweets=10, mode = 'a') \n",
    "    print(\"Downloaded {0} tweets for account {1}\".format(nrec, twitter_id))                       \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reading and cleaning tweets\n",
    "\n",
    "Last step we create a function that makes some basic cleanings\n",
    "\n",
    "and loads into a dataframe the tweets we saved previously\n",
    "\n",
    "tweets_to_df()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweets_to_df(path):\n",
    "    \n",
    "    tweets = list(open(path, 'rt'))\n",
    "    \n",
    "    text = []\n",
    "    weekday = []\n",
    "    month = []\n",
    "    day = []\n",
    "    hour = []\n",
    "    hashtag = []\n",
    "    url = []\n",
    "    favorite = []\n",
    "    reply = []\n",
    "    retweet = []\n",
    "    follower = []\n",
    "    following = []\n",
    "    user = []\n",
    "    screen_name = []\n",
    "\n",
    "    for t in tweets:\n",
    "        t = jsonpickle.decode(t)\n",
    "        \n",
    "        # Text\n",
    "        text.append(t['text'])\n",
    "        \n",
    "        # Decompose date\n",
    "        date = t['created_at']\n",
    "        weekday.append(date.split(' ')[0])\n",
    "        month.append(date.split(' ')[1])\n",
    "        day.append(date.split(' ')[2])\n",
    "        \n",
    "        time = date.split(' ')[3].split(':')\n",
    "        hour.append(time[0]) \n",
    "        \n",
    "        # Has hashtag\n",
    "        if len(t['entities']['hashtags']) == 0:\n",
    "            hashtag.append(0)\n",
    "        else:\n",
    "            hashtag.append(1)\n",
    "            \n",
    "        # Has url\n",
    "        if len(t['entities']['urls']) == 0:\n",
    "            url.append(0)\n",
    "        else:\n",
    "            url.append(1)\n",
    "            \n",
    "        # Number of favs\n",
    "        favorite.append(t['favorite_count'])\n",
    "        \n",
    "        # Is reply?\n",
    "        if t['in_reply_to_status_id'] == None:\n",
    "            reply.append(0)\n",
    "        else:\n",
    "            reply.append(1)       \n",
    "        \n",
    "        # Retweets count\n",
    "        retweet.append(t['retweet_count'])\n",
    "        \n",
    "        # Followers number\n",
    "        follower.append(t['user']['followers_count'])\n",
    "        \n",
    "        # Following number\n",
    "        following.append(t['user']['friends_count'])\n",
    "        \n",
    "        # Add user\n",
    "        user.append(t['user']['name'])\n",
    "\n",
    "        # Add screen name\n",
    "        screen_name.append(t['user']['screen_name'])\n",
    "        \n",
    "    d = {'text': text,\n",
    "         'weekday': weekday,\n",
    "         'month' : month,\n",
    "         'day': day,\n",
    "         'hour' : hour,\n",
    "         'has_hashtag': hashtag,\n",
    "         'has_url': url,\n",
    "         'fav_count': favorite,\n",
    "         'is_reply': reply,\n",
    "         'retweet_count': retweet,\n",
    "         'followers': follower,\n",
    "         'following' : following,\n",
    "         'user': user,\n",
    "         'screen_name' : screen_name\n",
    "        }\n",
    "    \n",
    "    return pd.DataFrame(data = d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = tweets_to_df('tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>fav_count</th>\n",
       "      <th>followers</th>\n",
       "      <th>following</th>\n",
       "      <th>has_hashtag</th>\n",
       "      <th>has_url</th>\n",
       "      <th>hour</th>\n",
       "      <th>is_reply</th>\n",
       "      <th>month</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>text</th>\n",
       "      <th>user</th>\n",
       "      <th>weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>05</td>\n",
       "      <td>0</td>\n",
       "      <td>476</td>\n",
       "      <td>270</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>Apr</td>\n",
       "      <td>0</td>\n",
       "      <td>DavidTimis</td>\n",
       "      <td>Young Leaders @thinkBDPST #Hungary #tech #inno...</td>\n",
       "      <td>David Timis</td>\n",
       "      <td>Fri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>05</td>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "      <td>161</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>Apr</td>\n",
       "      <td>0</td>\n",
       "      <td>pcminetti</td>\n",
       "      <td>New York’s Fashion Startup Scene Is Having a R...</td>\n",
       "      <td>pam minetti</td>\n",
       "      <td>Fri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>05</td>\n",
       "      <td>0</td>\n",
       "      <td>545</td>\n",
       "      <td>675</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>Apr</td>\n",
       "      <td>0</td>\n",
       "      <td>Arioneo_off</td>\n",
       "      <td>[RACING] Training on the track with #EQUIMETRE...</td>\n",
       "      <td>Arioneo</td>\n",
       "      <td>Fri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>05</td>\n",
       "      <td>0</td>\n",
       "      <td>422</td>\n",
       "      <td>412</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>Apr</td>\n",
       "      <td>0</td>\n",
       "      <td>SivaPrasadh_G</td>\n",
       "      <td>The 30 Technologies of the Next Decate\\n\\n#AI ...</td>\n",
       "      <td>Siva Prasadh .G</td>\n",
       "      <td>Fri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>05</td>\n",
       "      <td>0</td>\n",
       "      <td>3275</td>\n",
       "      <td>156</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>Apr</td>\n",
       "      <td>0</td>\n",
       "      <td>abunchofdata</td>\n",
       "      <td>Microsoft and BMW launch open platform to supp...</td>\n",
       "      <td>A bunch of data</td>\n",
       "      <td>Fri</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  day  fav_count  followers  following  has_hashtag  has_url hour  is_reply  \\\n",
       "0  05          0        476        270            1        0   13         0   \n",
       "1  05          0         96        161            1        1   13         0   \n",
       "2  05          0        545        675            1        1   13         0   \n",
       "3  05          0        422        412            1        1   13         0   \n",
       "4  05          0       3275        156            1        1   13         0   \n",
       "\n",
       "  month  retweet_count    screen_name  \\\n",
       "0   Apr              0     DavidTimis   \n",
       "1   Apr              0      pcminetti   \n",
       "2   Apr              0    Arioneo_off   \n",
       "3   Apr              0  SivaPrasadh_G   \n",
       "4   Apr              0   abunchofdata   \n",
       "\n",
       "                                                text             user weekday  \n",
       "0  Young Leaders @thinkBDPST #Hungary #tech #inno...      David Timis     Fri  \n",
       "1  New York’s Fashion Startup Scene Is Having a R...      pam minetti     Fri  \n",
       "2  [RACING] Training on the track with #EQUIMETRE...          Arioneo     Fri  \n",
       "3  The 30 Technologies of the Next Decate\\n\\n#AI ...  Siva Prasadh .G     Fri  \n",
       "4  Microsoft and BMW launch open platform to supp...  A bunch of data     Fri  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
