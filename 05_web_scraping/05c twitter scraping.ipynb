{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading data from Twitter\n",
    "\n",
    "\n",
    "tweepy\n",
    "\n",
    "Tweepy is a library that will help you to connect to Twitter API.\n",
    "\n",
    "https://medium.com/@wilamelima/mining-twitter-for-sentiment-analysis-using-python-a74679b85546\n",
    "\n",
    "http://docs.tweepy.org/en/3.7.0/api.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tweepy\n",
    "import jsonpickle\n",
    "\n",
    "# Consume:\n",
    "CONSUMER_KEY    = 'INSERT YOUR'\n",
    "CONSUMER_SECRET = 'INSERT YOUR'\n",
    "\n",
    "# Access:\n",
    "ACCESS_TOKEN  = 'INSERT YOUR'\n",
    "ACCESS_SECRET = 'INSERT YOUR'\n",
    "\n",
    "# Setup access API\n",
    "def connect_to_twitter_OAuth():\n",
    "    auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "    auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET)\n",
    "    \n",
    "    api = tweepy.API(auth)\n",
    "    return api\n",
    " \n",
    "# Create API object\n",
    "api = connect_to_twitter_OAuth()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scrape by ashtag\n",
    "\n",
    "So, let’s start by building a function that will:\n",
    "\n",
    "Create a json file that will hold all the tweets\n",
    "Access Twitter API, query it and return the tweets\n",
    "Save the tweets into the file we just created\n",
    "\n",
    "The function will accept as parameters:\n",
    "\n",
    "filepath: where the file should be saved and it’s name\n",
    "\n",
    "api: the api object we created earlier\n",
    "\n",
    "query: the query that will be used by Twitter to retrieve the tweets\n",
    "\n",
    "max_tweets: your developer account has a limit of how many requests you can do each 15 minutes.\n",
    "\n",
    "The function return the number of rweets downloaded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_save_tweets(filepath, api, query='', id='', max_tweets=1000, lang='en', mode ='w'):\n",
    "\n",
    "    tweetCount = 0\n",
    "\n",
    "    #Open file and save tweets\n",
    "    with open(filepath, mode) as f:\n",
    "        \n",
    "        # if id passed search id's timeline\n",
    "        if id:\n",
    "             try:    \n",
    "                for tweet in tweepy.Cursor(api.user_timeline,id=id,lang=lang).items(max_tweets):\n",
    "                    #Convert to JSON format\n",
    "                    f.write(jsonpickle.encode(tweet._json, unpicklable=False) + '\\n')\n",
    "                    tweetCount += 1\n",
    "                    \n",
    "             except: return(0)   \n",
    "            \n",
    "        # else try query\n",
    "        else:\n",
    "             try:                \n",
    "                for tweet in tweepy.Cursor(api.search,q=query,lang=lang).items(max_tweets): \n",
    "                    #Convert to JSON format\n",
    "                    f.write(jsonpickle.encode(tweet._json, unpicklable=False) + '\\n')\n",
    "                    tweetCount += 1\n",
    "                    \n",
    "             except: return(0)                      \n",
    "        \n",
    "    f.close()    \n",
    "    return (tweetCount) #Display how many tweets we have collected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We download 1000 tweets for astag #innovation \n",
    "\n",
    "They will be taken among the most recents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '#innovation'  \n",
    "filename = 'tweets.json'\n",
    "\n",
    "# Get those tweets\n",
    "nrec=get_save_tweets(filename, api, query)\n",
    "\n",
    "print(\"Downloaded {0} tweets\".format(nrec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other case: we want to get 10 tweets from a list of companies we previously prepared:\n",
    "\n",
    "the file  twitter_co.csv contains data for 85 companies size 1001-10000 employees and twitter url\n",
    "\n",
    "twitter id can be extracted by stripping the leading https://www.twitter.com/ from url.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "filename = 'comptweets.json'\n",
    "\n",
    "comptwitter = pd.read_csv('twitter_co.csv')\n",
    "\n",
    "twitter_urls = comptwitter['twitter_url']\n",
    "\n",
    "for twitter_url in twitter_urls:\n",
    "    twitter_id= twitter_url.replace(\"https://www.twitter.com/\",\"\")\n",
    "# Get those tweets\n",
    "    nrec = get_save_tweets(filename, api, id = twitter_id,max_tweets=10, mode = 'a') \n",
    "    print(\"Downloaded {0} tweets for account {1}\".format(nrec, twitter_id))                       \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reading and cleaning tweets\n",
    "\n",
    "Last step we create a function that makes some basic cleanings\n",
    "\n",
    "and loads into a dataframe the tweets we saved previously\n",
    "\n",
    "tweets_to_df()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweets_to_df(path):\n",
    "    \n",
    "    tweets = list(open(path, 'rt'))\n",
    "    \n",
    "    text = []\n",
    "    weekday = []\n",
    "    month = []\n",
    "    day = []\n",
    "    hour = []\n",
    "    hashtag = []\n",
    "    url = []\n",
    "    favorite = []\n",
    "    reply = []\n",
    "    retweet = []\n",
    "    follower = []\n",
    "    following = []\n",
    "    user = []\n",
    "    screen_name = []\n",
    "\n",
    "    for t in tweets:\n",
    "        t = jsonpickle.decode(t)\n",
    "        \n",
    "        # Text\n",
    "        text.append(t['text'])\n",
    "        \n",
    "        # Decompose date\n",
    "        date = t['created_at']\n",
    "        weekday.append(date.split(' ')[0])\n",
    "        month.append(date.split(' ')[1])\n",
    "        day.append(date.split(' ')[2])\n",
    "        \n",
    "        time = date.split(' ')[3].split(':')\n",
    "        hour.append(time[0]) \n",
    "        \n",
    "        # Has hashtag\n",
    "        if len(t['entities']['hashtags']) == 0:\n",
    "            hashtag.append(0)\n",
    "        else:\n",
    "            hashtag.append(1)\n",
    "            \n",
    "        # Has url\n",
    "        if len(t['entities']['urls']) == 0:\n",
    "            url.append(0)\n",
    "        else:\n",
    "            url.append(1)\n",
    "            \n",
    "        # Number of favs\n",
    "        favorite.append(t['favorite_count'])\n",
    "        \n",
    "        # Is reply?\n",
    "        if t['in_reply_to_status_id'] == None:\n",
    "            reply.append(0)\n",
    "        else:\n",
    "            reply.append(1)       \n",
    "        \n",
    "        # Retweets count\n",
    "        retweet.append(t['retweet_count'])\n",
    "        \n",
    "        # Followers number\n",
    "        follower.append(t['user']['followers_count'])\n",
    "        \n",
    "        # Following number\n",
    "        following.append(t['user']['friends_count'])\n",
    "        \n",
    "        # Add user\n",
    "        user.append(t['user']['name'])\n",
    "\n",
    "        # Add screen name\n",
    "        screen_name.append(t['user']['screen_name'])\n",
    "        \n",
    "    d = {'text': text,\n",
    "         'weekday': weekday,\n",
    "         'month' : month,\n",
    "         'day': day,\n",
    "         'hour' : hour,\n",
    "         'has_hashtag': hashtag,\n",
    "         'has_url': url,\n",
    "         'fav_count': favorite,\n",
    "         'is_reply': reply,\n",
    "         'retweet_count': retweet,\n",
    "         'followers': follower,\n",
    "         'following' : following,\n",
    "         'user': user,\n",
    "         'screen_name' : screen_name\n",
    "        }\n",
    "    \n",
    "    return pd.DataFrame(data = d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = tweets_to_df('tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
