{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping with python\n",
    "\n",
    "\n",
    "Web scraping is a technique to automatically access and extract large amounts of information from a website, which can save a huge amount of time and effort. \n",
    "\n",
    "\n",
    "#### Important notes about web scraping:\n",
    "Read through the website’s Terms and Conditions to understand how you can legally use the data. Most sites prohibit you from using the data for commercial purposes.\n",
    "Make sure you are not downloading data at too rapid a rate because this may break the website. You may potentially be blocked from the site as well 8for this purpose we will make use of time module)\n",
    "\n",
    "\n",
    "## First: inspect website\n",
    "\n",
    "The first thing that we need to do is to figure out where we can locate the links to the files we want to download inside the multiple levels of HTML tags. \n",
    "On the website, right click and click on “Inspect”(wiew page source / html ...). This allows you to see the raw code behind the site.\n",
    "\n",
    "You can also import and 'prettify' it via Python (with BeautifulSoup)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The following modules will be used:\n",
    "\n",
    "#### Request\n",
    "\n",
    "Is a module allowing to interrogate web sites and get response.\n",
    "\n",
    "request.get(url)\n",
    "\n",
    "you can pass parameters via a dictionary when interrogating websites with arguments\n",
    "\n",
    "payload = {'key1': 'value1', 'key2': ['value2', 'value3']}\n",
    "\n",
    "r = requests.get('https://zzz.org/get', params=payload)\n",
    "\n",
    "print(r.url)\n",
    "\n",
    "https://zzz.org/get?key1=value1&key2=value2&key2=value3\n",
    "\n",
    "\n",
    "\n",
    ".text property return text\n",
    ".json() funtion returns json structure\n",
    "\n",
    "requests allows also to handle forms cookies etc.\n",
    "\n",
    "\n",
    "#### BeautifulSoup (bs4)\n",
    "\n",
    "is a Python library for pulling data out of HTML and XML files.\n",
    "\n",
    "It allow to parse html trees and display nested structure\n",
    "\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "print(soup.prettify()) --> shows html tree nested\n",
    "\n",
    "Works very well on tags \n",
    "\n",
    "soup = BeautifulSoup('<b class=\"boldest\">Extremely bold</b>')\n",
    "tag = soup.b\n",
    "type(tag)\n",
    "\n",
    "tag.name  --> name of tag (b in the example)\n",
    "\n",
    "### Example\n",
    "\n",
    "In this example we start with Symantec virtual patent marking website:\n",
    "\n",
    "https://www.symantec.com/en/uk/about/legal/virtual-patent\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "#import urllib.request\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.symantec.com/en/uk/about/legal/virtual-patent\"\n",
    "response = requests.get(url)\n",
    "\n",
    "response  # 200 means page has been hooked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "print(soup.prettify()[155050:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# second step\n",
    "\n",
    "Decide which tags to use for parsing the 'soup'.\n",
    "\n",
    "function findAll(TAG) returns a list with all the items within a given tag\n",
    "\n",
    "One common task is extracting all the URLs found within a page’s '< a >' tags:\n",
    "\n",
    "for link in soup.find_all('a'):\n",
    "\n",
    "    print(link.get('href'))\n",
    "    \n",
    "In our case we are looking for paragraphs that contain the patent list.    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.findAll('p')[43:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful property is .content\n",
    "\n",
    "It allows to get whats inside the tag, as a list to account possibly nested tags.\n",
    "\n",
    "The first element of our soup above\n",
    "\n",
    "[<p><b>Norton Core:</b> Protected by U.S. Patents D791,768, and 9,565,192. Additional patents may be pending in the U.S. and elsewhere.</p>,\n",
    "\n",
    "will be thus splitted in two sub elements in the list.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vps = []\n",
    "\n",
    "for i in range(1,len(soup.findAll('p'))): #'p' tags are for paragraph \n",
    "    one_p_tag = soup.findAll('p')[i]\n",
    "    \n",
    "    if len(one_p_tag.contents)>1:   # only lines with title and list of patents\n",
    "        vps.append([one_p_tag.contents[0], one_p_tag.contents[1]])\n",
    "\n",
    "vps        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we remove html tags using .text property\n",
    "# we also transform element 0 into a string via .join() method\n",
    "\n",
    "for vp in vps:\n",
    "    if vp:\n",
    "        vp[0] = BeautifulSoup(''.join(vp[0]), \"lxml\").text\n",
    "      \n",
    "vps[:2]   # first 3 elements only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we create a list splitting the patents\n",
    "for vp in vps:\n",
    "    pats = vp[1].split(', ')\n",
    "    print(pats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we create a list splitting the patents and \n",
    "# a new list with item + patent\n",
    "\n",
    "vp_split = []\n",
    "\n",
    "for vp in vps:\n",
    "    pats = vp[1].split(', ')\n",
    "    for pat in pats:\n",
    "        vp_split.append([vp[0], pat])\n",
    "    \n",
    "vp_split[:10]   # leading 10 records only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last step: put all in a dataframe for better management\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "feature_list = ['item', 'patents']\n",
    "\n",
    "vps_df = pd.DataFrame(vp_split, columns=feature_list)\n",
    "\n",
    "vps_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### other websites with VP:\n",
    "\n",
    "https://www.3m.com/3M/en_US/company-us/patent/\n",
    "\n",
    "https://www.honeywellaidc.com/en-sg/working-with-us/patents \n",
    "\n",
    "https://www.pg.com/patents/brands.shtml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
