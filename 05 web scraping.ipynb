{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping with python\n",
    "\n",
    "\n",
    "Web scraping is a technique to automatically access and extract large amounts of information from a website, which can save a huge amount of time and effort. \n",
    "\n",
    "The following modules will be used:\n",
    "\n",
    "#### Request\n",
    "\n",
    "Is a module allowing to interrogate web sites and get response.\n",
    "\n",
    "request.get(url)\n",
    "\n",
    "you can pass parameters via a dictionary when interrogating websites with arguments\n",
    "\n",
    "payload = {'key1': 'value1', 'key2': ['value2', 'value3']}\n",
    "\n",
    "r = requests.get('https://zzz.org/get', params=payload)\n",
    "\n",
    "print(r.url)\n",
    "\n",
    "https://zzz.org/get?key1=value1&key2=value2&key2=value3\n",
    "\n",
    "\n",
    "\n",
    ".text property return text\n",
    ".json() funtion returns json structure\n",
    "\n",
    "requests allows also to handle forms cookies etc.\n",
    "\n",
    "\n",
    "#### BeautifulSoup (bs4)\n",
    "\n",
    "is a Python library for pulling data out of HTML and XML files.\n",
    "\n",
    "It allow to parse html trees and display nested structure\n",
    "\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "print(soup.prettify()) --> shows html tree nested\n",
    "\n",
    "Works very well on tags \n",
    "\n",
    "soup = BeautifulSoup('<b class=\"boldest\">Extremely bold</b>')\n",
    "tag = soup.b\n",
    "type(tag)\n",
    "\n",
    "tag.name  --> name of tag (b in the example)\n",
    "\n",
    "\n",
    "\n",
    "#### Important notes about web scraping:\n",
    "Read through the website’s Terms and Conditions to understand how you can legally use the data. Most sites prohibit you from using the data for commercial purposes.\n",
    "Make sure you are not downloading data at too rapid a rate because this may break the website. You may potentially be blocked from the site as well 8for this purpose we will make use of time module)\n",
    "\n",
    "\n",
    "## First: inspect website\n",
    "\n",
    "The first thing that we need to do is to figure out where we can locate the links to the files we want to download inside the multiple levels of HTML tags. \n",
    "On the website, right click and click on “Inspect”(wiew page source / html ...). This allows you to see the raw code behind the site.\n",
    "\n",
    "You can also import and prettify it via BS\n",
    "\n",
    "In this example we start with Symantec virtual patent marking website:\n",
    "\n",
    "https://www.symantec.com/en/uk/about/legal/virtual-patent\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "#import urllib.request\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.symantec.com/en/uk/about/legal/virtual-patent\"\n",
    "response = requests.get(url)\n",
    "\n",
    "response  # 200 means page has been hooked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "print(soup.prettify()[155050:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# second step\n",
    "\n",
    "Decide which tags to use for parsing the 'soup'.\n",
    "\n",
    "function findAll(TAG) returns a list with all the items within a given tag\n",
    "\n",
    "One common task is extracting all the URLs found within a page’s '< a >' tags:\n",
    "\n",
    "for link in soup.find_all('a'):\n",
    "\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.findAll('p')[43:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vps = []\n",
    "\n",
    "for i in range(1,len(soup.findAll('p'))): #'p' tags are for paragraph \n",
    "    one_p_tag = soup.findAll('p')[i]\n",
    "    \n",
    "    if len(one_p_tag.contents)>1:\n",
    "        vps.append([one_p_tag.contents[0], one_p_tag.contents[1]])\n",
    "\n",
    "vps        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we remove html tags using .text property\n",
    "# we also transform element 0 into a string via .join() method\n",
    "\n",
    "for vp in vps:\n",
    "    if vp:\n",
    "        vp[0] = BeautifulSoup(''.join(vp[0]), \"lxml\").text\n",
    "      \n",
    "vps[:2]   # first 3 elements only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we create a list splitting the patents\n",
    "for vp in vps:\n",
    "    pats = vp[1].split(', ')\n",
    "    print(pats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we create a list splitting the patents and \n",
    "# a new list with item + patent\n",
    "\n",
    "vp_split = []\n",
    "\n",
    "for vp in vps:\n",
    "    pats = vp[1].split(', ')\n",
    "    for pat in pats:\n",
    "        vp_split.append([vp[0], pat])\n",
    "    \n",
    "vp_split[:10]   # leading 10 records only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last step: put all in a dataframe for better management\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "feature_list = ['item', 'patents']\n",
    "\n",
    "vps_df = pd.DataFrame(vp_split, columns=feature_list)\n",
    "\n",
    "vps_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### other websites with VP:\n",
    "\n",
    "https://www.3m.com/3M/en_US/company-us/patent/\n",
    "\n",
    "https://www.honeywellaidc.com/en-sg/working-with-us/patents \n",
    "\n",
    "https://www.pg.com/patents/brands.shtml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
