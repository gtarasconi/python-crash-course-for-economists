{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning with tensorflow\n",
    "\n",
    "TensorFlow is an end-to-end open source platform for machine learning. \n",
    "\n",
    "It has been released by Google and works also in Javascripts, IoT, mobile etc. \n",
    "\n",
    "SensorFlow’s high-level APIs are based on the Keras API standard for defining and training neural networks (same as Theano).\n",
    "\n",
    "It also allow to use a notebook based environment (colab) to test models and prototypes\n",
    "\n",
    "https://www.tensorflow.org/\n",
    "\n",
    "This example is based on tutorial @\n",
    "https://www.tensorflow.org/hub/tutorials/text_classification_with_tf_hub\n",
    "\n",
    "\n",
    "Modules needed:\n",
    "\n",
    "pyyaml  - Required to save models in YAML format\n",
    "\n",
    "tensorflow\n",
    "tensorflow_hub\n",
    "(install in anaconda with conda install -c conda-forge tensorflow-hub )\n",
    "\n",
    "seaborn - data visualization library\n",
    "\n",
    "### NOTE: tensorflow work with python 3.7 since feb 2019\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Reduce logging output.\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## importing patent data\n",
    "\n",
    "Now we put in a dataframe a sample of patents from EPO with first IPC4 among:\n",
    "\n",
    "A61B\tDIAGNOSIS; SURGERY; IDENTIFICATION\n",
    "\n",
    "A61K    PREPARATIONS FOR MEDICAL, DENTAL, OR TOILET PURPOSES\n",
    "\n",
    "C07D\tHETEROCYCLIC COMPOUNDS ACYCLIC, CARBOCYCLIC\n",
    "\n",
    "G01N\tINVESTIGATING OR ANALYSING MATERIALS BY DETERMINING THEIR CHEMICAL OR PHYSICAL PROPERTIES \n",
    "\n",
    "G06F\tELECTRIC DIGITAL DATA PROCESSING \n",
    "\n",
    "H01L \tSEMICONDUCTOR DEVICES\n",
    "\n",
    "H04L\tTRANSMISSION OF DIGITAL INFORMATION\n",
    "\n",
    "H04N\tPICTORIAL COMMUNICATION, e.g. TELEVISION\n",
    "\n",
    "We also run some checks on distribution, nulls etc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"patentdata_ipc4_5000.csv\")\n",
    "\n",
    "data.groupby('IPC4')['APPLN_ABSTRACT'].count() # check distribution of ipc4 in sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we run a check for empty values\n",
    "\n",
    "data['APPLN_ABSTRACT'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data set up\n",
    "\n",
    "We replace here IPC4 with a numeric tag via a dictionary;\n",
    "\n",
    "as we have seen in previous lecture, classifiers work with numeric tags;\n",
    "\n",
    "we also set up description and clusters that will be the objects for our ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cluster']=data['IPC4']\n",
    "\n",
    "d1 = { 'A61B' : 0,\n",
    "       'A61K' : 1,\n",
    "       'C07D' : 2, \n",
    "       'G01N' : 3, \n",
    "       'G06F' : 4,\n",
    "       'H01L' : 5,  \n",
    "       'H04L' : 6,\n",
    "       'H04N' : 7}\n",
    "\n",
    "data.replace({'cluster': d1}, inplace=True)\n",
    "data[['cluster']] = data[['cluster']].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we can split our data into training and testing sets \n",
    "using an 80% / 20% train / test split;\n",
    "\n",
    "we also drop unnecessary columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(data) * .8)\n",
    "\n",
    "train_df = data.loc[1:train_size, ['APPLN_ABSTRACT','cluster']]\n",
    "test_df = data.loc[train_size:, ['APPLN_ABSTRACT','cluster']]\n",
    "\n",
    "\n",
    "# shows size of 2 dfs\n",
    "print(len(test_df) , len(train_df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input functions\n",
    "\n",
    "Estimator framework provides input functions that wrap Pandas dataframes.\n",
    "\n",
    "#### tf.estimator \n",
    "\n",
    "is a high-level TensorFlow API that greatly simplifies machine learning programming. Estimators encapsulate the following actions:\n",
    "\n",
    "training\n",
    "\n",
    "evaluation\n",
    "\n",
    "prediction\n",
    "\n",
    "export for serving\n",
    "\n",
    "\n",
    "#### tf.estimator.inputs.pandas_input_fn(x, y=None,  batch_size=128, num_epochs=1, shuffle=None,\n",
    "####       queue_capacity=1000,    num_threads=1,   target_column='target')\n",
    "\n",
    "Args:\n",
    "\n",
    "x: pandas DataFrame object.\n",
    "\n",
    "y: pandas Series object or DataFrame. None if absent.\n",
    "\n",
    "batch_size: int, size of batches to return.\n",
    "\n",
    "num_epochs: int, number of epochs to iterate over data. If not None, read attempts that would exceed this value will raise OutOfRangeError.\n",
    "\n",
    "shuffle: bool, whether to read the records in random order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns input function that feeds Pandas DataFrame into the model\n",
    "\n",
    "# Training input on the whole training set with no limit on training epochs.\n",
    "train_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
    "    train_df, train_df[\"cluster\"], num_epochs=None, shuffle=True)\n",
    "\n",
    "# Prediction on the whole training set.\n",
    "predict_train_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
    "    train_df, train_df[\"cluster\"], shuffle=False)\n",
    "\n",
    "# Prediction on the test set.\n",
    "predict_test_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
    "    test_df, test_df[\"cluster\"], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding layers\n",
    "\n",
    "Word embeddings provide a dense representation of words and their relative meanings.\n",
    "\n",
    "They are an improvement over sparse representations used in simpler bag of word model representations.\n",
    "\n",
    "Word embeddings can be learned from text data and reused among projects. They can also be learned as part of fitting a neural network on text data.\n",
    "\n",
    "Words are represented by dense vectors where a vector represents the projection of the word into a continuous vector space.\n",
    "\n",
    "The position of a word within the vector space is learned from text and is based on the words that surround the word when it is used.\n",
    "\n",
    "And for example a dense vector (1, 2, 0, 0, 5, 0, 9, 0, 0) will be represented as {(0,1,4,6), (1, 2, 5, 9)}\n",
    "(position, value)\n",
    "\n",
    "Our model will only have one feature (the description) and it’ll be represented as an embedding column.\n",
    "\n",
    "TF Hub simplifies this process by providing text embeddings that have already been trained on a variety of text data.\n",
    "\n",
    "For English text, TF Hub provides a variety of embeddings trained on different kinds of text data:\n",
    "\n",
    "##### Universal sentence encoder: for longer form text inputs\n",
    "\n",
    "##### ELMo: deep embeddings trained on the 1B Word Benchmark\n",
    "\n",
    "##### Neural Network Language Model embeddings: trained on Google News\n",
    "\n",
    "##### Word2vec: trained on Wikipedia\n",
    "\n",
    "full details @https://tfhub.dev/s?q=embedding%20\n",
    "\n",
    "For italian language is available a multilanguage embedding\n",
    "https://tfhub.dev/google/universal-sentence-encoder-xling-many/1\n",
    "\n",
    "The pre-trained text embeddings you choose is a hyperparameter in your model, so it’s best to experiment with different ones and see which one yields the highest accuracy. \n",
    "\n",
    "We can use hub.text_embedding_column to create a feature column for this layer in one line of code, passing it the name of our layer (“APPLN_ABSTRACT”) and the URL of the TF Hub model we’ll be using:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it takes some time\n",
    "# nnlm-en-dim128 = Neural Network Language Model embeddings\n",
    "# if you like you can try other embeddings\n",
    "# https://tfhub.dev/google/universal-sentence-encoder/2\n",
    "# https://tfhub.dev/google/Wiki-words-500-with-normalization/1\n",
    "\n",
    "\n",
    "embedded_text_feature_column = hub.text_embedding_column(\n",
    "    key=\"APPLN_ABSTRACT\", \n",
    "    module_spec=\"https://tfhub.dev/google/nnlm-en-dim128/1\",\n",
    "    trainable=True)\n",
    "\n",
    "print('Hub model loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimator\n",
    "\n",
    "The Estimator object wraps a model which is specified by a model_fn, which, given inputs and a number of other parameters, returns the ops necessary to perform training, evaluation, or predictions.\n",
    "\n",
    "In this case a prebuilt DNNClassifier (Deep Neural Network) will be used since we have a discrete range of values.\n",
    "If we had mapped our values on a continuous we could have used DNNRegressor.\n",
    "\n",
    "#### tf.estimator.DNNClassifier()\n",
    "\n",
    "hidden_units: aka the neurons or dimensional space:  List of hidden units per layer. All layers are fully connected. Ex. [64, 32] means first layer has 64 nodes and second one has 32.\n",
    "\n",
    "feature_columns: An iterable containing all the feature columns used by the model. All items in the set should be instances of classes derived from FeatureColumn.\n",
    "\n",
    "model_dir: Directory to save model parameters, graph and etc. This can also be used to load checkpoints from the directory into a estimator to continue training a previously saved model.\n",
    "\n",
    "n_classes: number of label classes. Default is binary classification. It must be greater than 1. Note: Class labels are integers representing the class index (i.e. values from 0 to n_classes-1). For arbitrary label values (e.g. string labels), convert to class indices first.\n",
    "\n",
    "weight_column_name: A string defining feature column name representing weights. It is used to down weight or boost examples during training. It will be multiplied by the loss of the example.\n",
    "\n",
    "optimizer: An instance of tf.Optimizer used to train the model. If None, will use an Adagrad optimizer.\n",
    "(Learning rate : is a hyper-parameter that controls how much we are adjusting the weights of our network with respect the loss gradient. The lower the value, the slower we travel along the downward slope.\n",
    "\n",
    "Full ilist of optimizers: https://www.tensorflow.org/api_docs/python/tf/train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run_config = tf.estimator.RunConfig(keep_checkpoint_max=1)\n",
    "\n",
    "n_class = len(data[\"cluster\"].unique())  # classes = distinct number of ipc4 in original dataset\n",
    "\n",
    "estimator = tf.estimator.DNNClassifier(\n",
    "    hidden_units=[250, 50],\n",
    "    feature_columns=[embedded_text_feature_column],\n",
    "    n_classes=n_class, \n",
    "    optimizer=tf.train.AdagradOptimizer(learning_rate=0.003))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.train(input_fn=train_input_fn, steps=1000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eval_result = estimator.evaluate(input_fn=predict_train_input_fn)\n",
    "test_eval_result = estimator.evaluate(input_fn=predict_test_input_fn)\n",
    "\n",
    "print(\"Training set accuracy: {accuracy}\".format(**train_eval_result))\n",
    "print(\"Test set accuracy: {accuracy}\".format(**test_eval_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix\n",
    "\n",
    "Using seaborn we can visually check the confusion matrix to understand the distribution of misclassifications.\n",
    "\n",
    "Each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class (or vice versa).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we rescue d1 dictionary for labels of graph\n",
    "\n",
    "LABELS = []\n",
    "for d in d1:\n",
    "    LABELS.append(d)\n",
    "\n",
    "LABELS.sort()\n",
    "print (LABELS)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_predictions(estimator, input_fn):\n",
    "  return [x[\"class_ids\"][0] for x in estimator.predict(input_fn=input_fn)]\n",
    "\n",
    "\n",
    "# Create a confusion matrix on training data.\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "  cm = tf.confusion_matrix(train_df[\"cluster\"], \n",
    "                           get_predictions(estimator, predict_train_input_fn))\n",
    "  with tf.Session() as session:\n",
    "    cm_out = session.run(cm)\n",
    "\n",
    "# Normalize the confusion matrix so that each row sums to 1.\n",
    "cm_out = cm_out.astype(float) / cm_out.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "sns.heatmap(cm_out, annot=True , xticklabels=LABELS, yticklabels=LABELS);\n",
    "plt.xlabel(\"Predicted\");\n",
    "plt.ylabel(\"True\");\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a confusion matrix on test data.\n",
    "with tf.Graph().as_default():\n",
    "  cm = tf.confusion_matrix(test_df[\"cluster\"], \n",
    "                           get_predictions(estimator, predict_test_input_fn))\n",
    "  with tf.Session() as session:\n",
    "    cm_out = session.run(cm)\n",
    "\n",
    "# Normalize the confusion matrix so that each row sums to 1.\n",
    "cm_out = cm_out.astype(float) / cm_out.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "sns.heatmap(cm_out, annot=True, xticklabels=LABELS, yticklabels=LABELS);\n",
    "plt.xlabel(\"Predicted\");\n",
    "plt.ylabel(\"True\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting the results\n",
    "\n",
    "\n",
    "This last step shows and exports forecasts:\n",
    "\n",
    "add a column to test df with prediction\n",
    "\n",
    "export to csv predicted results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in dataframe we add a column with preditions\n",
    "\n",
    "test_df[\"Predictions\"] = get_predictions(estimator, predict_test_input_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# step 1 invert my dictionary\n",
    "inv_d1 = dict((d1[k], k) for k in d1)\n",
    "\n",
    "\n",
    "# step 2 replace in predictions cluster index with label\n",
    "\n",
    "test_df_export=test_df.copy()\n",
    "\n",
    "test_df_export.replace({'cluster': inv_d1}, inplace=True)\n",
    "\n",
    "test_df_export.replace({'Predictions': inv_d1}, inplace=True)\n",
    "\n",
    "test_df_export[test_df_export.cluster != test_df_export.Predictions].head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last step export csv\n",
    "\n",
    "test_df_export.to_csv('test_check.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging another set of data\n",
    "\n",
    "Last step is to apply out ML algorithm is to run it on another dataset;\n",
    "\n",
    "For this purpose an extract of WOS abstracts in field of computer science will be used.\n",
    "\n",
    "We need only to name according the columns and get predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data2=pd.read_csv(\"wos_abstracts_cs_1000.csv\")\n",
    "\n",
    "predict_df = data2.loc[:, ['abstract']]\n",
    "predict_df.columns = ['APPLN_ABSTRACT']\n",
    "\n",
    "predict_df[\"cluster\"]=4  # G06F expectation \n",
    "\n",
    "predict_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_predict_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
    "    predict_df, predict_df[\"cluster\"], shuffle=False)\n",
    "\n",
    "# estimator.train(input_fn=predict_predict_input_fn, steps=1000);\n",
    "\n",
    "predict_df[\"Predictions\"] = get_predictions(estimator, predict_predict_input_fn)\n",
    "\n",
    "# put back IPC4\n",
    "\n",
    "predict_df.replace({'cluster': inv_d1}, inplace=True)\n",
    "predict_df.replace({'Predictions': inv_d1}, inplace=True)\n",
    "\n",
    "predict_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of records by prediction\n",
    "\n",
    "predict_df.groupby(\"Predictions\")['APPLN_ABSTRACT'].count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter tuning: \n",
    "\n",
    "we can improve the accuracy by tuning the meta-parameters like the learning rate or the number of steps, especially if we use a different module. A validation set is very important if we want to get any reasonable results, because it is very easy to set-up a model that learns to predict the training data without generalizing well to the test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
