{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Ludwig\n",
    "\n",
    "Ludwig is a toolbox built on top of TensorFlow that allows to train and test deep learning models without the need to write code.\n",
    "\n",
    "All you need to provide is a CSV file containing your data, a list of columns to use as inputs, and a list of columns to use as outputs, Ludwig will do the rest. Simple commands can be used to train models both locally and in a distributed way, and to use them to predict on new data.\n",
    "(note also HDF5 abd json input is available)\n",
    "\n",
    "Developed by Uber, is release under the open source Apache License 2.0.\n",
    "\n",
    "Available @ https://github.com/uber/ludwig\n",
    "\n",
    "### installation and steps\n",
    "\n",
    "#### Install:   \n",
    "pip install ludwig  \n",
    "\n",
    "python -m spacy download en\n",
    "\n",
    "[note works under python 3.6 not 3.7 due to Tensorflow dependencies]\n",
    "\n",
    "#### Train:    \n",
    "Prepare your data in a CSV file, define input and output feature in a model definition YAML file.\n",
    "\n",
    "#### Predict:    \n",
    "use a pre-trained model to predict the output targets.\n",
    "\n",
    "#### Visualize:    \n",
    "Ludwig comes with many visualization options to understand deep learning models performance and compare their predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset examined\n",
    "\n",
    "In our examples we will use a subset of patent data taken from EPO PATSTAT (patentdata.csv);\n",
    "\n",
    "the data contains 5.000 EP applications with the following fields\n",
    "\n",
    "APPLN_AUTH patent filing office (EPO)\n",
    "APPLN_ID  application unique id\n",
    "EARLIEST_FILING_YEAR   priority year\n",
    "APPLN_ABSTRACT   patent abstract\n",
    "IPC4  first IPC - leading 4 chars\n",
    "PSN_NAME   first applicant standardized name\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'patentdata_cit2.csv' does not exist: b'patentdata_cit2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e6493ed18ff8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0maa\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"patentdata_cit2.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0maa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'CITRANGE'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1122\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1123\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1855\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'patentdata_cit2.csv' does not exist: b'patentdata_cit2.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "aa =pd.read_csv(\"patentdata_cit2.csv\")\n",
    "aa.groupby('CITRANGE').count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data definition in Ludwig\n",
    "\n",
    "Previously was explain that training a model in Ludwig is pretty straightforward: you provide a CSV dataset and a model definition YAML file.\n",
    "\n",
    "The model definition contains a list of input features and output features, all you have to do is specify names of the columns in the CSV that are inputs to your model alongside with their datatypes, and names of columns in the CSV that will be outputs, the target variables which the model will learn to predict. Ludwig will compose a deep learning model accordingly and train it for you.\n",
    "\n",
    "Currently the available datatypes in Ludwig are:\n",
    "\n",
    "binary\n",
    "\n",
    "numerical\n",
    "\n",
    "category\n",
    "\n",
    "set\n",
    "\n",
    "bag\n",
    "\n",
    "sequence\n",
    "\n",
    "text\n",
    "\n",
    "timeseries\n",
    "\n",
    "image\n",
    "\n",
    "The model definition can contain additional information, in particular how to preprocess each column in the CSV.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "From Patents dataset we will use abstract, year and applicant standard name to forecast the content of IPC4 field\n",
    "\n",
    "\"APPLN_AUTH\",\"APPLN_ID\",\"EARLIEST_FILING_YEAR\",\"APPLN_ABSTRACT\",\"IPC4\",\"PSN_NAME\"\n",
    "\n",
    "\"EP\",1,1999,\"a lot of text\",\"G06K\",\"NOKIA CORPORATION\"\n",
    "\n",
    "Model training by calling \n",
    "\n",
    "ludwig train [options]\n",
    "\n",
    "  --data_csv DATA_CSV   input data CSV file. If it has a split column, it will\n",
    "                        be used for splitting (0: train, 1: validation, 2:\n",
    "                        test), otherwise the dataset will be randomly split\n",
    "                        \n",
    "  -mdf --model_definition_file MODEL_DEFINITION_FILE\n",
    "                        YAML file describing the model. \n",
    "                        \n",
    "  --output_directory OUTPUT_DIRECTORY\n",
    "                        directory that contains the results                        \n",
    "                        \n",
    "\n",
    "(refer to: https://uber.github.io/ludwig/user_guide/)\n",
    "\n",
    "We will use a multi input model defined in modeldefinition.yaml file\n",
    "\n",
    "\n",
    "The structure of the model definition file is a dictionary with five keys:\n",
    "\n",
    "\n",
    "input_features: []\n",
    "combiner: {}\n",
    "output_features: []\n",
    "training: {}\n",
    "preprocessing: {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_features:\n",
    "    -\n",
    "        name: EARLIEST_FILING_YEAR\n",
    "        type: numerical\n",
    "    -\n",
    "        name: APPLN_ABSTRACT\n",
    "        type: text\n",
    "        missing_value_strategy: ''\n",
    "    -\n",
    "        name: PSN_NAME\n",
    "        type: text\n",
    "        missing_value_strategy: ''\n",
    "\n",
    "output_features:\n",
    "    -\n",
    "        name: IPC4\n",
    "        type: category\n",
    "\n",
    "\n",
    "\n",
    "and start the training typing the following command in your console:\n",
    "\n",
    "#### ludwig train --data_csv patent_data.csv --model_definition_file model_definition.yaml --output_directory patresults1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "(note you can distribute the training of your models using Horovod, which allows to train on a single machine with multiple GPUs as well as on multiple machines)\n",
    "\n",
    "After training, Ludwig will create a directory under results containing the trained model with its hyperparameters and summary statistics of the training process. \n",
    "\n",
    "Inside the folders you will find a lot of intermediate files: in particular  one HDF5 and one JSON. The HDF5 file contains the data mapped to numpy ndarrays, while the JSON file contains the mappings from the values in the tensors to their original labels.\n",
    "When rerunning the training or going to next steps, those files will be used to save time.\n",
    "\n",
    "Data can be splitted among  train, validation and test in several ways: either providing a column named SPLIT or three separate data sets (--data_train_csv, --data_validation_csv, --data_test_csv).\n",
    "\n",
    "Other important files are:\n",
    "\n",
    "description.json - a file containing a description of the training process with all the information to reproduce it.\n",
    "\n",
    "training_statistics.json which contains records of all measures and losses for each epoch.\n",
    "\n",
    "model - a directory containing model hyperparameters, weights, checkpoints and logs (for TensorBoard).\n",
    "\n",
    "\n",
    "## visualize training results\n",
    "\n",
    "\n",
    "You can visualize them using one of the several visualization options available in the visualize tool,\n",
    "\n",
    "\n",
    "ludvig visualize -v (type of graph) -ts/ps path to stats.json\n",
    "\n",
    "#### ludwig visualize --visualization learning_curves --training_statistics path/to/training_statistics.json\n",
    "\n",
    "other visualizations available:\n",
    "\n",
    "https://uber.github.io/ludwig/user_guide/#visualizations\n",
    "\n",
    "\n",
    "confusion_matrix\n",
    "\n",
    "compare_performance\n",
    "\n",
    "compare_classifiers_performance_from_pred\n",
    "\n",
    "### how to read loss and accuracy:\n",
    "\n",
    "The lower the loss, the better a model. The loss is calculated on training and validation and its interperation is how well the model is doing for these two sets. Unlike accuracy, loss is not a percentage. It is a summation of the errors made for each example in training or validation sets.\n",
    "\n",
    "Loss is often used in the training process to find the \"best\" parameter values for your model \n",
    "\n",
    "Accuracy is more from an applied perspective. Once you find the optimized parameters above, you use this metrics to evaluate how accurate your model's prediction is compared to the true data.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "\n",
    "If you want your previously trained model to predict target output values, you can type the following command in your console:\n",
    "\n",
    "#### ludwig predict --data_csv path/to/data.csv --model_path /path/to/model -od output directory\n",
    "\n",
    "Running this command will return model predictions and some test performance statistics if the new dataset contains ground truth information to compare to. Those can be visualized by the visualize tool, which can also be used to compare performances and predictions of different models, for instance:\n",
    "\n",
    "\n",
    "#### ludwig visualize --visualization compare_performance -ps path/to/test_statistics_model_1.json path/to/test_statistics_model_2.json\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://dev.to/chrishunt/code-free-machine-learning-with-ludwig-2gap\n",
    "    \n",
    "    https://towardsdatascience.com/metrics-to-evaluate-your-machine-learning-algorithm-f10ba6e38234"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note with\n",
    "\n",
    "### ludwig.experiment\n",
    "\n",
    "it is possible to run train & predict at once\n",
    "\n",
    "#### ludwig experiment  --data_csv cars.csv --model_definition_file modeldef.yaml --output_directory results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
