"UID","itemtitle","pubyear","abstract","subject"
"WOS:000072238700006","The use of proof planning for co-operative theorem proving","1998","
<p>We describe BARNACLE: a co-operative interface to the CLAM inductive theorem proving system. For the foreseeable future, there will be theorems which cannot be proved completely automatically, so the ability to allow human intervention is desirable; for this intervention to be productive the problem of orienting the user in the proof attempt must be overcome. There are many semi-automatic theorem provers: we call our style of theorem proving co-operative, in that the skills of both human and automaton are used each to their best advantage, and used together may find a proof where other methods fail. The co-operative nature of the BARNACLE interface is made possible by the proof planning technique underpinning CLAM. Our claim is that proof planning makes new kinds of user interaction possible.</p>
<p>Proof planning is a technique for guiding the search for a proof in automatic theorem proving. Common patterns of reasoning in proofs are identified and represented computationally as proof plans, which can then be used to guide the search for proofs of new conjectures. We have harnessed the explanatory power of proof planning to enable the user to understand where the automatic prover got to and why it is stuck. A user can analyse the failed proof in terms of CLAM'S specification language, and hence override the prover to force or prevent the application of a tactic, or discover a proof patch. This patch might be to apply further rules or tactics to bridge the gap between the effects of previous tactics and the preconditions needed by a currently inapplicable tactic. (C) 1998 Academic Press Limited.</p>","Computer Science, Theory & Methods"
"WOS:000072238700006","The use of proof planning for co-operative theorem proving","1998","
<p>We describe BARNACLE: a co-operative interface to the CLAM inductive theorem proving system. For the foreseeable future, there will be theorems which cannot be proved completely automatically, so the ability to allow human intervention is desirable; for this intervention to be productive the problem of orienting the user in the proof attempt must be overcome. There are many semi-automatic theorem provers: we call our style of theorem proving co-operative, in that the skills of both human and automaton are used each to their best advantage, and used together may find a proof where other methods fail. The co-operative nature of the BARNACLE interface is made possible by the proof planning technique underpinning CLAM. Our claim is that proof planning makes new kinds of user interaction possible.</p>
<p>Proof planning is a technique for guiding the search for a proof in automatic theorem proving. Common patterns of reasoning in proofs are identified and represented computationally as proof plans, which can then be used to guide the search for proofs of new conjectures. We have harnessed the explanatory power of proof planning to enable the user to understand where the automatic prover got to and why it is stuck. A user can analyse the failed proof in terms of CLAM'S specification language, and hence override the prover to force or prevent the application of a tactic, or discover a proof patch. This patch might be to apply further rules or tactics to bridge the gap between the effects of previous tactics and the preconditions needed by a currently inapplicable tactic. (C) 1998 Academic Press Limited.</p>","Computer Science"
"WOS:000071193500035","Nonparametric estimation of the cyclic cross spectrum","1998","
<p>Cyclostationary processes are an important class of nonstationary processes. In this correspondence we consider nonparametric estimation of the cyclic cross-spectrum. A smoothed periodogram-based estimator is studied and its asymptotic behavior characterized, extending recent univariate work to the multivariate case. Application to cyclic coherence measurements is discussed, The results are useful in a variety of multisensor cyclostationary signal processing scenarios such as time delay and bearing estimation.</p>","Computer Science, Information Systems"
"WOS:000071193500035","Nonparametric estimation of the cyclic cross spectrum","1998","
<p>Cyclostationary processes are an important class of nonstationary processes. In this correspondence we consider nonparametric estimation of the cyclic cross-spectrum. A smoothed periodogram-based estimator is studied and its asymptotic behavior characterized, extending recent univariate work to the multivariate case. Application to cyclic coherence measurements is discussed, The results are useful in a variety of multisensor cyclostationary signal processing scenarios such as time delay and bearing estimation.</p>","Computer Science"
"WOS:000072522700009","On a mixed approach to the finite element solution of large strain elastoplastic problems","1998","
<p>A mixed approach to large strain elastoplastic problems is presented in a somewhat different may to that usually used within the context of the additive split of the rate of deformation tensor into an elastic and plastic part. A nonlinear extended mixed variational equation, in which the Jacobian of the deformation gradient and the pressure part of the stress tensor appear as additional independent variables, is introduced. This equation is then linearized in the accordance with the Newton-Raphson method to obtain the system of linear equations which represent the basis of the mixed finite element procedure. For the case of a bilinear isoparametric interpolation of the displacement field, and for piece-wise constant pressure and Jacobian, simplified expressions, differing from similar expressions corresponding to mixed finite element implementations, are obtained. The effectiveness of the proposed mixed approach is demonstrated by means of two examples.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000072522700009","On a mixed approach to the finite element solution of large strain elastoplastic problems","1998","
<p>A mixed approach to large strain elastoplastic problems is presented in a somewhat different may to that usually used within the context of the additive split of the rate of deformation tensor into an elastic and plastic part. A nonlinear extended mixed variational equation, in which the Jacobian of the deformation gradient and the pressure part of the stress tensor appear as additional independent variables, is introduced. This equation is then linearized in the accordance with the Newton-Raphson method to obtain the system of linear equations which represent the basis of the mixed finite element procedure. For the case of a bilinear isoparametric interpolation of the displacement field, and for piece-wise constant pressure and Jacobian, simplified expressions, differing from similar expressions corresponding to mixed finite element implementations, are obtained. The effectiveness of the proposed mixed approach is demonstrated by means of two examples.</p>","Computer Science"
"WOS:000086659400040","Improving translation quality by manipulating sentence length","1998","
<p>Translation systems tend to have mure trouble with long sentences than with short ones for a variety of reasons. When the source and target languages differ rather markedly, as do Japanese and English, this problem is reflected in lower quality output. To improve readability, we experimented with automatically splitting long sentences into shorter ones. This paper outlines the problem, describes the sentence splitting procedure and rules, and provides an evaluation of the results.</p>","Computer Science, Artificial Intelligence"
"WOS:000086659400040","Improving translation quality by manipulating sentence length","1998","
<p>Translation systems tend to have mure trouble with long sentences than with short ones for a variety of reasons. When the source and target languages differ rather markedly, as do Japanese and English, this problem is reflected in lower quality output. To improve readability, we experimented with automatically splitting long sentences into shorter ones. This paper outlines the problem, describes the sentence splitting procedure and rules, and provides an evaluation of the results.</p>","Computer Science"
"WOS:000074112600019","Communication optimizations for parallel C programs","1998","
<p>This paper presents algorithms for reducing the communication overhead for parallel C programs that use dynamically-allocated data structures. The framework consists of an analysis phase called possible-placement analysis, and a transformation phase called communication selection.</p>
<p>The fundamental idea of possible-placement analysis is to find all possible points for insertion of remote memory operations. Remote reads are propagated upwards, whereas remote writes are propagated downwards. Based on the results of the possible-placement analysis, the communication selection transformation selects the ""best"" place for inserting the communication, and determines if pipelining or blocking of communication should be performed.</p>
<p>The framework has been implemented in the EARTH-McCAT optimizing/parallelizing C compiler, and experimental results are presented for five pointer-intensive benchmarks running on the EARTH-MANNA distributed-memory parallel architecture. These experiments show that the communication optimization can provide performance improvements of up to 16% over the unoptimized benchmarks.</p>","Computer Science, Software Engineering"
"WOS:000074112600019","Communication optimizations for parallel C programs","1998","
<p>This paper presents algorithms for reducing the communication overhead for parallel C programs that use dynamically-allocated data structures. The framework consists of an analysis phase called possible-placement analysis, and a transformation phase called communication selection.</p>
<p>The fundamental idea of possible-placement analysis is to find all possible points for insertion of remote memory operations. Remote reads are propagated upwards, whereas remote writes are propagated downwards. Based on the results of the possible-placement analysis, the communication selection transformation selects the ""best"" place for inserting the communication, and determines if pipelining or blocking of communication should be performed.</p>
<p>The framework has been implemented in the EARTH-McCAT optimizing/parallelizing C compiler, and experimental results are presented for five pointer-intensive benchmarks running on the EARTH-MANNA distributed-memory parallel architecture. These experiments show that the communication optimization can provide performance improvements of up to 16% over the unoptimized benchmarks.</p>","Computer Science"
"WOS:000072281800006","A generic grouping algorithm and its quantitative analysis","1998","
<p>This paper presents a generic method for perceptual grouping and an analysis of its expected grouping quality. The grouping method is fairly general: It may be used for the grouping of various types of data features, and to incorporate different grouping cues operating over feature sets of different sizes. The proposed method is divided into two parts: constructing a graph representation of the available perceptual grouping evidence, and then finding the ""best"" partition of the graph into groups. The first stage includes a cue enhancement procedure, which integrates the information available from multifeature cues into very reliable bifeature cues. Both stages are implemented using known statistical tools such as Wald's SPRT algorithm and the Maximum Likelihood criterion. The accompanying theoretical analysis of this grouping criterion quantifies intuitive expectations and predicts that the expected grouping quality increases with cue reliability. It also shows that investing more computational effort in the grouping algorithm leads to better grouping results. This analysis, which quantifies the grouping power of the Maximum Likelihood criterion, is independent of the grouping domain. To our best knowledge, such an analysis of a grouping process is given here for the first time. Three grouping algorithms, in three different domains, are synthesized as instances of the generic method. They demonstrate the applicability and generality of this grouping method.</p>","Computer Science, Artificial Intelligence"
"WOS:000072281800006","A generic grouping algorithm and its quantitative analysis","1998","
<p>This paper presents a generic method for perceptual grouping and an analysis of its expected grouping quality. The grouping method is fairly general: It may be used for the grouping of various types of data features, and to incorporate different grouping cues operating over feature sets of different sizes. The proposed method is divided into two parts: constructing a graph representation of the available perceptual grouping evidence, and then finding the ""best"" partition of the graph into groups. The first stage includes a cue enhancement procedure, which integrates the information available from multifeature cues into very reliable bifeature cues. Both stages are implemented using known statistical tools such as Wald's SPRT algorithm and the Maximum Likelihood criterion. The accompanying theoretical analysis of this grouping criterion quantifies intuitive expectations and predicts that the expected grouping quality increases with cue reliability. It also shows that investing more computational effort in the grouping algorithm leads to better grouping results. This analysis, which quantifies the grouping power of the Maximum Likelihood criterion, is independent of the grouping domain. To our best knowledge, such an analysis of a grouping process is given here for the first time. Three grouping algorithms, in three different domains, are synthesized as instances of the generic method. They demonstrate the applicability and generality of this grouping method.</p>","Computer Science"
"WOS:000077612600059","COMPaS: A Pentium Pro PC-based SMP cluster and its experience","1998","
<p>We have built an eight node SMP cluster called COMPaS (Cluster Of Multi-Processor Systems), each node of which is a quad-processor Pentium Pro PC. We have designed and implemented a remote memory based user-level communication layer which provides low-overhead and high bandwidth using Myrinet. We designed a hybrid programming model in order to take advantage of locality in each SMP node. Intra-node computations utilize a multi-threaded programming style (Solaris threads) and inter-node programming is based on message passing and remote memory operations. In this paper we report on this hybrid shared memory/distributed memory programming on COMPaS and its preliminary evaluation. The performance of COMPaS is affected by data size and access patterns, and the proportion of inter-node communication. If the data size is small enough to all fit on the cache, parallel efficiency exceeds 1.0 using the hybrid programming model on COMPaS. But the performance is limited by the low memory bus bandwidth of PC-based SMP nodes for some memory intensive workloads.</p>","Computer Science, Theory & Methods"
"WOS:000077612600059","COMPaS: A Pentium Pro PC-based SMP cluster and its experience","1998","
<p>We have built an eight node SMP cluster called COMPaS (Cluster Of Multi-Processor Systems), each node of which is a quad-processor Pentium Pro PC. We have designed and implemented a remote memory based user-level communication layer which provides low-overhead and high bandwidth using Myrinet. We designed a hybrid programming model in order to take advantage of locality in each SMP node. Intra-node computations utilize a multi-threaded programming style (Solaris threads) and inter-node programming is based on message passing and remote memory operations. In this paper we report on this hybrid shared memory/distributed memory programming on COMPaS and its preliminary evaluation. The performance of COMPaS is affected by data size and access patterns, and the proportion of inter-node communication. If the data size is small enough to all fit on the cache, parallel efficiency exceeds 1.0 using the hybrid programming model on COMPaS. But the performance is limited by the low memory bus bandwidth of PC-based SMP nodes for some memory intensive workloads.</p>","Computer Science"
"WOS:000076599400007","Algebraic properties of multiple-valued module systems and their applications to current-mode CMOS circuits","1998","
<p>The paper presents the concepts of pseudoprime and modulo correlativity and establishes the relationships among completeness of module operations, uniqueness of solution of equations, invertibility of a square matrix, and correlativity of vectors in multiple-valued modulo system. It is shown that current-mode CMOS circuits are easy to design and economical using bounded operations.</p>","Computer Science, Hardware & Architecture"
"WOS:000076599400007","Algebraic properties of multiple-valued module systems and their applications to current-mode CMOS circuits","1998","
<p>The paper presents the concepts of pseudoprime and modulo correlativity and establishes the relationships among completeness of module operations, uniqueness of solution of equations, invertibility of a square matrix, and correlativity of vectors in multiple-valued modulo system. It is shown that current-mode CMOS circuits are easy to design and economical using bounded operations.</p>","Computer Science, Theory & Methods"
"WOS:000076599400007","Algebraic properties of multiple-valued module systems and their applications to current-mode CMOS circuits","1998","
<p>The paper presents the concepts of pseudoprime and modulo correlativity and establishes the relationships among completeness of module operations, uniqueness of solution of equations, invertibility of a square matrix, and correlativity of vectors in multiple-valued modulo system. It is shown that current-mode CMOS circuits are easy to design and economical using bounded operations.</p>","Computer Science"
"WOS:000082316200005","Self-evolution in a constructive binary string system","1998","
<p>We examine the qualitative dynamics of a catalytic self-organizing system of binary strings that is inspired by the chemical information processing metaphor. A string is interpreted in two different ways: either (a) as raw data or (b) as a machine that is able to process another string as data in order to produce a third one. This article focuses on the phenomena of evolution whose appearance is notable because no explicit mutation, recombination, or artificial selection operators are introduced. We call the system self-evolving because every variation is performed by the objects themselves in their machine form.</p>","Computer Science, Artificial Intelligence"
"WOS:000082316200005","Self-evolution in a constructive binary string system","1998","
<p>We examine the qualitative dynamics of a catalytic self-organizing system of binary strings that is inspired by the chemical information processing metaphor. A string is interpreted in two different ways: either (a) as raw data or (b) as a machine that is able to process another string as data in order to produce a third one. This article focuses on the phenomena of evolution whose appearance is notable because no explicit mutation, recombination, or artificial selection operators are introduced. We call the system self-evolving because every variation is performed by the objects themselves in their machine form.</p>","Computer Science, Theory & Methods"
"WOS:000082316200005","Self-evolution in a constructive binary string system","1998","
<p>We examine the qualitative dynamics of a catalytic self-organizing system of binary strings that is inspired by the chemical information processing metaphor. A string is interpreted in two different ways: either (a) as raw data or (b) as a machine that is able to process another string as data in order to produce a third one. This article focuses on the phenomena of evolution whose appearance is notable because no explicit mutation, recombination, or artificial selection operators are introduced. We call the system self-evolving because every variation is performed by the objects themselves in their machine form.</p>","Computer Science"
"WOS:000075882500010","A criterion to enforce correctness of indirectly cooperating applications","1998","
<p>Cooperative applications are expected to become commonplace in the future. We are concerned hen with a special case of cooperation called indirect cooperation. The idea of the paper is that a Concurrency Control approach better fits to indirect cooperation than a Concurrent Programming one. In other words, there do exist syntactic correctness criteria which define a large sphere of security in which application programmers are released from the burden of interaction explicit programming. This paper argues this point of view and describes such a criterion: the COO-serializability. It applies for a class of applications which cooperate indirectly. (C) 1998 Elsevier Science Inc. All rights reserved.</p>","Computer Science, Information Systems"
"WOS:000075882500010","A criterion to enforce correctness of indirectly cooperating applications","1998","
<p>Cooperative applications are expected to become commonplace in the future. We are concerned hen with a special case of cooperation called indirect cooperation. The idea of the paper is that a Concurrency Control approach better fits to indirect cooperation than a Concurrent Programming one. In other words, there do exist syntactic correctness criteria which define a large sphere of security in which application programmers are released from the burden of interaction explicit programming. This paper argues this point of view and describes such a criterion: the COO-serializability. It applies for a class of applications which cooperate indirectly. (C) 1998 Elsevier Science Inc. All rights reserved.</p>","Computer Science"
"WOS:000074376500011","Properties of support vector machines","1998","
<p>Support vector machines (SVMs) perform pattern recognition between two point classes by finding a decision surface determined by certain points of the training set, termed support vectors (SV). This surface, which in some feature space of possibly infinite dimension can be regarded as a hyperplane, is obtained from the solution of a problem of quadratic programming that depends on a regularization parameter. In this article, we study some mathematical properties of support vectors and show that the decision surface can be written as the sum of two orthogonal terms, the first depending on only the margin vectors (which are SVs lying on the margin), the second proportional to the regularization parameter. For almost all values of the parameter, this enables us to predict how the decision surface varies for small parameter changes. In the special but important case of feature space of finite dimension m, we also show that there are at most m + 1 margin vectors and observe that m + 1 SVs are usually sufficient to determine the decision surface fully. For relatively small m,this latter result leads to a consistent reduction of the SV number.</p>","Computer Science, Artificial Intelligence"
"WOS:000074376500011","Properties of support vector machines","1998","
<p>Support vector machines (SVMs) perform pattern recognition between two point classes by finding a decision surface determined by certain points of the training set, termed support vectors (SV). This surface, which in some feature space of possibly infinite dimension can be regarded as a hyperplane, is obtained from the solution of a problem of quadratic programming that depends on a regularization parameter. In this article, we study some mathematical properties of support vectors and show that the decision surface can be written as the sum of two orthogonal terms, the first depending on only the margin vectors (which are SVs lying on the margin), the second proportional to the regularization parameter. For almost all values of the parameter, this enables us to predict how the decision surface varies for small parameter changes. In the special but important case of feature space of finite dimension m, we also show that there are at most m + 1 margin vectors and observe that m + 1 SVs are usually sufficient to determine the decision surface fully. For relatively small m,this latter result leads to a consistent reduction of the SV number.</p>","Computer Science"
"WOS:000076459600006","Backwards analysis of the Karger-Klein-Tarjan algorithm for minimum spanning trees","1998","
<p>This note gives a short proof of a sampling lemma used by Karger, Klein, and Tajan in the analysis of their randomized linear-time algorithm for minimum spanning trees. (C) 1998 Published by Elsevier Science B.V. All rights reserved.</p>","Computer Science, Information Systems"
"WOS:000076459600006","Backwards analysis of the Karger-Klein-Tarjan algorithm for minimum spanning trees","1998","
<p>This note gives a short proof of a sampling lemma used by Karger, Klein, and Tajan in the analysis of their randomized linear-time algorithm for minimum spanning trees. (C) 1998 Published by Elsevier Science B.V. All rights reserved.</p>","Computer Science"
"WOS:000082774400076","Power of cooperation and multihead finite systems","1998","
<p>We consider systems of finite automats performing together computation on an input string. Each automaton has its own read head that moves independently of the Ether heads, but the automats cooperate in making state transitions. Computational power of such devices depends on the number of states of automata, the number of automate, and the way they cooperate. We concentrate our attention an the last issue. The first situation that we consider is that each automaton has a full knowledge on the states of all automata (multihead automata). The other extreme is that each automaton (called also a processor) has no knowledge of the states of other automata; merely, there is a central processing unit that may ""freeze"" any automaton or let it proceed its work (so called multiprocessor automata). The second model seems to be severely restricted, but we show that multihead and multiprocessor automata have similar computational power. Nevertheless, we show a separation result.</p>","Computer Science, Software Engineering"
"WOS:000082774400076","Power of cooperation and multihead finite systems","1998","
<p>We consider systems of finite automats performing together computation on an input string. Each automaton has its own read head that moves independently of the Ether heads, but the automats cooperate in making state transitions. Computational power of such devices depends on the number of states of automata, the number of automate, and the way they cooperate. We concentrate our attention an the last issue. The first situation that we consider is that each automaton has a full knowledge on the states of all automata (multihead automata). The other extreme is that each automaton (called also a processor) has no knowledge of the states of other automata; merely, there is a central processing unit that may ""freeze"" any automaton or let it proceed its work (so called multiprocessor automata). The second model seems to be severely restricted, but we show that multihead and multiprocessor automata have similar computational power. Nevertheless, we show a separation result.</p>","Computer Science"
"WOS:000072557900002","Interpolation models with multiple hyperparameters","1998","
<p>A traditional interpolation model is characterized by the choice of regularizer applied to the interpolant, and the choice of noise model. Typically, the regularizer has a single regularization constant a, and the noise model has a single parameter beta. The ratio alpha/beta alone is responsible. for determining globally all these attributes of the interpolant: its `complexity', 'flexibility', `smoothness', `characteristic scale length', and `characteristic amplitude'. We suggest that interpolation models should be able to capture more than just one flavour of simplicity and complexity. We describe Bayesian models in which the interpolant has a smoothness that varies spatially. We emphasize the importance, in practical implementation, of the concept of `conditional convexity' when designing models with many hyperparameters. We apply the new models to the interpolation of neuronal spike data and demonstrate a substantial improvement in generalization error. 0960-3174 1998 (C) Chapman & Hall.</p>","Computer Science, Theory & Methods"
"WOS:000072557900002","Interpolation models with multiple hyperparameters","1998","
<p>A traditional interpolation model is characterized by the choice of regularizer applied to the interpolant, and the choice of noise model. Typically, the regularizer has a single regularization constant a, and the noise model has a single parameter beta. The ratio alpha/beta alone is responsible. for determining globally all these attributes of the interpolant: its `complexity', 'flexibility', `smoothness', `characteristic scale length', and `characteristic amplitude'. We suggest that interpolation models should be able to capture more than just one flavour of simplicity and complexity. We describe Bayesian models in which the interpolant has a smoothness that varies spatially. We emphasize the importance, in practical implementation, of the concept of `conditional convexity' when designing models with many hyperparameters. We apply the new models to the interpolation of neuronal spike data and demonstrate a substantial improvement in generalization error. 0960-3174 1998 (C) Chapman & Hall.</p>","Computer Science"
"WOS:000079049400001","Protecting IT systems from cyber crime","1998","
<p>Large-scale commercial, industrial and financial operations are becoming ever more interdependent, and ever more dependent on IT. At the same time, the rapidly growing interconnectivity of IT systems, and the convergence of their technology towards industry-standard hardware and software components and sub-systems, renders these IT systems increasingly vulnerable to malicious attack. This paper is aimed particularly at readers concerned with major systems employed in medium to large commercial or industrial enterprises. It examines the nature and significance of the various potential attacks, and surveys the defence options available, It concludes that IT owners need to think of the threat in more global terms, and to give a new focus and priority to their defence. Prompt action can ensure a major improvement in IT resilience at a modest marginal cost, both in terms of finance and in terms of normal IT operation.</p>","Computer Science, Hardware & Architecture"
"WOS:000079049400001","Protecting IT systems from cyber crime","1998","
<p>Large-scale commercial, industrial and financial operations are becoming ever more interdependent, and ever more dependent on IT. At the same time, the rapidly growing interconnectivity of IT systems, and the convergence of their technology towards industry-standard hardware and software components and sub-systems, renders these IT systems increasingly vulnerable to malicious attack. This paper is aimed particularly at readers concerned with major systems employed in medium to large commercial or industrial enterprises. It examines the nature and significance of the various potential attacks, and surveys the defence options available, It concludes that IT owners need to think of the threat in more global terms, and to give a new focus and priority to their defence. Prompt action can ensure a major improvement in IT resilience at a modest marginal cost, both in terms of finance and in terms of normal IT operation.</p>","Computer Science, Information Systems"
"WOS:000079049400001","Protecting IT systems from cyber crime","1998","
<p>Large-scale commercial, industrial and financial operations are becoming ever more interdependent, and ever more dependent on IT. At the same time, the rapidly growing interconnectivity of IT systems, and the convergence of their technology towards industry-standard hardware and software components and sub-systems, renders these IT systems increasingly vulnerable to malicious attack. This paper is aimed particularly at readers concerned with major systems employed in medium to large commercial or industrial enterprises. It examines the nature and significance of the various potential attacks, and surveys the defence options available, It concludes that IT owners need to think of the threat in more global terms, and to give a new focus and priority to their defence. Prompt action can ensure a major improvement in IT resilience at a modest marginal cost, both in terms of finance and in terms of normal IT operation.</p>","Computer Science, Software Engineering"
"WOS:000079049400001","Protecting IT systems from cyber crime","1998","
<p>Large-scale commercial, industrial and financial operations are becoming ever more interdependent, and ever more dependent on IT. At the same time, the rapidly growing interconnectivity of IT systems, and the convergence of their technology towards industry-standard hardware and software components and sub-systems, renders these IT systems increasingly vulnerable to malicious attack. This paper is aimed particularly at readers concerned with major systems employed in medium to large commercial or industrial enterprises. It examines the nature and significance of the various potential attacks, and surveys the defence options available, It concludes that IT owners need to think of the threat in more global terms, and to give a new focus and priority to their defence. Prompt action can ensure a major improvement in IT resilience at a modest marginal cost, both in terms of finance and in terms of normal IT operation.</p>","Computer Science, Theory & Methods"
"WOS:000079049400001","Protecting IT systems from cyber crime","1998","
<p>Large-scale commercial, industrial and financial operations are becoming ever more interdependent, and ever more dependent on IT. At the same time, the rapidly growing interconnectivity of IT systems, and the convergence of their technology towards industry-standard hardware and software components and sub-systems, renders these IT systems increasingly vulnerable to malicious attack. This paper is aimed particularly at readers concerned with major systems employed in medium to large commercial or industrial enterprises. It examines the nature and significance of the various potential attacks, and surveys the defence options available, It concludes that IT owners need to think of the threat in more global terms, and to give a new focus and priority to their defence. Prompt action can ensure a major improvement in IT resilience at a modest marginal cost, both in terms of finance and in terms of normal IT operation.</p>","Computer Science"
"WOS:000083174200022","Integral uniform flows in symmetric networks - (Extended abstract)","1998","
<p>We study the integral uniform (multicommodity) flow problem in a graph G and construct a fractional solution whose properties are invariant under the action of the automorphism group Aut(G) of G. The fractional solution is shown to be close to an integral solution (depending on properties of Aut(G)), and in particular becomes an integral solution for a class of graphs containing Cayley graphs. As an application we estimate asymptotically (up to error terms) the edge congestion of an optimal integral uniform flow (edge forwarding index) in the cube connected cycles and the butterfly.</p>","Computer Science, Theory & Methods"
"WOS:000083174200022","Integral uniform flows in symmetric networks - (Extended abstract)","1998","
<p>We study the integral uniform (multicommodity) flow problem in a graph G and construct a fractional solution whose properties are invariant under the action of the automorphism group Aut(G) of G. The fractional solution is shown to be close to an integral solution (depending on properties of Aut(G)), and in particular becomes an integral solution for a class of graphs containing Cayley graphs. As an application we estimate asymptotically (up to error terms) the edge congestion of an optimal integral uniform flow (edge forwarding index) in the cube connected cycles and the butterfly.</p>","Computer Science"
"WOS:000077563200015","The timeless way: Making living cooperative buildings with design patterns","1998","
<p>Interfaces to information systems, and the buildings in which such systems are embedded will typically be the result of the work of a large number of different disciplines, potentially ranging from ethnographers to architects. A common language and conceptual framework has the potential for greatly enhancing the effectiveness and ease of cross-disciplinary communication. In this paper we describe some aspects of the notion of design patterns developed by architect Christopher Alexander and colleagues in the 1970's. We briefly show how Alexander-style patterns can be used for analysis and design in some of the disciplines implicated in the creation of successful cooperative buildings - interface design, ergonomic design, functionality design and office design and suggest that pattern languages might be a way of bridging the communication gaps between professions to produce a shared vision of the cooperative building project.</p>","Computer Science, Cybernetics"
"WOS:000077563200015","The timeless way: Making living cooperative buildings with design patterns","1998","
<p>Interfaces to information systems, and the buildings in which such systems are embedded will typically be the result of the work of a large number of different disciplines, potentially ranging from ethnographers to architects. A common language and conceptual framework has the potential for greatly enhancing the effectiveness and ease of cross-disciplinary communication. In this paper we describe some aspects of the notion of design patterns developed by architect Christopher Alexander and colleagues in the 1970's. We briefly show how Alexander-style patterns can be used for analysis and design in some of the disciplines implicated in the creation of successful cooperative buildings - interface design, ergonomic design, functionality design and office design and suggest that pattern languages might be a way of bridging the communication gaps between professions to produce a shared vision of the cooperative building project.</p>","Computer Science, Theory & Methods"
"WOS:000077563200015","The timeless way: Making living cooperative buildings with design patterns","1998","
<p>Interfaces to information systems, and the buildings in which such systems are embedded will typically be the result of the work of a large number of different disciplines, potentially ranging from ethnographers to architects. A common language and conceptual framework has the potential for greatly enhancing the effectiveness and ease of cross-disciplinary communication. In this paper we describe some aspects of the notion of design patterns developed by architect Christopher Alexander and colleagues in the 1970's. We briefly show how Alexander-style patterns can be used for analysis and design in some of the disciplines implicated in the creation of successful cooperative buildings - interface design, ergonomic design, functionality design and office design and suggest that pattern languages might be a way of bridging the communication gaps between professions to produce a shared vision of the cooperative building project.</p>","Computer Science"
"WOS:000082776000055","Comparison of implicit and explicit parallel programming models for a finite element simulation algorithm","1998","
<p>In this paper we compare efficiency of two versions of a parallel algorithm for finite element compressible fluid flow simulations on unstructured grids. The first version is based on the explicit model of parallel programming (with message-passing paradigm), while the second incorporates the implicit model (in which data-parallel programming is used). Time discretization of the compressible Euler equations is organized with a linear, implicit version of the Taylor-Galerkin time scheme, while finite elements are employed for space discretization of one step problems. The resulting nonsymmetric system of linear equations is solved iteratively with the preconditioned GMRES method.</p>
<p>The algorithm has been tested on HP Exemplar SPP1600 computer using a benchmark problem of 2D inviscid how simulations - the ramp problem.</p>","Computer Science, Theory & Methods"
"WOS:000082776000055","Comparison of implicit and explicit parallel programming models for a finite element simulation algorithm","1998","
<p>In this paper we compare efficiency of two versions of a parallel algorithm for finite element compressible fluid flow simulations on unstructured grids. The first version is based on the explicit model of parallel programming (with message-passing paradigm), while the second incorporates the implicit model (in which data-parallel programming is used). Time discretization of the compressible Euler equations is organized with a linear, implicit version of the Taylor-Galerkin time scheme, while finite elements are employed for space discretization of one step problems. The resulting nonsymmetric system of linear equations is solved iteratively with the preconditioned GMRES method.</p>
<p>The algorithm has been tested on HP Exemplar SPP1600 computer using a benchmark problem of 2D inviscid how simulations - the ramp problem.</p>","Computer Science"
"WOS:000078325700003","Ensuring consistency in multidatabases by preserving two-level serializability","1998","
<p>The concept of serializability has been the traditionally accepted correctness criterion in database systems. However in multidatabase systems (MDBSs), ensuring global serializability is a difficult task. The difficulty arises due to the heterogeneity of the concurrency control protocols used by the participating local database management systems (DBMSs), and the desire to preserve the autonomy of the local DBMSs. In general, solutions to the global serializability problem result in executions with a low degree of concurrency. The alternative, relaxed serializability, may result in data inconsistency.</p>
<p>In this article, we introduce a systematic approach to relaxing the serializability requirement in MDBS environments. Our approach exploits the structure of the integrity constraints and the nature of transaction programs to ensure consistency without requiring executions to be serializable. We develop a simple yet powerful classification of MDBSs based on the nature of integrity constraints and transaction programs. For each of the identified models we show how consistency can be preserved by ensuring that executions are two-level serializable (2LSR). 2LSR is a correctness criterion for MDBS environments weaker than serializability. What makes our approach interesting is that unlike global serializability, ensuring 2LSR in MDBS environments is relatively simple and protocols to ensure 2LSR permit a high degree of concurrency. Furthermore, we believe the range of models we consider cover many practical MDBS environments to which the results of this article can be applied to preserve database consistency.</p>","Computer Science, Information Systems"
"WOS:000078325700003","Ensuring consistency in multidatabases by preserving two-level serializability","1998","
<p>The concept of serializability has been the traditionally accepted correctness criterion in database systems. However in multidatabase systems (MDBSs), ensuring global serializability is a difficult task. The difficulty arises due to the heterogeneity of the concurrency control protocols used by the participating local database management systems (DBMSs), and the desire to preserve the autonomy of the local DBMSs. In general, solutions to the global serializability problem result in executions with a low degree of concurrency. The alternative, relaxed serializability, may result in data inconsistency.</p>
<p>In this article, we introduce a systematic approach to relaxing the serializability requirement in MDBS environments. Our approach exploits the structure of the integrity constraints and the nature of transaction programs to ensure consistency without requiring executions to be serializable. We develop a simple yet powerful classification of MDBSs based on the nature of integrity constraints and transaction programs. For each of the identified models we show how consistency can be preserved by ensuring that executions are two-level serializable (2LSR). 2LSR is a correctness criterion for MDBS environments weaker than serializability. What makes our approach interesting is that unlike global serializability, ensuring 2LSR in MDBS environments is relatively simple and protocols to ensure 2LSR permit a high degree of concurrency. Furthermore, we believe the range of models we consider cover many practical MDBS environments to which the results of this article can be applied to preserve database consistency.</p>","Computer Science, Software Engineering"
"WOS:000078325700003","Ensuring consistency in multidatabases by preserving two-level serializability","1998","
<p>The concept of serializability has been the traditionally accepted correctness criterion in database systems. However in multidatabase systems (MDBSs), ensuring global serializability is a difficult task. The difficulty arises due to the heterogeneity of the concurrency control protocols used by the participating local database management systems (DBMSs), and the desire to preserve the autonomy of the local DBMSs. In general, solutions to the global serializability problem result in executions with a low degree of concurrency. The alternative, relaxed serializability, may result in data inconsistency.</p>
<p>In this article, we introduce a systematic approach to relaxing the serializability requirement in MDBS environments. Our approach exploits the structure of the integrity constraints and the nature of transaction programs to ensure consistency without requiring executions to be serializable. We develop a simple yet powerful classification of MDBSs based on the nature of integrity constraints and transaction programs. For each of the identified models we show how consistency can be preserved by ensuring that executions are two-level serializable (2LSR). 2LSR is a correctness criterion for MDBS environments weaker than serializability. What makes our approach interesting is that unlike global serializability, ensuring 2LSR in MDBS environments is relatively simple and protocols to ensure 2LSR permit a high degree of concurrency. Furthermore, we believe the range of models we consider cover many practical MDBS environments to which the results of this article can be applied to preserve database consistency.</p>","Computer Science"
"WOS:000074533300024","ODBMS for LHC detector simulation","1998","
<p>One of the new and powerful features of the GEANT4 detector simulation toolkit is the newly designed ""Hits + Digitization"" category. Using this, the user can easily simulate responses of his/her own detector setup. On the other hand, it is well known that a huge number of simulated events are essential to large scale high energy experiments. Simulated detector responses are one candidate for the heavy use of an ODBMS. Following the first attempt to merge the efforts of CERN RD44 and CERN RD45, we report on the use and performance of ODBMS persistency for LHC detector simulation. The main topics are (1) design of persistent ""hit"" and ""trajectory"" classes, (2) use of the ODBMS for ""hits"" simulated data, and (3) the benchmark performance. (C) 1998 Elsevier Science B.V.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000074533300024","ODBMS for LHC detector simulation","1998","
<p>One of the new and powerful features of the GEANT4 detector simulation toolkit is the newly designed ""Hits + Digitization"" category. Using this, the user can easily simulate responses of his/her own detector setup. On the other hand, it is well known that a huge number of simulated events are essential to large scale high energy experiments. Simulated detector responses are one candidate for the heavy use of an ODBMS. Following the first attempt to merge the efforts of CERN RD44 and CERN RD45, we report on the use and performance of ODBMS persistency for LHC detector simulation. The main topics are (1) design of persistent ""hit"" and ""trajectory"" classes, (2) use of the ODBMS for ""hits"" simulated data, and (3) the benchmark performance. (C) 1998 Elsevier Science B.V.</p>","Computer Science"
"WOS:000073086800014","Simple inexpensive device for monitoring patient respiration","1998","
<p>A simple device is described that monitors respiration by sounding an alarm if respiration ceases. It is light and portable and performs well, even in a 60dB noise environment.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000073086800014","Simple inexpensive device for monitoring patient respiration","1998","
<p>A simple device is described that monitors respiration by sounding an alarm if respiration ceases. It is light and portable and performs well, even in a 60dB noise environment.</p>","Computer Science"
"WOS:000071038700002","Workflow evolution","1998","
<p>A basic step towards flexibility in workflow systems is the consistent and effective management of workflow evolution, i.e. of changing existing workflows while they are operational. One of the most challenging issue is the handling of running instances when their schemata are modified: simple solutions can be devised, but they often imply losing all the work done or failing in capturing the advantages offered by workflow modifications; this is unacceptable for many applications. In this paper we address the problem of workflow evolution, from both a static and a dynamic point of view. We define a complete, minimal, and consistent set of modification primitives that allow modifications of workflow schemata and we introduce a taxonomy of policies to manage evolution of running instances when the corresponding workflow schema is modified. Formal criteria are introduced, based on a simple workflow conceptual model, in order to determine which running instances can be transparently migrated to the new version. A case study, relating the assembling of a desktop computer, will exemplify the introduced concepts.</p>","Computer Science, Artificial Intelligence"
"WOS:000071038700002","Workflow evolution","1998","
<p>A basic step towards flexibility in workflow systems is the consistent and effective management of workflow evolution, i.e. of changing existing workflows while they are operational. One of the most challenging issue is the handling of running instances when their schemata are modified: simple solutions can be devised, but they often imply losing all the work done or failing in capturing the advantages offered by workflow modifications; this is unacceptable for many applications. In this paper we address the problem of workflow evolution, from both a static and a dynamic point of view. We define a complete, minimal, and consistent set of modification primitives that allow modifications of workflow schemata and we introduce a taxonomy of policies to manage evolution of running instances when the corresponding workflow schema is modified. Formal criteria are introduced, based on a simple workflow conceptual model, in order to determine which running instances can be transparently migrated to the new version. A case study, relating the assembling of a desktop computer, will exemplify the introduced concepts.</p>","Computer Science, Information Systems"
"WOS:000071038700002","Workflow evolution","1998","
<p>A basic step towards flexibility in workflow systems is the consistent and effective management of workflow evolution, i.e. of changing existing workflows while they are operational. One of the most challenging issue is the handling of running instances when their schemata are modified: simple solutions can be devised, but they often imply losing all the work done or failing in capturing the advantages offered by workflow modifications; this is unacceptable for many applications. In this paper we address the problem of workflow evolution, from both a static and a dynamic point of view. We define a complete, minimal, and consistent set of modification primitives that allow modifications of workflow schemata and we introduce a taxonomy of policies to manage evolution of running instances when the corresponding workflow schema is modified. Formal criteria are introduced, based on a simple workflow conceptual model, in order to determine which running instances can be transparently migrated to the new version. A case study, relating the assembling of a desktop computer, will exemplify the introduced concepts.</p>","Computer Science"
"WOS:000073493200004","Pulsatile blood flow in a pipe","1998","
<p>The unsteady flow of blood in a straight, long, rigid pipe, driven by an oscillatory pressure gradient is studied. Three different non-newtonian models for blood are considered and compared. One of them turns out to offer the best fit of experimental data, when the rheological parameters ale suitably fixed. Numerical results are obtained by a spectral collocation method in space and the implicit trapezoidal finite difference method in time. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000073493200004","Pulsatile blood flow in a pipe","1998","
<p>The unsteady flow of blood in a straight, long, rigid pipe, driven by an oscillatory pressure gradient is studied. Three different non-newtonian models for blood are considered and compared. One of them turns out to offer the best fit of experimental data, when the rheological parameters ale suitably fixed. Numerical results are obtained by a spectral collocation method in space and the implicit trapezoidal finite difference method in time. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science"
"WOS:000082774900016","Optimization with noisy function evaluations","1998","
<p>In the optimization literature it is frequently assumed that the quality of solutions can be determined by calculating deterministic objective function values. Practical optimization problems, however, often require the evaluation of solutions through experimentation, stochastic simulation, sampling, or even interaction with the user. Thus, most practical problems involve noise. We empirically investigate the robustness of population-based versus point-based optimization methods on a range of parameter optimization problems when noise is added. Our results favor population-based optimization, and the evolution strategy in particular.</p>","Computer Science, Theory & Methods"
"WOS:000082774900016","Optimization with noisy function evaluations","1998","
<p>In the optimization literature it is frequently assumed that the quality of solutions can be determined by calculating deterministic objective function values. Practical optimization problems, however, often require the evaluation of solutions through experimentation, stochastic simulation, sampling, or even interaction with the user. Thus, most practical problems involve noise. We empirically investigate the robustness of population-based versus point-based optimization methods on a range of parameter optimization problems when noise is added. Our results favor population-based optimization, and the evolution strategy in particular.</p>","Computer Science"
"WOS:000074653100010","System diagnosis with smallest risk of error","1998","
<p>We consider the problem of fault diagnosis in multiprocessor systems. Every processor can test its neighbors; fault-free processors correctly identify the fault status of tested neighbors, while faulty testers can give arbitrary test results. Processors fail independently with constant probability p<1/2 and the goal is to identify correctly the status of all processors, based on the set of test results. We give fast diagnosis algorithms with the highest possible probability of correctness for systems represented by complete bipartite graphs and by simple paths. This is for the first time that the most reliable fault diagnosis is given for these systems in a probabilistic model without any assumptions on the behavior of faulty processors. (C) 1998-Elsevier Science B.V. All rights reserved.</p>","Computer Science, Theory & Methods"
"WOS:000074653100010","System diagnosis with smallest risk of error","1998","
<p>We consider the problem of fault diagnosis in multiprocessor systems. Every processor can test its neighbors; fault-free processors correctly identify the fault status of tested neighbors, while faulty testers can give arbitrary test results. Processors fail independently with constant probability p<1/2 and the goal is to identify correctly the status of all processors, based on the set of test results. We give fast diagnosis algorithms with the highest possible probability of correctness for systems represented by complete bipartite graphs and by simple paths. This is for the first time that the most reliable fault diagnosis is given for these systems in a probabilistic model without any assumptions on the behavior of faulty processors. (C) 1998-Elsevier Science B.V. All rights reserved.</p>","Computer Science"
"WOS:000076904900005","Using agent-based software for scientific computing in the NetSolve system","1998","
<p>Agent-based computing is increasingly regarded as an elegant and efficient way of providing access to computational resources. Several metacomputing research projects are using intelligent agents to manage a resource space and to map user computation to these resources in an optimal fashion. Such a project is NetSolve, developed at the University of Tennessee and Oak Ridge National Laboratory. NetSolve provides the user with a variety of interfaces that afford direct access to preinstalled, freely available numerical libraries. These libraries are embedded in computational servers. New numerical functionalities can be integrated easily into the servers by a specific framework. The NetSolve agent manages the coherency of the computational servers. It also uses predictions about the network and processor performances to assign user requests to the most suitable servers. This article reviews some of the basic concepts in agent-based design, discusses the NetSolve project and how its agent enhances flexibility and performance, and provides examples of other research efforts. Also discussed are future directions in agent-based computing in general and in NetSolve in particular. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science, Theory & Methods"
"WOS:000076904900005","Using agent-based software for scientific computing in the NetSolve system","1998","
<p>Agent-based computing is increasingly regarded as an elegant and efficient way of providing access to computational resources. Several metacomputing research projects are using intelligent agents to manage a resource space and to map user computation to these resources in an optimal fashion. Such a project is NetSolve, developed at the University of Tennessee and Oak Ridge National Laboratory. NetSolve provides the user with a variety of interfaces that afford direct access to preinstalled, freely available numerical libraries. These libraries are embedded in computational servers. New numerical functionalities can be integrated easily into the servers by a specific framework. The NetSolve agent manages the coherency of the computational servers. It also uses predictions about the network and processor performances to assign user requests to the most suitable servers. This article reviews some of the basic concepts in agent-based design, discusses the NetSolve project and how its agent enhances flexibility and performance, and provides examples of other research efforts. Also discussed are future directions in agent-based computing in general and in NetSolve in particular. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science"
"WOS:000072911100022","Knowledge transform from a set of cases to production rule knowledgebase","1998","
<p>This paper presents a methodology of using the expert system SCANKEE for an automatic generation of knowledge bases which make it possible to simulate perturbed states of industrial chemical installations. The bases prepared by expert process engineers contain: a computer representation of the production scheme (graphic base); data of the production process parameters describing the undisturbed production of the plant (numerical base); a set of installation breakdowns (alpha-numerical base); and a description of the actions performed by a production engineer who traces and removes the breakdown (rule knowledge base). The elaborated programming tool makes it possible to train and examine production controllers supervising the functioning of the plant's installations. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000072911100022","Knowledge transform from a set of cases to production rule knowledgebase","1998","
<p>This paper presents a methodology of using the expert system SCANKEE for an automatic generation of knowledge bases which make it possible to simulate perturbed states of industrial chemical installations. The bases prepared by expert process engineers contain: a computer representation of the production scheme (graphic base); data of the production process parameters describing the undisturbed production of the plant (numerical base); a set of installation breakdowns (alpha-numerical base); and a description of the actions performed by a production engineer who traces and removes the breakdown (rule knowledge base). The elaborated programming tool makes it possible to train and examine production controllers supervising the functioning of the plant's installations. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science"
"WOS:000072040100013","Approximate Green's functions and a boundary element method for electro-elastic analyses of active materials","1998","
<p>A boundary element method (BEM) for the analysis of two-dimensional, time independent problems of linear electro-elasticity is presented. Emphasis is given to the derivation of representation formulas and fundamental Solutions as well as to the construction of an efficient numerical algorithm. The method is particularly suitable for studying the behavior of active materials such as electrostrictive, ferroelectric and piezoelectric ceramics. Two numerical examples with direct impact on the structural safety and reliability of piezoceramics are provided to demonstrate the virtues of the new BEM. (C) 1997 Elsevier Science Ltd.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000072040100013","Approximate Green's functions and a boundary element method for electro-elastic analyses of active materials","1998","
<p>A boundary element method (BEM) for the analysis of two-dimensional, time independent problems of linear electro-elasticity is presented. Emphasis is given to the derivation of representation formulas and fundamental Solutions as well as to the construction of an efficient numerical algorithm. The method is particularly suitable for studying the behavior of active materials such as electrostrictive, ferroelectric and piezoelectric ceramics. Two numerical examples with direct impact on the structural safety and reliability of piezoceramics are provided to demonstrate the virtues of the new BEM. (C) 1997 Elsevier Science Ltd.</p>","Computer Science"
"WOS:000073527400002","Fundamentals of implementing real-time control applications in distributed computer systems","1998","
<p>Automatic control applications are real-time systems which pose stringent requirements on precisely time-triggered synchronized actions and constant end-to-end delays in feedback loops which often constitute multi-rate systems. Motivated by the apparent gap between computer science and automatic control theory, a set of requirements for real-time implementation of control applications is given. A real-time behavioral model for control applications is then presented and exemplified. Important sources and characteristics of time-variations in distributed computer systems are investigated. This illuminates key execution strategies to ensure the required timing behavior. Implications on design and implementation and directions for further work are discussed.</p>","Computer Science, Theory & Methods"
"WOS:000073527400002","Fundamentals of implementing real-time control applications in distributed computer systems","1998","
<p>Automatic control applications are real-time systems which pose stringent requirements on precisely time-triggered synchronized actions and constant end-to-end delays in feedback loops which often constitute multi-rate systems. Motivated by the apparent gap between computer science and automatic control theory, a set of requirements for real-time implementation of control applications is given. A real-time behavioral model for control applications is then presented and exemplified. Important sources and characteristics of time-variations in distributed computer systems are investigated. This illuminates key execution strategies to ensure the required timing behavior. Implications on design and implementation and directions for further work are discussed.</p>","Computer Science"
"WOS:000083173400023","Encoding the hydra battle as a rewrite system","1998","
<p>In rewriting theory, termination of a rewrite system by Kruskal's theorem implies a theoretical upper bound on the complexity of the system. This bound is, however, far from having been reached by known examples of rewrite systems. All known orderings used to establish termination by Kruskal's theorem yield a multiply recursive bound. Furthermore, the study of the order types of such orderings suggests that the class of multiple recursive functions constitutes the least upper bound. Contradicting this intuition, we construct here a rewrite system which reduces by Kruskal's theorem and whose complexity is not multiply recursive. This system is even totally terminating. This leads to a new lower bound for the complexity of totally terminating rewrite systems and rewrite systems which reduce by Kruskal's theorem. Our construction relies on the Hydra battle using classical tools from ordinal theory and subrecursive functions.</p>","Computer Science, Theory & Methods"
"WOS:000083173400023","Encoding the hydra battle as a rewrite system","1998","
<p>In rewriting theory, termination of a rewrite system by Kruskal's theorem implies a theoretical upper bound on the complexity of the system. This bound is, however, far from having been reached by known examples of rewrite systems. All known orderings used to establish termination by Kruskal's theorem yield a multiply recursive bound. Furthermore, the study of the order types of such orderings suggests that the class of multiple recursive functions constitutes the least upper bound. Contradicting this intuition, we construct here a rewrite system which reduces by Kruskal's theorem and whose complexity is not multiply recursive. This system is even totally terminating. This leads to a new lower bound for the complexity of totally terminating rewrite systems and rewrite systems which reduce by Kruskal's theorem. Our construction relies on the Hydra battle using classical tools from ordinal theory and subrecursive functions.</p>","Computer Science"
"WOS:000082112700014","A formal embedding of AgentSpeak(L) in 3APL","1998","
<p>Agent-based computing in Artificial Intelligence has given rise to a number of diverse and competing proposals for agent programming languages. For several reasons it has been difficult to evaluate and compare those different proposals. One of the main reasons is the lack of a general semantic framework. In this paper, we give a formal embedding of the agent language AgentSpeak(L) in our own agent language 3APL. To this end we define a notion of simulation based on the formal operational semantics of the languages. A main result of the paper is a proof that 3APL can simulate AgentSpeak(L). As a consequence, 3APL has at least the same expressive power as AgentSpeak(L). The comparison yields some new insights into the features of the agent languages. One of the results is that AgentSpeak(L) can be substantially simplified.</p>","Computer Science, Artificial Intelligence"
"WOS:000082112700014","A formal embedding of AgentSpeak(L) in 3APL","1998","
<p>Agent-based computing in Artificial Intelligence has given rise to a number of diverse and competing proposals for agent programming languages. For several reasons it has been difficult to evaluate and compare those different proposals. One of the main reasons is the lack of a general semantic framework. In this paper, we give a formal embedding of the agent language AgentSpeak(L) in our own agent language 3APL. To this end we define a notion of simulation based on the formal operational semantics of the languages. A main result of the paper is a proof that 3APL can simulate AgentSpeak(L). As a consequence, 3APL has at least the same expressive power as AgentSpeak(L). The comparison yields some new insights into the features of the agent languages. One of the results is that AgentSpeak(L) can be substantially simplified.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000082112700014","A formal embedding of AgentSpeak(L) in 3APL","1998","
<p>Agent-based computing in Artificial Intelligence has given rise to a number of diverse and competing proposals for agent programming languages. For several reasons it has been difficult to evaluate and compare those different proposals. One of the main reasons is the lack of a general semantic framework. In this paper, we give a formal embedding of the agent language AgentSpeak(L) in our own agent language 3APL. To this end we define a notion of simulation based on the formal operational semantics of the languages. A main result of the paper is a proof that 3APL can simulate AgentSpeak(L). As a consequence, 3APL has at least the same expressive power as AgentSpeak(L). The comparison yields some new insights into the features of the agent languages. One of the results is that AgentSpeak(L) can be substantially simplified.</p>","Computer Science"
"WOS:000072747800004","Usenet newsgroups' profile analysis, utilising standard and non-standard statistical methods","1998","
<p>The paper explores building profiles of newsgroups from a corpus of Usenet e-mail messages, employing some standard statistical techniques as well as fuzzy clustering methods. A large set of data from a number of newsgroups has been analysed to elicit some text attributes, such as number of words, length of sentences and other stylistic characteristics. Readability scores have also been obtained by using recognised assessment methods. These text attributes were used for building newsgroups' profiles. Three newsgroups, each with a similar number of messages, were selected from the processed sample for the analysis of two types of one-dimensional profiles: one by length of texts and the second by readability scores. Those profiles are compared with corresponding profiles of the whole sample and also with those of a group of frequent participants in the newsgroups. Fuzzy clustering is used for creating two-dimensional profiles of the same groups. An attempt is made to identify the newsgroups by defining centres of data clusters.</p>
<p>It is contended that this approach to newsgroups' profile analysis could facilitate a better understanding of computer-mediated communication on the Usenet, which is a growing medium of informal business and personal correspondence.</p>","Computer Science, Information Systems"
"WOS:000072747800004","Usenet newsgroups' profile analysis, utilising standard and non-standard statistical methods","1998","
<p>The paper explores building profiles of newsgroups from a corpus of Usenet e-mail messages, employing some standard statistical techniques as well as fuzzy clustering methods. A large set of data from a number of newsgroups has been analysed to elicit some text attributes, such as number of words, length of sentences and other stylistic characteristics. Readability scores have also been obtained by using recognised assessment methods. These text attributes were used for building newsgroups' profiles. Three newsgroups, each with a similar number of messages, were selected from the processed sample for the analysis of two types of one-dimensional profiles: one by length of texts and the second by readability scores. Those profiles are compared with corresponding profiles of the whole sample and also with those of a group of frequent participants in the newsgroups. Fuzzy clustering is used for creating two-dimensional profiles of the same groups. An attempt is made to identify the newsgroups by defining centres of data clusters.</p>
<p>It is contended that this approach to newsgroups' profile analysis could facilitate a better understanding of computer-mediated communication on the Usenet, which is a growing medium of informal business and personal correspondence.</p>","Computer Science"
"WOS:000075865300003","P3a, perceptual distinctiveness, and stimulus modality","1998","
<p>A three-stimulus oddball paradigm (target, standard, nontarget) was employed in which subjects responded to an infrequent target, when its discrimination from the frequent standard was difficult. In separate auditory and visual modality conditions, the stimulus characteristics of an infrequent nontarget were manipulated such that its perceptual distinctiveness from the target was varied systematically. For both the low and high distinctiveness conditions, target stimulus P300 amplitude was larger than the nontarget only at the parietal electrode. In contrast, nontarget P3a amplitude was larger and earlier than the target P300 over the frontal/central electrode sites. The distinctiveness manipulation between the target and nontarget produced larger P3a component profiles for the auditory compared to visual stimuli. The results support previous findings that target/standard stimulus context determines P3a generation for both auditory and visual stimulus modalities and suggest that the distinctiveness of the eliciting stimulus contributes to P3a amplitude. Theoretical implications are discussed. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science, Artificial Intelligence"
"WOS:000075865300003","P3a, perceptual distinctiveness, and stimulus modality","1998","
<p>A three-stimulus oddball paradigm (target, standard, nontarget) was employed in which subjects responded to an infrequent target, when its discrimination from the frequent standard was difficult. In separate auditory and visual modality conditions, the stimulus characteristics of an infrequent nontarget were manipulated such that its perceptual distinctiveness from the target was varied systematically. For both the low and high distinctiveness conditions, target stimulus P300 amplitude was larger than the nontarget only at the parietal electrode. In contrast, nontarget P3a amplitude was larger and earlier than the target P300 over the frontal/central electrode sites. The distinctiveness manipulation between the target and nontarget produced larger P3a component profiles for the auditory compared to visual stimuli. The results support previous findings that target/standard stimulus context determines P3a generation for both auditory and visual stimulus modalities and suggest that the distinctiveness of the eliciting stimulus contributes to P3a amplitude. Theoretical implications are discussed. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science"
"WOS:000083172000011","A constraint-based approach for examination timetabling using local repair techniques","1998","
<p>We present in this paper an algorithm based upon the Constraint Satisfaction Problem model, used at the ""Ecole des Mines de Nantes"" to generate examination timetables. A strong constraint is that the computing time must be less than 1 minute. This led us to develop an incomplete algorithm, using local repair techniques, instead of an exhaustive search method. The program has been validated on fifty ""hand-made"" problems, and has succesfully solved the thirteen ""real"" problems.</p>","Computer Science, Theory & Methods"
"WOS:000083172000011","A constraint-based approach for examination timetabling using local repair techniques","1998","
<p>We present in this paper an algorithm based upon the Constraint Satisfaction Problem model, used at the ""Ecole des Mines de Nantes"" to generate examination timetables. A strong constraint is that the computing time must be less than 1 minute. This led us to develop an incomplete algorithm, using local repair techniques, instead of an exhaustive search method. The program has been validated on fifty ""hand-made"" problems, and has succesfully solved the thirteen ""real"" problems.</p>","Computer Science"
"WOS:000072740500027","Analysis of permanent electric dipole moments of aliphatic hydrocarbon molecules. 2. DFT results","1998","
<p>The B3LYP hybrid DFT method was applied to calculate the permanent electric dipole moments of aliphatic hydrocarbon molecules. For most of the molecules, it was found that the B3LYP results are in good agreement with the calculated data published previously and with the experimental values. For molecules that contain conjugate C=C and C equivalent to C bonds, the DFT calculated values are closer to the experimental ones. The situation is the reverse for molecules containing conjugate C=C bonds. The experimental predictions of the dipole moment for 1-buten-3-yne range from 0.22 to 0.44 D, whereas DFT supports a value of similar to 0.39 D.</p>","Computer Science, Information Systems"
"WOS:000072740500027","Analysis of permanent electric dipole moments of aliphatic hydrocarbon molecules. 2. DFT results","1998","
<p>The B3LYP hybrid DFT method was applied to calculate the permanent electric dipole moments of aliphatic hydrocarbon molecules. For most of the molecules, it was found that the B3LYP results are in good agreement with the calculated data published previously and with the experimental values. For molecules that contain conjugate C=C and C equivalent to C bonds, the DFT calculated values are closer to the experimental ones. The situation is the reverse for molecules containing conjugate C=C bonds. The experimental predictions of the dipole moment for 1-buten-3-yne range from 0.22 to 0.44 D, whereas DFT supports a value of similar to 0.39 D.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000072740500027","Analysis of permanent electric dipole moments of aliphatic hydrocarbon molecules. 2. DFT results","1998","
<p>The B3LYP hybrid DFT method was applied to calculate the permanent electric dipole moments of aliphatic hydrocarbon molecules. For most of the molecules, it was found that the B3LYP results are in good agreement with the calculated data published previously and with the experimental values. For molecules that contain conjugate C=C and C equivalent to C bonds, the DFT calculated values are closer to the experimental ones. The situation is the reverse for molecules containing conjugate C=C bonds. The experimental predictions of the dipole moment for 1-buten-3-yne range from 0.22 to 0.44 D, whereas DFT supports a value of similar to 0.39 D.</p>","Computer Science"
"WOS:000074720000040","3D finite element analysis modelling of the arc chamber of a current limiting miniature circuit breaker","1998","
<p>This paper presents the initial stage of a 3D finite element (FE) model of the electromagnetic field in the are chamber of a current limiting miniature circuit breaker (MCB). The final objective of the model is to compute the magnetic forces on the are, and predict the position of the are at a series of discrete time steps. The trajectory of the are calculated from the model will then be compared.with experimental data recorded by a high speed are imaging system (AIS) on a flexible test apparatus (FTA) designed to simulate the operation of a commercial MCB under laboratory conditions. By comparing the FE model of the are behaviour with the actual are images generated from the AIS an insight into the factors governing the motion of the are can be gained. In particular the relative importance of the average flux density across the are chamber is compared with the local flux density distribution at the are roots. An understanding of the influence of the magnetic flux distribution can then be used to improve the magnetic design of the MCB to promote low immobility times and effective current limiting operation.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000074720000040","3D finite element analysis modelling of the arc chamber of a current limiting miniature circuit breaker","1998","
<p>This paper presents the initial stage of a 3D finite element (FE) model of the electromagnetic field in the are chamber of a current limiting miniature circuit breaker (MCB). The final objective of the model is to compute the magnetic forces on the are, and predict the position of the are at a series of discrete time steps. The trajectory of the are calculated from the model will then be compared.with experimental data recorded by a high speed are imaging system (AIS) on a flexible test apparatus (FTA) designed to simulate the operation of a commercial MCB under laboratory conditions. By comparing the FE model of the are behaviour with the actual are images generated from the AIS an insight into the factors governing the motion of the are can be gained. In particular the relative importance of the average flux density across the are chamber is compared with the local flux density distribution at the are roots. An understanding of the influence of the magnetic flux distribution can then be used to improve the magnetic design of the MCB to promote low immobility times and effective current limiting operation.</p>","Computer Science"
"WOS:000075377000007","Local periods and period propagation in one word","1998","
<p>A word of length n over an alphabet A is a sequence a(1)... a(n) of letters of A. It is convenient to consider a ""long enough"" word over A as an infinite right word, that is an infinite sequence a(1)... a(i)... of elements of A. An integer lambda is a period of the word in the interval [j... k] if we have a(i) = a(i+lambda) for those indices i and i +lambda in the considered interval. The period of a word designates its smallest period over its whole length. A point p of a word is the cut (a(1)... a(p+1)...). A non-negative integer lambda is a local period at point p iff i, is a period in the interval [p - lambda + 1... p + lambda]. According to the critical point's theorem [1, 2], the period of a ""long enough (or not)"" word is the maximum of the minimal local periods taken in each point of this word. M.P. Schutzenberger, who was at the origin of the research work on the relations between local periods and periods of a word obtained by concatenation of periodical words, and our ability to characterize its period from the observation of the local period at the concatenation points only. This is the formulation of the unpublished answer we offered him that we suggest here. (C) 1998-Elsevier Science B.V. All rights reserved.</p>","Computer Science, Theory & Methods"
"WOS:000075377000007","Local periods and period propagation in one word","1998","
<p>A word of length n over an alphabet A is a sequence a(1)... a(n) of letters of A. It is convenient to consider a ""long enough"" word over A as an infinite right word, that is an infinite sequence a(1)... a(i)... of elements of A. An integer lambda is a period of the word in the interval [j... k] if we have a(i) = a(i+lambda) for those indices i and i +lambda in the considered interval. The period of a word designates its smallest period over its whole length. A point p of a word is the cut (a(1)... a(p+1)...). A non-negative integer lambda is a local period at point p iff i, is a period in the interval [p - lambda + 1... p + lambda]. According to the critical point's theorem [1, 2], the period of a ""long enough (or not)"" word is the maximum of the minimal local periods taken in each point of this word. M.P. Schutzenberger, who was at the origin of the research work on the relations between local periods and periods of a word obtained by concatenation of periodical words, and our ability to characterize its period from the observation of the local period at the concatenation points only. This is the formulation of the unpublished answer we offered him that we suggest here. (C) 1998-Elsevier Science B.V. All rights reserved.</p>","Computer Science"
"WOS:000082112700016","Computation in recurrent neural networks: From counters to iterated function systems","1998","
<p>In the paper we address the problem of computation in recurrent neural networks (RNN). In the first part we provide a formal analysis of the dynamical behavior of a RNN with a single self-recurrent unit in the hidden layer, show how such a RNN may be designed to perform an (unrestricted) counting task and describe a generalization of the counter network that performs binary stack operations. In the second part of the paper we focus on the analysis of RNNs. We show how a layered RNN can be mapped to a, corresponding iterated function system (IFS) and formulate conditions under which the behavior of the IFS and therefore the behavior of the corresponding RNN can be characterized as the performance of stack operations. This result enables us to analyze any layered RNN in terms of classical computation and, hence, improves our understanding of computation within a broad class of RNNs.</p>
<p>Moreover, we show how to use this knowledge as a design principle for RNNs which implement computational tasks that require stack operations. This principle is exemplified by presenting the design of particular RNNs for the recognition of words within the class of Dyck languages.</p>","Computer Science, Artificial Intelligence"
"WOS:000082112700016","Computation in recurrent neural networks: From counters to iterated function systems","1998","
<p>In the paper we address the problem of computation in recurrent neural networks (RNN). In the first part we provide a formal analysis of the dynamical behavior of a RNN with a single self-recurrent unit in the hidden layer, show how such a RNN may be designed to perform an (unrestricted) counting task and describe a generalization of the counter network that performs binary stack operations. In the second part of the paper we focus on the analysis of RNNs. We show how a layered RNN can be mapped to a, corresponding iterated function system (IFS) and formulate conditions under which the behavior of the IFS and therefore the behavior of the corresponding RNN can be characterized as the performance of stack operations. This result enables us to analyze any layered RNN in terms of classical computation and, hence, improves our understanding of computation within a broad class of RNNs.</p>
<p>Moreover, we show how to use this knowledge as a design principle for RNNs which implement computational tasks that require stack operations. This principle is exemplified by presenting the design of particular RNNs for the recognition of words within the class of Dyck languages.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000082112700016","Computation in recurrent neural networks: From counters to iterated function systems","1998","
<p>In the paper we address the problem of computation in recurrent neural networks (RNN). In the first part we provide a formal analysis of the dynamical behavior of a RNN with a single self-recurrent unit in the hidden layer, show how such a RNN may be designed to perform an (unrestricted) counting task and describe a generalization of the counter network that performs binary stack operations. In the second part of the paper we focus on the analysis of RNNs. We show how a layered RNN can be mapped to a, corresponding iterated function system (IFS) and formulate conditions under which the behavior of the IFS and therefore the behavior of the corresponding RNN can be characterized as the performance of stack operations. This result enables us to analyze any layered RNN in terms of classical computation and, hence, improves our understanding of computation within a broad class of RNNs.</p>
<p>Moreover, we show how to use this knowledge as a design principle for RNNs which implement computational tasks that require stack operations. This principle is exemplified by presenting the design of particular RNNs for the recognition of words within the class of Dyck languages.</p>","Computer Science"
"WOS:000073243400001","A comparison of implicitly parallel multithreaded and data-parallel implementations of an ocean model","1998","
<p>Two parallel implementations of a state-of-the-art ocean model are described and analyzed: one is written in the implicitly parallel language Id for the Monsoon multithreaded dataflow architecture, and the other in data-parallel CM Fortran for the CM-5. The multithreaded programming model is inherently more expressive than the data-parallel model but is not especially adapted to regular data structures common to many scientific codes. One goal of this study is to understand what, if any, are the performance penalties of multithreaded execution when implementing a program that is well suited for data-parallel execution. To avoid technology and machine configuration issues, the two implementations are compared in terms of overhead cycles per required floating point operation. When flows in complex geometries typical of ocean basins are simulated, the data-parallel model only remains efficient if redundant computations are performed over land. The generality of the Id programming model, however, allows one to easily and transparently implement a parallel code that computes only in the ocean. When ocean basins with complex and irregular geometry are simulated the normalized performance on Monsoon is comparable with that of the CM-5. For more regular geometries that map well to the computational domain, the data-parallel approach proves to be a better match. We conclude by examining the extent to which clusters of mainstream symmetric multiprocessor (SMP) systems offer a scientific computing environment which can capitalize on and combine the strengths of the two paradigms. (C) 1998 Academic Press.</p>","Computer Science, Theory & Methods"
"WOS:000073243400001","A comparison of implicitly parallel multithreaded and data-parallel implementations of an ocean model","1998","
<p>Two parallel implementations of a state-of-the-art ocean model are described and analyzed: one is written in the implicitly parallel language Id for the Monsoon multithreaded dataflow architecture, and the other in data-parallel CM Fortran for the CM-5. The multithreaded programming model is inherently more expressive than the data-parallel model but is not especially adapted to regular data structures common to many scientific codes. One goal of this study is to understand what, if any, are the performance penalties of multithreaded execution when implementing a program that is well suited for data-parallel execution. To avoid technology and machine configuration issues, the two implementations are compared in terms of overhead cycles per required floating point operation. When flows in complex geometries typical of ocean basins are simulated, the data-parallel model only remains efficient if redundant computations are performed over land. The generality of the Id programming model, however, allows one to easily and transparently implement a parallel code that computes only in the ocean. When ocean basins with complex and irregular geometry are simulated the normalized performance on Monsoon is comparable with that of the CM-5. For more regular geometries that map well to the computational domain, the data-parallel approach proves to be a better match. We conclude by examining the extent to which clusters of mainstream symmetric multiprocessor (SMP) systems offer a scientific computing environment which can capitalize on and combine the strengths of the two paradigms. (C) 1998 Academic Press.</p>","Computer Science"
"WOS:000073086800001","Spatial resolution of body surface potential maps and magnetic field maps: a simulation study applied to the identification of ventricular pre-excitation sites","1998","
<p>The spatial resolution of body surface potential maps (BSPMs) and magnetic field maps (MFMs) is investigated by means of an anatomically accurate computer model of the human ventricular myocardium. BSPMs and MFMs are calculated for the simulated activation sequences initiated at 35 pre-excitation sites located along the atrioventricular (AV) ring of the epicardium. Changes in the BSPMs and MFMs corresponding to different pre-excitation sites are quantified in terms of the correlation coefficient r. The spatial resolution (selectivity) for a given pre-excitation site is defined as the half-distance between those neighbouring locations at which morphological features of maps, in terms of r, become distinct (r<0.95). It is found that, at 28 ms after the onset of preexcitation and with no noise added, this distance +/-SD, for all sites along the AV ring for the 117-lead BSPMs, is 0.83 +/- 0.32 cm, and for the 64-lead and 128-lead MFMs it is 1.54 +/- 0.84 cm and 115 +/- 0.43 cm, respectively. The findings suggest that, when features of non-invasively recorded electrocardiographic and magnetocardiographic map patterns are used for identifying accessory pathways in patients suffering from WPW syndrome, BSPMs are likely to provide more detailed information for guiding the ablative treatment than MFMs. For some sites MFMs provide more information. Both modalities may provide additional assistance to the cardiologist in locating the site of the accessory pathway.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000073086800001","Spatial resolution of body surface potential maps and magnetic field maps: a simulation study applied to the identification of ventricular pre-excitation sites","1998","
<p>The spatial resolution of body surface potential maps (BSPMs) and magnetic field maps (MFMs) is investigated by means of an anatomically accurate computer model of the human ventricular myocardium. BSPMs and MFMs are calculated for the simulated activation sequences initiated at 35 pre-excitation sites located along the atrioventricular (AV) ring of the epicardium. Changes in the BSPMs and MFMs corresponding to different pre-excitation sites are quantified in terms of the correlation coefficient r. The spatial resolution (selectivity) for a given pre-excitation site is defined as the half-distance between those neighbouring locations at which morphological features of maps, in terms of r, become distinct (r<0.95). It is found that, at 28 ms after the onset of preexcitation and with no noise added, this distance +/-SD, for all sites along the AV ring for the 117-lead BSPMs, is 0.83 +/- 0.32 cm, and for the 64-lead and 128-lead MFMs it is 1.54 +/- 0.84 cm and 115 +/- 0.43 cm, respectively. The findings suggest that, when features of non-invasively recorded electrocardiographic and magnetocardiographic map patterns are used for identifying accessory pathways in patients suffering from WPW syndrome, BSPMs are likely to provide more detailed information for guiding the ablative treatment than MFMs. For some sites MFMs provide more information. Both modalities may provide additional assistance to the cardiologist in locating the site of the accessory pathway.</p>","Computer Science"
"WOS:000077631900007","Dynamic program slicing methods","1998","
<p>A dynamic program slice is that part of a program that ""affects"" the computation of a variable of interest during program execution on a specific program input. Dynamic program slicing refers to a collection of program slicing methods that are based on program execution and may significantly reduce the size of a program slice because run-time information, collected during program execution, is used to compute program slices. Dynamic program slicing was originally proposed only for program debugging, but its application has been extended to program comprehension, software testing, and software maintenance. Different types of dynamic program slices, together with algorithms to compute them, have been proposed in the literature. In this paper we present a classification of existing dynamic slicing methods and discuss the algorithms to compute dynamic slices. In the second part of the paper, we compare the existing methods of dynamic slice computation. (C) 1998 Elsevier Science BN. All rights reserved.</p>","Computer Science, Information Systems"
"WOS:000077631900007","Dynamic program slicing methods","1998","
<p>A dynamic program slice is that part of a program that ""affects"" the computation of a variable of interest during program execution on a specific program input. Dynamic program slicing refers to a collection of program slicing methods that are based on program execution and may significantly reduce the size of a program slice because run-time information, collected during program execution, is used to compute program slices. Dynamic program slicing was originally proposed only for program debugging, but its application has been extended to program comprehension, software testing, and software maintenance. Different types of dynamic program slices, together with algorithms to compute them, have been proposed in the literature. In this paper we present a classification of existing dynamic slicing methods and discuss the algorithms to compute dynamic slices. In the second part of the paper, we compare the existing methods of dynamic slice computation. (C) 1998 Elsevier Science BN. All rights reserved.</p>","Computer Science, Software Engineering"
"WOS:000077631900007","Dynamic program slicing methods","1998","
<p>A dynamic program slice is that part of a program that ""affects"" the computation of a variable of interest during program execution on a specific program input. Dynamic program slicing refers to a collection of program slicing methods that are based on program execution and may significantly reduce the size of a program slice because run-time information, collected during program execution, is used to compute program slices. Dynamic program slicing was originally proposed only for program debugging, but its application has been extended to program comprehension, software testing, and software maintenance. Different types of dynamic program slices, together with algorithms to compute them, have been proposed in the literature. In this paper we present a classification of existing dynamic slicing methods and discuss the algorithms to compute dynamic slices. In the second part of the paper, we compare the existing methods of dynamic slice computation. (C) 1998 Elsevier Science BN. All rights reserved.</p>","Computer Science"
"WOS:000076675200005","Case study investigating the application of neural networks for process modelling and condition monitoring","1998","
<p>This paper presents two practical applications where artificial neural networks have been used to solve difficult process engineering problems. Firstly, the ability of artificial neural networks to provide an accurate process model of a vitrification process is demonstrated on real-process data. Vitrification is a process that encapsulates highly active liquid waste in glass to provide a safe and convenient method of storage. The second application again employs artificial neural networks, but this time they are applied in a novel way in which they are used to capture non-linear system characteristics and then recalled to provide a means of detecting imminent failure of a vessel used in the same vitrification process (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000076675200005","Case study investigating the application of neural networks for process modelling and condition monitoring","1998","
<p>This paper presents two practical applications where artificial neural networks have been used to solve difficult process engineering problems. Firstly, the ability of artificial neural networks to provide an accurate process model of a vitrification process is demonstrated on real-process data. Vitrification is a process that encapsulates highly active liquid waste in glass to provide a safe and convenient method of storage. The second application again employs artificial neural networks, but this time they are applied in a novel way in which they are used to capture non-linear system characteristics and then recalled to provide a means of detecting imminent failure of a vessel used in the same vitrification process (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science"
"WOS:000076121300001","Denotational semantics of object specification","1998","
<p>From an arbitrary temporal logic institution we show how to set up the corresponding institution of objects. The main properties of the resulting institution are studied and used in establishing a categorial, denotational semantics of several basic constructs of object specification, namely aggregation (parallel composition), interconnection, abstraction (interfacing) and monotonic specialization. A duality is established between the category of theories and the category of objects, as a corollary of the Galois correspondence between these concrete categories. The special case of linear temporal logic is analysed in detail in order to show that categorial products do reflect interleaving and reducts may lead to internal non-determinism.</p>","Computer Science, Information Systems"
"WOS:000076121300001","Denotational semantics of object specification","1998","
<p>From an arbitrary temporal logic institution we show how to set up the corresponding institution of objects. The main properties of the resulting institution are studied and used in establishing a categorial, denotational semantics of several basic constructs of object specification, namely aggregation (parallel composition), interconnection, abstraction (interfacing) and monotonic specialization. A duality is established between the category of theories and the category of objects, as a corollary of the Galois correspondence between these concrete categories. The special case of linear temporal logic is analysed in detail in order to show that categorial products do reflect interleaving and reducts may lead to internal non-determinism.</p>","Computer Science"
"WOS:000077431600021","Skin substitutes from cultured cells and collagen-GAG polymers","1998","
<p>Engineering skin substitutes provides a potential source of advanced therapies for the treatment of acute and chronic wounds. Cultured skin substitutes (CSS) consisting of human keratinocytes and fibroblasts attached to collagen-glycosaminoglycan substrates have been designed and tested in preclinical and clinical studies. Cell culture techniques follow general principles of primary culture and cryopreservation in liquid nitrogen for long-term storage. Biopolymer substrates are fabricated from xenogeneic (bovine) collagen and glycosaminoglycan that are lyophilised for storage until use. At maturity in air-exposed culture, CSS develop an epidermal barrier that is not statistically different from native human skin, as measured by surface electrical capacitance. Preclinical studies in athymic mice show rapid healing, expression of cytokines and regulation of pigmentation. Clinical studies in burn patients demonstrate a qualitative outcome with autologous skin that is not different from 1:4 meshed, split-thickness autograft skin, and with a quantitative advantage over autograft skin in the ratio of healed skin to biopsy areas. Chronic wounds resulting from diabetes or venous stasis have been closed successfully with allogeneic CSS prepared from cryopreserved skin cells. These results define the therapeutic benefits of cultured skin substitutes prepared with skin cells from the patient or from cadaver donors. Future directions include genetic modification of transplanted cells to improve wound healing transiently or to deliver gene products systemically.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000077431600021","Skin substitutes from cultured cells and collagen-GAG polymers","1998","
<p>Engineering skin substitutes provides a potential source of advanced therapies for the treatment of acute and chronic wounds. Cultured skin substitutes (CSS) consisting of human keratinocytes and fibroblasts attached to collagen-glycosaminoglycan substrates have been designed and tested in preclinical and clinical studies. Cell culture techniques follow general principles of primary culture and cryopreservation in liquid nitrogen for long-term storage. Biopolymer substrates are fabricated from xenogeneic (bovine) collagen and glycosaminoglycan that are lyophilised for storage until use. At maturity in air-exposed culture, CSS develop an epidermal barrier that is not statistically different from native human skin, as measured by surface electrical capacitance. Preclinical studies in athymic mice show rapid healing, expression of cytokines and regulation of pigmentation. Clinical studies in burn patients demonstrate a qualitative outcome with autologous skin that is not different from 1:4 meshed, split-thickness autograft skin, and with a quantitative advantage over autograft skin in the ratio of healed skin to biopsy areas. Chronic wounds resulting from diabetes or venous stasis have been closed successfully with allogeneic CSS prepared from cryopreserved skin cells. These results define the therapeutic benefits of cultured skin substitutes prepared with skin cells from the patient or from cadaver donors. Future directions include genetic modification of transplanted cells to improve wound healing transiently or to deliver gene products systemically.</p>","Computer Science"
"WOS:000074454200004","Distributed concurrency control with local wait-depth control policy","1998","
<p>Parallel Transaction Processing (TP) systems have great potential to serve the ever-increasing demands for high transaction processing rate. This potential, however, may not be reached due to the data contention and the widely-used two-phase locking (2PL) Concurrency Control (CC) method. In this paper, a distributed locking-based CC policy called LWDC (Local Wait-Depth Control) was proposed for dealing with this problem for the shared-nothing parallel TP system. On the basis of the LWDC policy, an algorithm called LWDCk was designed. Using simulation LWDCk was compared with the 2PL and the base-line Distributed Wait-Depth Limited (DWDL) CC methods. Simulation studies show that the new algorithm offers better system performance than those compared.</p>","Computer Science, Information Systems"
"WOS:000074454200004","Distributed concurrency control with local wait-depth control policy","1998","
<p>Parallel Transaction Processing (TP) systems have great potential to serve the ever-increasing demands for high transaction processing rate. This potential, however, may not be reached due to the data contention and the widely-used two-phase locking (2PL) Concurrency Control (CC) method. In this paper, a distributed locking-based CC policy called LWDC (Local Wait-Depth Control) was proposed for dealing with this problem for the shared-nothing parallel TP system. On the basis of the LWDC policy, an algorithm called LWDCk was designed. Using simulation LWDCk was compared with the 2PL and the base-line Distributed Wait-Depth Limited (DWDL) CC methods. Simulation studies show that the new algorithm offers better system performance than those compared.</p>","Computer Science, Software Engineering"
"WOS:000074454200004","Distributed concurrency control with local wait-depth control policy","1998","
<p>Parallel Transaction Processing (TP) systems have great potential to serve the ever-increasing demands for high transaction processing rate. This potential, however, may not be reached due to the data contention and the widely-used two-phase locking (2PL) Concurrency Control (CC) method. In this paper, a distributed locking-based CC policy called LWDC (Local Wait-Depth Control) was proposed for dealing with this problem for the shared-nothing parallel TP system. On the basis of the LWDC policy, an algorithm called LWDCk was designed. Using simulation LWDCk was compared with the 2PL and the base-line Distributed Wait-Depth Limited (DWDL) CC methods. Simulation studies show that the new algorithm offers better system performance than those compared.</p>","Computer Science"
"WOS:000078927600028","Quorum-based secure multi-party computation","1998","
<p>This paper describes efficient protocols for multi-party computations that are information-theoretically secure against passive attacks. The results presented here apply to access structures based on quorum systems, which are collections of sets enjoying a naturally-motivated self-intersection property. Quorum-based access structures include threshold systems but are far richer and more general, and they have specific applicability to several problems in distributed control and management. The achievable limits of security in quorum-based multi-party computation are shown to be equivalent to those determined by Hirt and Maurer in [HM97], drawing a natural but non-obvious connection between quorum systems and the extremes of secure multi-party computation. Moreover, for both the general case and for specific applications, the protocols presented here are simpler and more efficient.</p>","Computer Science, Theory & Methods"
"WOS:000078927600028","Quorum-based secure multi-party computation","1998","
<p>This paper describes efficient protocols for multi-party computations that are information-theoretically secure against passive attacks. The results presented here apply to access structures based on quorum systems, which are collections of sets enjoying a naturally-motivated self-intersection property. Quorum-based access structures include threshold systems but are far richer and more general, and they have specific applicability to several problems in distributed control and management. The achievable limits of security in quorum-based multi-party computation are shown to be equivalent to those determined by Hirt and Maurer in [HM97], drawing a natural but non-obvious connection between quorum systems and the extremes of secure multi-party computation. Moreover, for both the general case and for specific applications, the protocols presented here are simpler and more efficient.</p>","Computer Science"
"WOS:000072663300003","Xception: A technique for the experimental evaluation of dependability in modern computers","1998","
<p>An important step in the development of dependable systems is the validation of their fault tolerance properties. Fault injection has been widely used for this purpose, however with the rapid increase in processor complexity, traditional techniques are also increasingly more difficult to apply. This paper presents a new software implemented fault injection and monitoring environment, called Xception, which is targeted for the modern and complex processors. Xception uses the advanced debugging and performance monitoring features existing in most of the modern processors to inject quite realistic faults by software, and to monitor the activation of the faults and their impact on the target system behavior in detail. Faults are injected with minimum interference with the target application. The target application is not modified, no software traps are inserted, and it is not necessary to execute the target application in special trace mode (the application is executed at full speed). Xception provides a comprehensive set of fault triggers, including spatial and temporal fault triggers, and triggers related to the manipulation of data in memory. Faults injected by Xception can affect any process running on the target system (including the kernel), and it is possible to inject faults in applications for which the source code is not available. Experimental results are presented to demonstrate the accuracy and potential of Xception in the evaluation of the dependability properties of the complex computer systems available nowadays.</p>","Computer Science, Software Engineering"
"WOS:000072663300003","Xception: A technique for the experimental evaluation of dependability in modern computers","1998","
<p>An important step in the development of dependable systems is the validation of their fault tolerance properties. Fault injection has been widely used for this purpose, however with the rapid increase in processor complexity, traditional techniques are also increasingly more difficult to apply. This paper presents a new software implemented fault injection and monitoring environment, called Xception, which is targeted for the modern and complex processors. Xception uses the advanced debugging and performance monitoring features existing in most of the modern processors to inject quite realistic faults by software, and to monitor the activation of the faults and their impact on the target system behavior in detail. Faults are injected with minimum interference with the target application. The target application is not modified, no software traps are inserted, and it is not necessary to execute the target application in special trace mode (the application is executed at full speed). Xception provides a comprehensive set of fault triggers, including spatial and temporal fault triggers, and triggers related to the manipulation of data in memory. Faults injected by Xception can affect any process running on the target system (including the kernel), and it is possible to inject faults in applications for which the source code is not available. Experimental results are presented to demonstrate the accuracy and potential of Xception in the evaluation of the dependability properties of the complex computer systems available nowadays.</p>","Computer Science"
"WOS:000080459500004","Incorporating turbulence models into the lattice-Boltzmann method","1998","
<p>The Lattice-Boltzmann method (LBM) is extended to allow incorporation of traditional turbulence models. Implementation of a two-layer mixing-length algebraic model and two versions of the k - epsilon two-equation model, Standard and RNG, in conjunction with a wall model, are presented. Validation studies are done for turbulent Bows in a straight pipe at three Re numbers and over a backwards facing step of expansion ratio 1.5 and Re-H = 44 000. All model produce good agreement with experiment for the straight pipes but the RNG k - epsilon model is best able to capture both the recirculation length, within 2% of experiment, and the detailed structure of the mean fluid flow for the backwards facing step.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000080459500004","Incorporating turbulence models into the lattice-Boltzmann method","1998","
<p>The Lattice-Boltzmann method (LBM) is extended to allow incorporation of traditional turbulence models. Implementation of a two-layer mixing-length algebraic model and two versions of the k - epsilon two-equation model, Standard and RNG, in conjunction with a wall model, are presented. Validation studies are done for turbulent Bows in a straight pipe at three Re numbers and over a backwards facing step of expansion ratio 1.5 and Re-H = 44 000. All model produce good agreement with experiment for the straight pipes but the RNG k - epsilon model is best able to capture both the recirculation length, within 2% of experiment, and the detailed structure of the mean fluid flow for the backwards facing step.</p>","Computer Science"
"WOS:000075539300001","Analytical mean squared error curves for temporal difference learning","1998","
<p>We provide analytical expressions governing changes to the bias and variance of the lookup table estimators provided by various Monte Carlo and temporal difference value estimation algorithms with offline updates over trials in absorbing Markov reward processes. We have used these expressions to develop software that serves as an analysis tool: given a complete description of a Markov reward process, it rapidly yields an exact mean-square-error curve, the curve one would get from averaging together sample mean-square-error curves from an infinite number of learning trials on the given problem. We use our analysis tool to illustrate classes of mean-square-error curve behavior in a variety of example reward processes, and we show that although the various temporal difference algorithms are quite sensitive to the choice of step-size and eligibility-trace parameters, there are values of these parameters that make them similarly competent, and generally good.</p>","Computer Science, Artificial Intelligence"
"WOS:000075539300001","Analytical mean squared error curves for temporal difference learning","1998","
<p>We provide analytical expressions governing changes to the bias and variance of the lookup table estimators provided by various Monte Carlo and temporal difference value estimation algorithms with offline updates over trials in absorbing Markov reward processes. We have used these expressions to develop software that serves as an analysis tool: given a complete description of a Markov reward process, it rapidly yields an exact mean-square-error curve, the curve one would get from averaging together sample mean-square-error curves from an infinite number of learning trials on the given problem. We use our analysis tool to illustrate classes of mean-square-error curve behavior in a variety of example reward processes, and we show that although the various temporal difference algorithms are quite sensitive to the choice of step-size and eligibility-trace parameters, there are values of these parameters that make them similarly competent, and generally good.</p>","Computer Science"
"WOS:000075623500004","A framework for describing visual interfaces to databases","1998","
<p>In the field of HCI there exist many formalisms for analysing, describing and evaluating interactive systems. However, in developing and evaluating user interfaces to databases, we found it necessary to be able to describe presentation and interaction aspects that are catered for poorly or not at all. in current formalisms. This paper presents a framework for the systematic description of data model, presentation and interaction components that together form a graphical user interface. The utility of the framework is then demonstrated by showing how it can be used to describe two existing visual query interfaces. These examples show that the framework provides a systematic method for the concise description of graphical interfaces to databases that can be used either during interface design or as a communication aid. (C) 1998 Academic Press.</p>","Computer Science, Software Engineering"
"WOS:000075623500004","A framework for describing visual interfaces to databases","1998","
<p>In the field of HCI there exist many formalisms for analysing, describing and evaluating interactive systems. However, in developing and evaluating user interfaces to databases, we found it necessary to be able to describe presentation and interaction aspects that are catered for poorly or not at all. in current formalisms. This paper presents a framework for the systematic description of data model, presentation and interaction components that together form a graphical user interface. The utility of the framework is then demonstrated by showing how it can be used to describe two existing visual query interfaces. These examples show that the framework provides a systematic method for the concise description of graphical interfaces to databases that can be used either during interface design or as a communication aid. (C) 1998 Academic Press.</p>","Computer Science"
"WOS:000075734200001","The application of three approximate free energy calculations methods to structure based ligand design: Trypsin and its complex with inhibitors","1998","
<p>Three approximate free energy calculation methods are examined and applied to an example ligand design problem. The first of the methods uses a single simulation to estimate the relative binding free energies for related ligands that are not simulated. The second method is similar, except that it uses only first derivatives of free energy with respect to atomic parameters (most often charge, van der Waals equilibrium distance, and van der Waals well depth) to calculate free energy differences. The last method PROFEC (Pictorial Representation of Free Energy Components), generates contour maps that show how binding free energy changes when additional particles are added near the ligand. These three methods are applied to a benzamidine/trypsin complex. They each reproduce the general trends in the binding free energies, indicating that they might be useful for suggesting how ligands could be modified to improve binding and, consequently, useful in structure-based drug design.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000075734200001","The application of three approximate free energy calculations methods to structure based ligand design: Trypsin and its complex with inhibitors","1998","
<p>Three approximate free energy calculation methods are examined and applied to an example ligand design problem. The first of the methods uses a single simulation to estimate the relative binding free energies for related ligands that are not simulated. The second method is similar, except that it uses only first derivatives of free energy with respect to atomic parameters (most often charge, van der Waals equilibrium distance, and van der Waals well depth) to calculate free energy differences. The last method PROFEC (Pictorial Representation of Free Energy Components), generates contour maps that show how binding free energy changes when additional particles are added near the ligand. These three methods are applied to a benzamidine/trypsin complex. They each reproduce the general trends in the binding free energies, indicating that they might be useful for suggesting how ligands could be modified to improve binding and, consequently, useful in structure-based drug design.</p>","Computer Science"
"WOS:000072981400006","Overcoming the barriers: National to European to G7","1998","
<p>An information revolution is underway which will have an impact on all sectors of society. It will fundamentally change national and international health systems. The global Internet is a key influence and will change the balance of power within and between healthcare professions, and between them and the general public. This revolution offers enormous potential benefits to global health but there is also potential for harm. There are a wide range of barriers to realising the potential benefits. They lie in areas such as the protection of personal information; ownership and legal accountability; data meanings; structures and database navigation; deficiencies in the global Internet and lack of access by many communities. This paper considers the nature of those barriers. In 1994, the Group of Seven Nations launched an initiative to stimulate a global information society. Theme 8 deals with healthcare and therein Sub-project 5 'Enabling Mechanisms', which the author leads, is seeking to identify barriers and the authoritative international sources of advise and good practice. It is conducting an international survey, the results of which should be published by the end of 1997. This paper describes the aims of this Sub-project. (C) 1998 Elsevier Science Ireland Ltd. All rights reserved.</p>","Computer Science, Information Systems"
"WOS:000072981400006","Overcoming the barriers: National to European to G7","1998","
<p>An information revolution is underway which will have an impact on all sectors of society. It will fundamentally change national and international health systems. The global Internet is a key influence and will change the balance of power within and between healthcare professions, and between them and the general public. This revolution offers enormous potential benefits to global health but there is also potential for harm. There are a wide range of barriers to realising the potential benefits. They lie in areas such as the protection of personal information; ownership and legal accountability; data meanings; structures and database navigation; deficiencies in the global Internet and lack of access by many communities. This paper considers the nature of those barriers. In 1994, the Group of Seven Nations launched an initiative to stimulate a global information society. Theme 8 deals with healthcare and therein Sub-project 5 'Enabling Mechanisms', which the author leads, is seeking to identify barriers and the authoritative international sources of advise and good practice. It is conducting an international survey, the results of which should be published by the end of 1997. This paper describes the aims of this Sub-project. (C) 1998 Elsevier Science Ireland Ltd. All rights reserved.</p>","Computer Science"
"WOS:000074119100001","Evaluations of fractal geometry and invariant moments for shape classification of corn germplasm","1998","
<p>Computer vision-based techniques were developed and evaluated for classifying different shapes of germplasms tear of corn). An algorithm was developed to discriminate round-shaped germplasms based on two features, i.e. circularity and dimensional ratio. Two different approaches based on fractal geometry and higher order invariant moments were used for classification of non-round shaped germplasms. In the fractal-based approach, two additional fractal geometry-based features (i.e. fractal-shape factor and fractal perimeter) were developed and used with fractal dimension and aspect ratio to represent the shape features of the germplasms. Classifications rules based on modified Euclidean measures and distance weighted K-nearest neighborhood were used for classifying the germplasms into one of three non-round-shape classes (cylindrical, cylindrical-conical and conical). Though the overall correspondence for classifying non-round germplasms was 60% (based on 80 samples), a maximum correspondence of 80% could be obtained for classifying cylindrical germplasms (based on 18 samples). Neither method could provide similar classification correspondence for cylindrical-conical germplasms, On the other hand, these methods, however, showed a correspondence of 82.5% for classifying non-round corn germplasms into cylindrical and non-cylindrical (conical and cylindrical-conical) shapes. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000074119100001","Evaluations of fractal geometry and invariant moments for shape classification of corn germplasm","1998","
<p>Computer vision-based techniques were developed and evaluated for classifying different shapes of germplasms tear of corn). An algorithm was developed to discriminate round-shaped germplasms based on two features, i.e. circularity and dimensional ratio. Two different approaches based on fractal geometry and higher order invariant moments were used for classification of non-round shaped germplasms. In the fractal-based approach, two additional fractal geometry-based features (i.e. fractal-shape factor and fractal perimeter) were developed and used with fractal dimension and aspect ratio to represent the shape features of the germplasms. Classifications rules based on modified Euclidean measures and distance weighted K-nearest neighborhood were used for classifying the germplasms into one of three non-round-shape classes (cylindrical, cylindrical-conical and conical). Though the overall correspondence for classifying non-round germplasms was 60% (based on 80 samples), a maximum correspondence of 80% could be obtained for classifying cylindrical germplasms (based on 18 samples). Neither method could provide similar classification correspondence for cylindrical-conical germplasms, On the other hand, these methods, however, showed a correspondence of 82.5% for classifying non-round corn germplasms into cylindrical and non-cylindrical (conical and cylindrical-conical) shapes. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science"
"WOS:000077612700010","The optimal determination of the space requirement and the number of transfer cranes for import containers","1998","
<p>It is discussed how to determine the optimal amount of storage space and the optimal number of transfer cranes for import containers. A cost model is developed for the decision making. It includes the space cost, the fixed cost of transfer cranes which corresponds to the investment cost, the variable cost of transfer cranes and outside trucks which is related to the time spent for the transfer of containers. A simple solution procedure for the optimal solution is provided. The solution procedure is illustrated using a numerical example. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000077612700010","The optimal determination of the space requirement and the number of transfer cranes for import containers","1998","
<p>It is discussed how to determine the optimal amount of storage space and the optimal number of transfer cranes for import containers. A cost model is developed for the decision making. It includes the space cost, the fixed cost of transfer cranes which corresponds to the investment cost, the variable cost of transfer cranes and outside trucks which is related to the time spent for the transfer of containers. A simple solution procedure for the optimal solution is provided. The solution procedure is illustrated using a numerical example. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science"
"WOS:000077926000003","A decision support system for sales forecasting through fuzzy neural networks with asymmetric fuzzy weights","1998","
<p>Sales forecasting plays a very prominent role in business strategy. Numerous investigations addressing this problem have generally employed statistical methods, such as regression or autoregressive and moving average (ARMA). However, sales forecasting is very complicated owing to influence by internal and external environments. Recently, artificial neural networks (ANNs) have also been applied in sales forecasting since their promising performances in the areas of control and pattern recognition. However, further improvement is still necessary since unique circumstances, e.g., promotion, cause a sudden change in the sales pattern. Thus, this study utilizes fuzzy logic a proposed fuzzy neural network (FNN) for the sake of learning fuzzy IF-THEN rules obtained from the marketing experts with respect to promotion. The result from FNN is further integrated with the forecast from ANN using the time series data and the promotion length through the other ANN. Model evaluation results indicate that the proposed system can more accurately perform than the conventional statistical method and single ANN. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science, Artificial Intelligence"
"WOS:000077926000003","A decision support system for sales forecasting through fuzzy neural networks with asymmetric fuzzy weights","1998","
<p>Sales forecasting plays a very prominent role in business strategy. Numerous investigations addressing this problem have generally employed statistical methods, such as regression or autoregressive and moving average (ARMA). However, sales forecasting is very complicated owing to influence by internal and external environments. Recently, artificial neural networks (ANNs) have also been applied in sales forecasting since their promising performances in the areas of control and pattern recognition. However, further improvement is still necessary since unique circumstances, e.g., promotion, cause a sudden change in the sales pattern. Thus, this study utilizes fuzzy logic a proposed fuzzy neural network (FNN) for the sake of learning fuzzy IF-THEN rules obtained from the marketing experts with respect to promotion. The result from FNN is further integrated with the forecast from ANN using the time series data and the promotion length through the other ANN. Model evaluation results indicate that the proposed system can more accurately perform than the conventional statistical method and single ANN. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science, Information Systems"
"WOS:000077926000003","A decision support system for sales forecasting through fuzzy neural networks with asymmetric fuzzy weights","1998","
<p>Sales forecasting plays a very prominent role in business strategy. Numerous investigations addressing this problem have generally employed statistical methods, such as regression or autoregressive and moving average (ARMA). However, sales forecasting is very complicated owing to influence by internal and external environments. Recently, artificial neural networks (ANNs) have also been applied in sales forecasting since their promising performances in the areas of control and pattern recognition. However, further improvement is still necessary since unique circumstances, e.g., promotion, cause a sudden change in the sales pattern. Thus, this study utilizes fuzzy logic a proposed fuzzy neural network (FNN) for the sake of learning fuzzy IF-THEN rules obtained from the marketing experts with respect to promotion. The result from FNN is further integrated with the forecast from ANN using the time series data and the promotion length through the other ANN. Model evaluation results indicate that the proposed system can more accurately perform than the conventional statistical method and single ANN. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science"
"WOS:000072546000006","Time in a causal theory","1998","
<p>We present a causal theory based on an interventionist conception of causality, i.e., a preference to select causes among a set of actions which an agent has the ability to perform or not (free will). Emphasis is put on the temporal and explanatory aspects of causal reasoning. We introduce a formal framework enabling to define the notion of voluntary cause in a way allowing for an effective retrieval of causes in a given situation. The causal knowledge is represented by causal rules of two kinds: strict and ""normal"". The latter is based on the notions of preferred time lines (futures that the agent normally has in mind when (s)he opts for performing the action) and of inhibiting events (the occurrence of which prevents the anticipated effect to happen). A situation is described by a set of events occurring on time lines; this description is completed by default assumptions (when an agent performs an action, we assume, unless this is inconsistent, that its preconditions are fulfilled and that no inhibiting event will take place). An example is presented, extension to first-order is briefly discussed, and our approach is compared to related works.</p>","Computer Science, Artificial Intelligence"
"WOS:000072546000006","Time in a causal theory","1998","
<p>We present a causal theory based on an interventionist conception of causality, i.e., a preference to select causes among a set of actions which an agent has the ability to perform or not (free will). Emphasis is put on the temporal and explanatory aspects of causal reasoning. We introduce a formal framework enabling to define the notion of voluntary cause in a way allowing for an effective retrieval of causes in a given situation. The causal knowledge is represented by causal rules of two kinds: strict and ""normal"". The latter is based on the notions of preferred time lines (futures that the agent normally has in mind when (s)he opts for performing the action) and of inhibiting events (the occurrence of which prevents the anticipated effect to happen). A situation is described by a set of events occurring on time lines; this description is completed by default assumptions (when an agent performs an action, we assume, unless this is inconsistent, that its preconditions are fulfilled and that no inhibiting event will take place). An example is presented, extension to first-order is briefly discussed, and our approach is compared to related works.</p>","Computer Science"
"WOS:000072217300020","Behavioral-level synthesis of heterogeneous BISR reconfigurable ASIC's","1998","
<p>In this paper, behavioral-level synthesis techniques are presented for the design of reconfigurable hardware. The techniques are applicable for synthesis of several classes of designs, including 1) design for fault tolerance against permanent faults, 2) design for improved manufacturability, and 3) design of application specific programmable processors (ASPP's)-processors designed to perform any computation from a specified set on a single implementation platform, This paper focuses on design techniques for efficient built-in self-repair (BISR), and thus directly addresses the former two applications. Previous BISR techniques have been based on replacing a failed module with a backup of the same type, We present new heterogeneous BISR methodologies which remove this constraint and enable replacement of a module with a spare of a different type. The approach is based on the flexibility of behavioral-level synthesis to explore the design space, Two behavioral synthesis techniques are developed; the first method is through assignment and scheduling, and the second utilizes transformations. Experimental results verify the effectiveness of the approaches.</p>","Computer Science, Hardware & Architecture"
"WOS:000072217300020","Behavioral-level synthesis of heterogeneous BISR reconfigurable ASIC's","1998","
<p>In this paper, behavioral-level synthesis techniques are presented for the design of reconfigurable hardware. The techniques are applicable for synthesis of several classes of designs, including 1) design for fault tolerance against permanent faults, 2) design for improved manufacturability, and 3) design of application specific programmable processors (ASPP's)-processors designed to perform any computation from a specified set on a single implementation platform, This paper focuses on design techniques for efficient built-in self-repair (BISR), and thus directly addresses the former two applications. Previous BISR techniques have been based on replacing a failed module with a backup of the same type, We present new heterogeneous BISR methodologies which remove this constraint and enable replacement of a module with a spare of a different type. The approach is based on the flexibility of behavioral-level synthesis to explore the design space, Two behavioral synthesis techniques are developed; the first method is through assignment and scheduling, and the second utilizes transformations. Experimental results verify the effectiveness of the approaches.</p>","Computer Science"
"WOS:000071856900010","Neural network models for initial public offerings","1998","
<p>In this article, we construct models that predict the first-day return of an initial public offering. Our data set consists of the first-day returns for 1075 firms that went public between 1989 and 1994 and information that we gathered on 16 predictor variables. We segment the data set into technology and nontechnology offerings and construct three types of models for each segment - a regression model and two neural network models. Factorial experiments are used to construct the neural network models. We find that the neural network models perform well on both types of offerings. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science, Artificial Intelligence"
"WOS:000071856900010","Neural network models for initial public offerings","1998","
<p>In this article, we construct models that predict the first-day return of an initial public offering. Our data set consists of the first-day returns for 1075 firms that went public between 1989 and 1994 and information that we gathered on 16 predictor variables. We segment the data set into technology and nontechnology offerings and construct three types of models for each segment - a regression model and two neural network models. Factorial experiments are used to construct the neural network models. We find that the neural network models perform well on both types of offerings. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science"
"WOS:000076435700004","A critique of the reliability-analysis-center failure-rate-model for plastic encapsulated microcircuits","1998","
<p>As the use of plastic encapsulated microcircuits (PEM) has escalated in the electronics industry, especially in applications in which ""mission"" success must be accomplished at ""any cost"", some researchers have proposed reliability models to predict their failure rates or mean times-to-failure. One organization that took on such a challenge was the US Dep't of Defense (DoD) Reliability Analysis Center (RAC). Despite it admirable aims, the model has inherent limitations which result in unrealistic predictions, which could negatively affect parts selection, environmental management, and logistics support. This paper presents an analysis of the model developed by the RAC.</p>
<p>The RAC model for the failure rate of PEM, while having laudable aims, falls into the trap of attempting to be all-embracing without recognizing its limitations or misleading consequences. Although the model quite reasonably includes variables to cope with some operational and environmental conditions, the curve-fitting approach is too remote from physics-of-failure since it does not provide any physical explanation or justification of its terms. The postulate that non-selected manufacturers have higher failure rates than those selected by the RAC is also contrary to the tenets of best commercial practice.</p>
<p>It is probably unwise to attempt an all-embracing model since the full range of variables cannot sensibly be accounted for without using so many qualifying conditions that the model becomes impractical. Instead it would be wise to develop a consortium of models which can combine in a series of simultaneous equations the interdependencies of temperature and relative humidity (RH), dust and temperature, dust and RH, vibration and temperature, materials and geometries, bias and geometries, manufacturing process control and duty cycle, etc.</p>","Computer Science, Hardware & Architecture"
"WOS:000076435700004","A critique of the reliability-analysis-center failure-rate-model for plastic encapsulated microcircuits","1998","
<p>As the use of plastic encapsulated microcircuits (PEM) has escalated in the electronics industry, especially in applications in which ""mission"" success must be accomplished at ""any cost"", some researchers have proposed reliability models to predict their failure rates or mean times-to-failure. One organization that took on such a challenge was the US Dep't of Defense (DoD) Reliability Analysis Center (RAC). Despite it admirable aims, the model has inherent limitations which result in unrealistic predictions, which could negatively affect parts selection, environmental management, and logistics support. This paper presents an analysis of the model developed by the RAC.</p>
<p>The RAC model for the failure rate of PEM, while having laudable aims, falls into the trap of attempting to be all-embracing without recognizing its limitations or misleading consequences. Although the model quite reasonably includes variables to cope with some operational and environmental conditions, the curve-fitting approach is too remote from physics-of-failure since it does not provide any physical explanation or justification of its terms. The postulate that non-selected manufacturers have higher failure rates than those selected by the RAC is also contrary to the tenets of best commercial practice.</p>
<p>It is probably unwise to attempt an all-embracing model since the full range of variables cannot sensibly be accounted for without using so many qualifying conditions that the model becomes impractical. Instead it would be wise to develop a consortium of models which can combine in a series of simultaneous equations the interdependencies of temperature and relative humidity (RH), dust and temperature, dust and RH, vibration and temperature, materials and geometries, bias and geometries, manufacturing process control and duty cycle, etc.</p>","Computer Science, Software Engineering"
"WOS:000076435700004","A critique of the reliability-analysis-center failure-rate-model for plastic encapsulated microcircuits","1998","
<p>As the use of plastic encapsulated microcircuits (PEM) has escalated in the electronics industry, especially in applications in which ""mission"" success must be accomplished at ""any cost"", some researchers have proposed reliability models to predict their failure rates or mean times-to-failure. One organization that took on such a challenge was the US Dep't of Defense (DoD) Reliability Analysis Center (RAC). Despite it admirable aims, the model has inherent limitations which result in unrealistic predictions, which could negatively affect parts selection, environmental management, and logistics support. This paper presents an analysis of the model developed by the RAC.</p>
<p>The RAC model for the failure rate of PEM, while having laudable aims, falls into the trap of attempting to be all-embracing without recognizing its limitations or misleading consequences. Although the model quite reasonably includes variables to cope with some operational and environmental conditions, the curve-fitting approach is too remote from physics-of-failure since it does not provide any physical explanation or justification of its terms. The postulate that non-selected manufacturers have higher failure rates than those selected by the RAC is also contrary to the tenets of best commercial practice.</p>
<p>It is probably unwise to attempt an all-embracing model since the full range of variables cannot sensibly be accounted for without using so many qualifying conditions that the model becomes impractical. Instead it would be wise to develop a consortium of models which can combine in a series of simultaneous equations the interdependencies of temperature and relative humidity (RH), dust and temperature, dust and RH, vibration and temperature, materials and geometries, bias and geometries, manufacturing process control and duty cycle, etc.</p>","Computer Science"
"WOS:000076029400001","Error-resilient pyramid vector quantization for image compression","1998","
<p>Pyramid vector quantization (PVQ) uses the lattice points of a pyramidal shape in multidimensional space as the quantizer codebook, It is a fixed-rate quantization technique that can be used for the compression of Laplacian-like sources arising from transform and subband image coding, where its performance approaches the optimal entropy-coded scalar quantizer without the necessity of variable length codes. In this paper, we investigate the use of PVQ for compressed image transmission over noisy channels, where the fixed-rate quantization seduces the susceptibility to bit-error corruption. We propose a new method of deriving the indices of the lattice points of the multidimensional pyramid and describe how these techniques can also improve the channel noise immunity of general symmetric lattice quantizers. Our new indexing scheme improves channel robustness by up to 3 dB over previous indexing methods, and can be performed with similar computational rest. The final fixed-rate coding algorithm surpasses the performance of typical Joint Photographic Experts Group (JPEG) implementations and exhibits much greater error resilience.</p>","Computer Science, Artificial Intelligence"
"WOS:000076029400001","Error-resilient pyramid vector quantization for image compression","1998","
<p>Pyramid vector quantization (PVQ) uses the lattice points of a pyramidal shape in multidimensional space as the quantizer codebook, It is a fixed-rate quantization technique that can be used for the compression of Laplacian-like sources arising from transform and subband image coding, where its performance approaches the optimal entropy-coded scalar quantizer without the necessity of variable length codes. In this paper, we investigate the use of PVQ for compressed image transmission over noisy channels, where the fixed-rate quantization seduces the susceptibility to bit-error corruption. We propose a new method of deriving the indices of the lattice points of the multidimensional pyramid and describe how these techniques can also improve the channel noise immunity of general symmetric lattice quantizers. Our new indexing scheme improves channel robustness by up to 3 dB over previous indexing methods, and can be performed with similar computational rest. The final fixed-rate coding algorithm surpasses the performance of typical Joint Photographic Experts Group (JPEG) implementations and exhibits much greater error resilience.</p>","Computer Science"
"WOS:000077186300029","Correlation and prediction of the refractive indices of polymers by QSPR","1998","
<p>A general QSPR model (R-2 = 0.940, s = 0.018) was developed for the prediction of the refractive index for a diverse set of amorphous homopolymers with the CODESSA program. The five descriptors, involved in the model, are calculated from the structure of the repeating unit of the polymer. The average prediction error by this model is 0.9%.</p>","Computer Science, Information Systems"
"WOS:000077186300029","Correlation and prediction of the refractive indices of polymers by QSPR","1998","
<p>A general QSPR model (R-2 = 0.940, s = 0.018) was developed for the prediction of the refractive index for a diverse set of amorphous homopolymers with the CODESSA program. The five descriptors, involved in the model, are calculated from the structure of the repeating unit of the polymer. The average prediction error by this model is 0.9%.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000077186300029","Correlation and prediction of the refractive indices of polymers by QSPR","1998","
<p>A general QSPR model (R-2 = 0.940, s = 0.018) was developed for the prediction of the refractive index for a diverse set of amorphous homopolymers with the CODESSA program. The five descriptors, involved in the model, are calculated from the structure of the repeating unit of the polymer. The average prediction error by this model is 0.9%.</p>","Computer Science"
"WOS:000072981400028","A European de facto standard for image folders applied to telepathology and teaching","1998","
<p>Since 1980, French pathologists at ADICAP (Association pour le Developpement de l'Informatique en Cytologie et en Anatomie Pathologique) have created a common language code allowing the use of computers for routine applications. This code permitted the production of an associated exhaustive image bank of approximately 30 000 images. This task involved many specialists necessitating the definition of specific processes for security and simplicity of data handling. In particular, it has been necessary to develop image communication. To achieve that goal, it was necessary to define a folder, associating textual information to images. That was done through several industrial software providers contribution. Consequently, this folder, using a common packaging standard, allowed any pathologist access to images, codified data and clinical information. Accessing folders has been made easy by launching a Web server at CRIHAN under the supervision of ADICAP. An ADICAP software user may not only browse through the folder but may also import them into their own system and produce new folders. Today more than a hundred users in France and in foreign countries are able to provide diagnostic advice and also referential products useful for further education and quality control. The next challenge is the development of this preliminary de facto approach toward an internationally admitted standard suited for morphological image exchange. (C) 1998 Elsevier Science Ireland Ltd, All rights reserved.</p>","Computer Science, Information Systems"
"WOS:000072981400028","A European de facto standard for image folders applied to telepathology and teaching","1998","
<p>Since 1980, French pathologists at ADICAP (Association pour le Developpement de l'Informatique en Cytologie et en Anatomie Pathologique) have created a common language code allowing the use of computers for routine applications. This code permitted the production of an associated exhaustive image bank of approximately 30 000 images. This task involved many specialists necessitating the definition of specific processes for security and simplicity of data handling. In particular, it has been necessary to develop image communication. To achieve that goal, it was necessary to define a folder, associating textual information to images. That was done through several industrial software providers contribution. Consequently, this folder, using a common packaging standard, allowed any pathologist access to images, codified data and clinical information. Accessing folders has been made easy by launching a Web server at CRIHAN under the supervision of ADICAP. An ADICAP software user may not only browse through the folder but may also import them into their own system and produce new folders. Today more than a hundred users in France and in foreign countries are able to provide diagnostic advice and also referential products useful for further education and quality control. The next challenge is the development of this preliminary de facto approach toward an internationally admitted standard suited for morphological image exchange. (C) 1998 Elsevier Science Ireland Ltd, All rights reserved.</p>","Computer Science"
"WOS:000072123900011","Rothko: A three-dimensional FPGA","1998","
<p>Using transferred circuits and metal interconnections placed between layers of active devices anywhere on the chip, Rothko aims at solving utilization, routing, and delay problems of existing FPGA architectures. Experimental implementations have demonstrated important performance advantages.</p>","Computer Science, Hardware & Architecture"
"WOS:000072123900011","Rothko: A three-dimensional FPGA","1998","
<p>Using transferred circuits and metal interconnections placed between layers of active devices anywhere on the chip, Rothko aims at solving utilization, routing, and delay problems of existing FPGA architectures. Experimental implementations have demonstrated important performance advantages.</p>","Computer Science"
"WOS:000073252800011","A weakly equivalent condition of convex fuzzy sets","1998","
<p>In this short communication, we first introduce inner straight line G((x,y)) on the subset G in the Euclidean space R-n, and define the concept of the weakly quasi-convex fuzzy sets. Second, we will give a weak equivalent condition that a fuzzy closed set is a convex fuzzy set. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science, Theory & Methods"
"WOS:000073252800011","A weakly equivalent condition of convex fuzzy sets","1998","
<p>In this short communication, we first introduce inner straight line G((x,y)) on the subset G in the Euclidean space R-n, and define the concept of the weakly quasi-convex fuzzy sets. Second, we will give a weak equivalent condition that a fuzzy closed set is a convex fuzzy set. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science"
"WOS:000071688400004","k-Arbiter: A safe and general scheme for h out of k mutual exclusion","1998","
<p>Mutual exclusion is a well-known problem that arises when multiple processes compete, in an uncoordinated way, for the acquisition of shared resources over a distributed system. In particular, k-mutual exclusion allows at most k processes to get one unit of the same resource simultaneously. These paradigms do not cover all the cases in which resource accesses must be serialized over a distributed system. There exist cases (e.g. the bandwidth of communication lines) where the amount of shared resource might differ from request to request (for example, audio and video communications). In this paper, we formalize this problem as the h-out of-k mutual exclusion problem, in which each request concerns some number h (1 less than or equal to h less than or equal to k) of units of shared resource and no unit is allocated to multiple processes at the same time. Former simple and k-mutual algorithms cannot be used to solve this problem. We present a general scheme for a quorum-based h-out of-k mutual exclusion algorithm that relies on a collection of quorums called k-arbiter. Several examples of k-arbiters are discussed, two particular classes of k-arbiters are investigated and a metric to evaluate the resiliency with respect to failures of k-arbiters is also given.</p>","Computer Science, Theory & Methods"
"WOS:000071688400004","k-Arbiter: A safe and general scheme for h out of k mutual exclusion","1998","
<p>Mutual exclusion is a well-known problem that arises when multiple processes compete, in an uncoordinated way, for the acquisition of shared resources over a distributed system. In particular, k-mutual exclusion allows at most k processes to get one unit of the same resource simultaneously. These paradigms do not cover all the cases in which resource accesses must be serialized over a distributed system. There exist cases (e.g. the bandwidth of communication lines) where the amount of shared resource might differ from request to request (for example, audio and video communications). In this paper, we formalize this problem as the h-out of-k mutual exclusion problem, in which each request concerns some number h (1 less than or equal to h less than or equal to k) of units of shared resource and no unit is allocated to multiple processes at the same time. Former simple and k-mutual algorithms cannot be used to solve this problem. We present a general scheme for a quorum-based h-out of-k mutual exclusion algorithm that relies on a collection of quorums called k-arbiter. Several examples of k-arbiters are discussed, two particular classes of k-arbiters are investigated and a metric to evaluate the resiliency with respect to failures of k-arbiters is also given.</p>","Computer Science"
"WOS:000073729100004","Dynamic financial imaging: using multimedia to measure the health of your business","1998","
<p>Businesses of the late 1990s have available a wealth of data and information that managers use to measure the health of their business and to identify problems and opportunities. Unfortunately, current measures of business activity are static and do not capture the dynamic flows of business transactions as they occur. Warning signs of pending changes are frequently not seen until after the fact when the financial impact of these changes are reported. It is our proposal that business transactions and performance can and should be measured in a dynamic rather than static manner. Recent advances in computer and communications technology combined with powerful multimedia software enable the construction of algorithms and on-screen instruments which can be used to put business transactions and performance into a dynamic visible form that is readily understood by users.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000073729100004","Dynamic financial imaging: using multimedia to measure the health of your business","1998","
<p>Businesses of the late 1990s have available a wealth of data and information that managers use to measure the health of their business and to identify problems and opportunities. Unfortunately, current measures of business activity are static and do not capture the dynamic flows of business transactions as they occur. Warning signs of pending changes are frequently not seen until after the fact when the financial impact of these changes are reported. It is our proposal that business transactions and performance can and should be measured in a dynamic rather than static manner. Recent advances in computer and communications technology combined with powerful multimedia software enable the construction of algorithms and on-screen instruments which can be used to put business transactions and performance into a dynamic visible form that is readily understood by users.</p>","Computer Science"
"WOS:000076088700007","A multi-level dynamic programming method for line segment matching in axial motion stereo","1998","
<p>This paper propose a multi-level dynamic programming method to solve the line segment-based correspondence problem in axial motion stereo. In this method, a local similarity measure (LSM) is calculated For each line segment pair between the front and back images. A threshold T and certain constraints are used as selecting criteria for choosing potential matching pairs in each level. In Level I, threshold T is set to a relatively high value to ensure the probability of correct match in the first level is very high. In this level, the matching probability between line segments is represented by their local similarity measure. Dynamic programming is then used to search for the best match for those selected potential matching pairs. Matched pairs are used to assist the matching process of the next level. By considering the geometric properties between the matched and the remaining line segments, a global similarity measure (GSM) is calculated for each remaining line segment pair. An overall similarity measure (matching probability) for each remaining line segment pair is then obtained by the LSM and the GSM. The algorithm then proceeds with the second match, but with a slightly lower threshold T-2. New matched results are then used to modify the GSM and the overall similarity measure of the remaining line segment pairs. These processes are repeated until a predefined level n(stop) (or a predefined condition) is reached. By using the GSM and the multi-level searching technique, the proposed technique increases the matching accuracy and the number of matches while reducing the number of unmatched line segment due to misordering when dynamic programming is used for axial motion stereo matching. (C) 1998 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved.</p>","Computer Science, Artificial Intelligence"
"WOS:000076088700007","A multi-level dynamic programming method for line segment matching in axial motion stereo","1998","
<p>This paper propose a multi-level dynamic programming method to solve the line segment-based correspondence problem in axial motion stereo. In this method, a local similarity measure (LSM) is calculated For each line segment pair between the front and back images. A threshold T and certain constraints are used as selecting criteria for choosing potential matching pairs in each level. In Level I, threshold T is set to a relatively high value to ensure the probability of correct match in the first level is very high. In this level, the matching probability between line segments is represented by their local similarity measure. Dynamic programming is then used to search for the best match for those selected potential matching pairs. Matched pairs are used to assist the matching process of the next level. By considering the geometric properties between the matched and the remaining line segments, a global similarity measure (GSM) is calculated for each remaining line segment pair. An overall similarity measure (matching probability) for each remaining line segment pair is then obtained by the LSM and the GSM. The algorithm then proceeds with the second match, but with a slightly lower threshold T-2. New matched results are then used to modify the GSM and the overall similarity measure of the remaining line segment pairs. These processes are repeated until a predefined level n(stop) (or a predefined condition) is reached. By using the GSM and the multi-level searching technique, the proposed technique increases the matching accuracy and the number of matches while reducing the number of unmatched line segment due to misordering when dynamic programming is used for axial motion stereo matching. (C) 1998 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved.</p>","Computer Science"
"WOS:000075931600008","Photoelectrochemical etching of semiconductors","1998","
<p>Photoelectrochemical (PEC) etching of III-V semiconductors has been used to fabricate unique structures in electronic and photonic devices, such as integral lenses on light-emitting diodes, gratings on laser structures, and through-wafer via connections in field-effect transistors. The advantages and characteristics of PEC etching are reviewed, and the extension of this processing technique to silicon is addressed, Three-dimensional structures are of great interest in silicon for electronic and micromechanical devices. Silicon is a challenging material to PEG-etch because the oxides formed during etching inhibit the dissolution rate and decrease the spatial resolution. In addition, the long carrier lifetime permits holes to react at unilluminated sites. Nonaqueous solvents provide a processing environment where oxides do not interfere with the spatial resolution and free fluoride is no longer needed in the dissolution of silicon.</p>","Computer Science, Hardware & Architecture"
"WOS:000075931600008","Photoelectrochemical etching of semiconductors","1998","
<p>Photoelectrochemical (PEC) etching of III-V semiconductors has been used to fabricate unique structures in electronic and photonic devices, such as integral lenses on light-emitting diodes, gratings on laser structures, and through-wafer via connections in field-effect transistors. The advantages and characteristics of PEC etching are reviewed, and the extension of this processing technique to silicon is addressed, Three-dimensional structures are of great interest in silicon for electronic and micromechanical devices. Silicon is a challenging material to PEG-etch because the oxides formed during etching inhibit the dissolution rate and decrease the spatial resolution. In addition, the long carrier lifetime permits holes to react at unilluminated sites. Nonaqueous solvents provide a processing environment where oxides do not interfere with the spatial resolution and free fluoride is no longer needed in the dissolution of silicon.</p>","Computer Science, Information Systems"
"WOS:000075931600008","Photoelectrochemical etching of semiconductors","1998","
<p>Photoelectrochemical (PEC) etching of III-V semiconductors has been used to fabricate unique structures in electronic and photonic devices, such as integral lenses on light-emitting diodes, gratings on laser structures, and through-wafer via connections in field-effect transistors. The advantages and characteristics of PEC etching are reviewed, and the extension of this processing technique to silicon is addressed, Three-dimensional structures are of great interest in silicon for electronic and micromechanical devices. Silicon is a challenging material to PEG-etch because the oxides formed during etching inhibit the dissolution rate and decrease the spatial resolution. In addition, the long carrier lifetime permits holes to react at unilluminated sites. Nonaqueous solvents provide a processing environment where oxides do not interfere with the spatial resolution and free fluoride is no longer needed in the dissolution of silicon.</p>","Computer Science, Software Engineering"
"WOS:000075931600008","Photoelectrochemical etching of semiconductors","1998","
<p>Photoelectrochemical (PEC) etching of III-V semiconductors has been used to fabricate unique structures in electronic and photonic devices, such as integral lenses on light-emitting diodes, gratings on laser structures, and through-wafer via connections in field-effect transistors. The advantages and characteristics of PEC etching are reviewed, and the extension of this processing technique to silicon is addressed, Three-dimensional structures are of great interest in silicon for electronic and micromechanical devices. Silicon is a challenging material to PEG-etch because the oxides formed during etching inhibit the dissolution rate and decrease the spatial resolution. In addition, the long carrier lifetime permits holes to react at unilluminated sites. Nonaqueous solvents provide a processing environment where oxides do not interfere with the spatial resolution and free fluoride is no longer needed in the dissolution of silicon.</p>","Computer Science, Theory & Methods"
"WOS:000075931600008","Photoelectrochemical etching of semiconductors","1998","
<p>Photoelectrochemical (PEC) etching of III-V semiconductors has been used to fabricate unique structures in electronic and photonic devices, such as integral lenses on light-emitting diodes, gratings on laser structures, and through-wafer via connections in field-effect transistors. The advantages and characteristics of PEC etching are reviewed, and the extension of this processing technique to silicon is addressed, Three-dimensional structures are of great interest in silicon for electronic and micromechanical devices. Silicon is a challenging material to PEG-etch because the oxides formed during etching inhibit the dissolution rate and decrease the spatial resolution. In addition, the long carrier lifetime permits holes to react at unilluminated sites. Nonaqueous solvents provide a processing environment where oxides do not interfere with the spatial resolution and free fluoride is no longer needed in the dissolution of silicon.</p>","Computer Science"
"WOS:000072618800008","A 1.6-GHz current-controlled oscillator with integrated inductor","1998","
<p>A 1.6 GHz fully monolithic silicon bipolar LC current-controlled oscillator (CCO) circuit implemented in a 0.8 mu m BiCMOS technology and characterized for use in wireless applications is presented. The integrated resonator circuit uses high speed (18 GHz) bipolar transistors, a 14 nH rectangular spiral inductor fabricated by using a standard 2-level metallization, and a wideband pn-varactor structure. Additionally, to save chip area, the integrated capacitors were fabricated below the planar inductor structure. In order to aid the IC design, a simple equivalent circuit model for the integrated inductor on silicon was developed and tested. The measured quiescent power dissipation of the integrated CCO circuit is 1.9 mW to 5.5 mW from a supply of 2-3 V, and a typical phase noise varies from -82 to -86 dBc/Hz at 100 kHz offset.</p>","Computer Science, Hardware & Architecture"
"WOS:000072618800008","A 1.6-GHz current-controlled oscillator with integrated inductor","1998","
<p>A 1.6 GHz fully monolithic silicon bipolar LC current-controlled oscillator (CCO) circuit implemented in a 0.8 mu m BiCMOS technology and characterized for use in wireless applications is presented. The integrated resonator circuit uses high speed (18 GHz) bipolar transistors, a 14 nH rectangular spiral inductor fabricated by using a standard 2-level metallization, and a wideband pn-varactor structure. Additionally, to save chip area, the integrated capacitors were fabricated below the planar inductor structure. In order to aid the IC design, a simple equivalent circuit model for the integrated inductor on silicon was developed and tested. The measured quiescent power dissipation of the integrated CCO circuit is 1.9 mW to 5.5 mW from a supply of 2-3 V, and a typical phase noise varies from -82 to -86 dBc/Hz at 100 kHz offset.</p>","Computer Science"
"WOS:000073280100002","Simultaneous registration of multiple range views for use in reverse engineering of CAD models","1998","
<p>When reverse engineering a CAD model, it is necessary to integrate information from several views of an object into a common reference frame. Given a rough initial alignment of local 3-D shape data in several images, further refinement is achieved using an improved version of the recently popular Iterative Closest Point algorithm. Improved data correspondence is determined by considering the merging data sets as a whole. A potentially incorrect distance threshold for removing outlier correspondences is not needed as in previous efforts. Incremental pose adjustments are computed simultaneously for all data sets, resulting in a more globally optimal set of transformations. Individual motion updates are computed using force-based optimization, by considering the data sets as implicitly connected by groups of springs. Experiments on both 2-D and 3-D data sets show that convergence is possible even for very rough initial positionings, and that the final registration accuracy typically approaches less than one quarter of the interpoint sampling resolution of the images. (C) 1998 Academic Press.</p>","Computer Science, Artificial Intelligence"
"WOS:000073280100002","Simultaneous registration of multiple range views for use in reverse engineering of CAD models","1998","
<p>When reverse engineering a CAD model, it is necessary to integrate information from several views of an object into a common reference frame. Given a rough initial alignment of local 3-D shape data in several images, further refinement is achieved using an improved version of the recently popular Iterative Closest Point algorithm. Improved data correspondence is determined by considering the merging data sets as a whole. A potentially incorrect distance threshold for removing outlier correspondences is not needed as in previous efforts. Incremental pose adjustments are computed simultaneously for all data sets, resulting in a more globally optimal set of transformations. Individual motion updates are computed using force-based optimization, by considering the data sets as implicitly connected by groups of springs. Experiments on both 2-D and 3-D data sets show that convergence is possible even for very rough initial positionings, and that the final registration accuracy typically approaches less than one quarter of the interpoint sampling resolution of the images. (C) 1998 Academic Press.</p>","Computer Science"
"WOS:000074402600003","A theory of implementation and refinement in timed Petri nets","1998","
<p>We define formally the notion of implementation for time critical systems in terms of provability of properties described abstractly at the specification level. We characterize this notion in terms of formulas of the temporal logic TRIO and operational models of timed Petri nets, and provide a method to prove that two given nets are in the implementation relation. Refinement steps are often used as a means to derive in a systematic way the system design starting from its abstract specification. We present a method to formally prove the correctness of refinement rules for timed Petri nets and apply it to a few simple cases. We show how the possibility to retain properties of the specification in its implementation can simplify the verification of the designed systems by performing incremental analysis at various levels of the specification/implementation hierarchy. (C) 1998 - Elsevier Science B.V. All rights reserved.</p>","Computer Science, Theory & Methods"
"WOS:000074402600003","A theory of implementation and refinement in timed Petri nets","1998","
<p>We define formally the notion of implementation for time critical systems in terms of provability of properties described abstractly at the specification level. We characterize this notion in terms of formulas of the temporal logic TRIO and operational models of timed Petri nets, and provide a method to prove that two given nets are in the implementation relation. Refinement steps are often used as a means to derive in a systematic way the system design starting from its abstract specification. We present a method to formally prove the correctness of refinement rules for timed Petri nets and apply it to a few simple cases. We show how the possibility to retain properties of the specification in its implementation can simplify the verification of the designed systems by performing incremental analysis at various levels of the specification/implementation hierarchy. (C) 1998 - Elsevier Science B.V. All rights reserved.</p>","Computer Science"
"WOS:000076728700003","Resolving occlusion in image sequence made easy","1998","
<p>While the task of seamlessly merging computer-generated 3D objects into an image sequence can be done manually, such effort often lacks consistency across the images. It is also time consuming and prone to error. This paper proposes a framework that solves the occlusion problem without assuming a priori computer models from the input scene. It includes a new algorithm to derive approximate 3D models about the real scene based on recovered geometry information and user-supplied segmentation results. The framework has been implemented, and it works for amateur home videos. The result is an easy-to-use system for applications like the visualization of new architectures in a real environment.</p>","Computer Science, Software Engineering"
"WOS:000076728700003","Resolving occlusion in image sequence made easy","1998","
<p>While the task of seamlessly merging computer-generated 3D objects into an image sequence can be done manually, such effort often lacks consistency across the images. It is also time consuming and prone to error. This paper proposes a framework that solves the occlusion problem without assuming a priori computer models from the input scene. It includes a new algorithm to derive approximate 3D models about the real scene based on recovered geometry information and user-supplied segmentation results. The framework has been implemented, and it works for amateur home videos. The result is an easy-to-use system for applications like the visualization of new architectures in a real environment.</p>","Computer Science"
"WOS:000071601900002","A process algebraic view of Linda coordination primitives","1998","
<p>The main Linda coordination primitives (asynchronous communication, read operation, non-blocking in/rd predicates) are studied in a process algebraic setting. A lattice of eight languages is proposed, where its bottom element L is a process algebra differing from CCS only for the asynchrony of the output operation, while all the other languages in the lattice are obtained as extension of this basic language by adding some of the Linda coordination primitives. The observational semantics for these languages are all obtained as the coarsest congruences contained in the barbed semantics, where only tuples are observable. The lattice of the eight languages collapses to a smaller four-points lattice of different bisimulation-based semantics. Notably, for L this semantics is the standard notion of strong bisimulation, where inputs and outputs/tuples are treated symmetrically.</p>","Computer Science, Theory & Methods"
"WOS:000071601900002","A process algebraic view of Linda coordination primitives","1998","
<p>The main Linda coordination primitives (asynchronous communication, read operation, non-blocking in/rd predicates) are studied in a process algebraic setting. A lattice of eight languages is proposed, where its bottom element L is a process algebra differing from CCS only for the asynchrony of the output operation, while all the other languages in the lattice are obtained as extension of this basic language by adding some of the Linda coordination primitives. The observational semantics for these languages are all obtained as the coarsest congruences contained in the barbed semantics, where only tuples are observable. The lattice of the eight languages collapses to a smaller four-points lattice of different bisimulation-based semantics. Notably, for L this semantics is the standard notion of strong bisimulation, where inputs and outputs/tuples are treated symmetrically.</p>","Computer Science"
"WOS:000076703600005","On the number of minimum weight codewords of subcodes of Reed-Muller codes","1998","
<p>In this paper, we consider linear subcodes of RMr, m whose bases are formed from the monomial basis of RMr, (m) by deleting Delta K monomials of degree r where Delta K < ((m)(r)). For such subcodes, a procedure for computing the number ber of minimum weight codewords is presented and it is shown how to delete Delta K monomials in order to obtain a subcode with the smallest number of codewords of the minimum weight. For Delta K less than or equal to 3, a formula for the number of codewords of the minimum weight is presented. A (64, 40) subcode of RM3, 6 is being considered as an inner code in a concatenated coding system for NASA's high-speed satellite communications. For (64,40) subcodes, there are three equivalent classes. For each class, the number of minimum weight codewords, that of the second smallest weight codewords and simulation results on error probabilities of soft-decision maximum likelihood decoding are presented.</p>","Computer Science, Hardware & Architecture"
"WOS:000076703600005","On the number of minimum weight codewords of subcodes of Reed-Muller codes","1998","
<p>In this paper, we consider linear subcodes of RMr, m whose bases are formed from the monomial basis of RMr, (m) by deleting Delta K monomials of degree r where Delta K < ((m)(r)). For such subcodes, a procedure for computing the number ber of minimum weight codewords is presented and it is shown how to delete Delta K monomials in order to obtain a subcode with the smallest number of codewords of the minimum weight. For Delta K less than or equal to 3, a formula for the number of codewords of the minimum weight is presented. A (64, 40) subcode of RM3, 6 is being considered as an inner code in a concatenated coding system for NASA's high-speed satellite communications. For (64,40) subcodes, there are three equivalent classes. For each class, the number of minimum weight codewords, that of the second smallest weight codewords and simulation results on error probabilities of soft-decision maximum likelihood decoding are presented.</p>","Computer Science, Information Systems"
"WOS:000076703600005","On the number of minimum weight codewords of subcodes of Reed-Muller codes","1998","
<p>In this paper, we consider linear subcodes of RMr, m whose bases are formed from the monomial basis of RMr, (m) by deleting Delta K monomials of degree r where Delta K < ((m)(r)). For such subcodes, a procedure for computing the number ber of minimum weight codewords is presented and it is shown how to delete Delta K monomials in order to obtain a subcode with the smallest number of codewords of the minimum weight. For Delta K less than or equal to 3, a formula for the number of codewords of the minimum weight is presented. A (64, 40) subcode of RM3, 6 is being considered as an inner code in a concatenated coding system for NASA's high-speed satellite communications. For (64,40) subcodes, there are three equivalent classes. For each class, the number of minimum weight codewords, that of the second smallest weight codewords and simulation results on error probabilities of soft-decision maximum likelihood decoding are presented.</p>","Computer Science"
"WOS:000074967200003","Partitioning a sequence into few monotone subsequences","1998","
<p>In this paper we consider the problem of finding sets of long disjoint monotone subsequences of a sequence of n numbers. We give an algorithm that, after O(n log n) preprocessing time, finds and deletes an increasing subsequence of size Ic (if it exists) in time O(n + k(2)). Using this algorithm, it is possible to partition a sequence of n numbers into 2[root n] monotone subsequences in time O(n(1.5)).</p>
<p>Our algorithm yields improvements for two applications: The first is constructing good splitters for a set of lines in the plane. Good splitters are useful for two dimensional simplex range searching. The second application is in VLSI, where we seek a partitioning of a given graph into subsets, commonly refered to as the pages of a book, where all the vertices can be placed on the spine of the book, and each subgraph is planar.</p>","Computer Science, Information Systems"
"WOS:000074967200003","Partitioning a sequence into few monotone subsequences","1998","
<p>In this paper we consider the problem of finding sets of long disjoint monotone subsequences of a sequence of n numbers. We give an algorithm that, after O(n log n) preprocessing time, finds and deletes an increasing subsequence of size Ic (if it exists) in time O(n + k(2)). Using this algorithm, it is possible to partition a sequence of n numbers into 2[root n] monotone subsequences in time O(n(1.5)).</p>
<p>Our algorithm yields improvements for two applications: The first is constructing good splitters for a set of lines in the plane. Good splitters are useful for two dimensional simplex range searching. The second application is in VLSI, where we seek a partitioning of a given graph into subsets, commonly refered to as the pages of a book, where all the vertices can be placed on the spine of the book, and each subgraph is planar.</p>","Computer Science"
"WOS:000075141300004","Performance-driven simultaneous placement and routing for FPGA's","1998","
<p>Sequential place and route tools for field programmable gate arrays (FPGA's) are inherently weak at addressing both wirability and timing optimizations. This is primarily due to the difficulty of accurately predicting wirability and delay during placement. A set of new performance-driven simultaneous placement/routing techniques has been developed for both row-based and island-style FPGA designs. These techniques rely on an iterative improvement placement algorithm augmented with fast, complete routing heuristics in the placement loop, For row-based designs, this new layout strategy yielded up to 28% improvements in timing and 33% in,wirability for several MCNC benchmarks when compared to a traditional sequential place and route system in use at Texas Instruments. On a set of industrial designs for Xilinx 4000-series island-style FPGA's, our scheme produced 100% routed designs with 8-15% improvement in delay when compared to the Xilinx XACT5.0 place and route system.</p>","Computer Science, Hardware & Architecture"
"WOS:000075141300004","Performance-driven simultaneous placement and routing for FPGA's","1998","
<p>Sequential place and route tools for field programmable gate arrays (FPGA's) are inherently weak at addressing both wirability and timing optimizations. This is primarily due to the difficulty of accurately predicting wirability and delay during placement. A set of new performance-driven simultaneous placement/routing techniques has been developed for both row-based and island-style FPGA designs. These techniques rely on an iterative improvement placement algorithm augmented with fast, complete routing heuristics in the placement loop, For row-based designs, this new layout strategy yielded up to 28% improvements in timing and 33% in,wirability for several MCNC benchmarks when compared to a traditional sequential place and route system in use at Texas Instruments. On a set of industrial designs for Xilinx 4000-series island-style FPGA's, our scheme produced 100% routed designs with 8-15% improvement in delay when compared to the Xilinx XACT5.0 place and route system.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000075141300004","Performance-driven simultaneous placement and routing for FPGA's","1998","
<p>Sequential place and route tools for field programmable gate arrays (FPGA's) are inherently weak at addressing both wirability and timing optimizations. This is primarily due to the difficulty of accurately predicting wirability and delay during placement. A set of new performance-driven simultaneous placement/routing techniques has been developed for both row-based and island-style FPGA designs. These techniques rely on an iterative improvement placement algorithm augmented with fast, complete routing heuristics in the placement loop, For row-based designs, this new layout strategy yielded up to 28% improvements in timing and 33% in,wirability for several MCNC benchmarks when compared to a traditional sequential place and route system in use at Texas Instruments. On a set of industrial designs for Xilinx 4000-series island-style FPGA's, our scheme produced 100% routed designs with 8-15% improvement in delay when compared to the Xilinx XACT5.0 place and route system.</p>","Computer Science"
"WOS:000072873000003","The Auction technique for the sensor based navigation planning of an autonomous mobile robot","1998","
<p>The problem of finding a path for the motion of a small mobile robot from a starring point to a fixed target in a two dimensional domain is considered in the presence of arbitrary shaped obstacles. No a priori information is known in advance about the geometry and the dimensions of the workspace nor about the number, extension and location of obstacles. The robot has a sensing device that detects all obstacles or pieces of wails lying beyond a fixed view range. A discrete version of the problem is solved by an iterative algorithm that at any iteration step finds the smallest path length from the actual point to the target with respect to the actual knowledge about the obstacles, then the robot is steered along the path until a new obstacle point interfering with the path is found, at this point a new iteration is started. Such an algorithm stops in a number of steps depending on the geometry, finding a solution for the problem or detecting that the problem is unfeasible. Since the algorithm must be applied on line, the effectiveness of the method depends strongly on the efficiency of the optimization step. The use of the Auction method speeds up this step greatly both for the intrinsic properties of this method and because we fully exploit a property relating two successive optimizations, proved on paper, that in practical instances enables the mean computational cost requested by the optimization step to be greatly reduced. It is proved that the algorithm converges in a finite number of steps finding a solution when the problem is feasible or detecting the infeasibility condition otherwise. Moreover the worst case computational complexity of the whole algorithm is shown to be polynomial in the number of nodes of the discretization grid. Finally numerical examples are reported in order to show the effectiveness of this technique.</p>","Computer Science, Artificial Intelligence"
"WOS:000072873000003","The Auction technique for the sensor based navigation planning of an autonomous mobile robot","1998","
<p>The problem of finding a path for the motion of a small mobile robot from a starring point to a fixed target in a two dimensional domain is considered in the presence of arbitrary shaped obstacles. No a priori information is known in advance about the geometry and the dimensions of the workspace nor about the number, extension and location of obstacles. The robot has a sensing device that detects all obstacles or pieces of wails lying beyond a fixed view range. A discrete version of the problem is solved by an iterative algorithm that at any iteration step finds the smallest path length from the actual point to the target with respect to the actual knowledge about the obstacles, then the robot is steered along the path until a new obstacle point interfering with the path is found, at this point a new iteration is started. Such an algorithm stops in a number of steps depending on the geometry, finding a solution for the problem or detecting that the problem is unfeasible. Since the algorithm must be applied on line, the effectiveness of the method depends strongly on the efficiency of the optimization step. The use of the Auction method speeds up this step greatly both for the intrinsic properties of this method and because we fully exploit a property relating two successive optimizations, proved on paper, that in practical instances enables the mean computational cost requested by the optimization step to be greatly reduced. It is proved that the algorithm converges in a finite number of steps finding a solution when the problem is feasible or detecting the infeasibility condition otherwise. Moreover the worst case computational complexity of the whole algorithm is shown to be polynomial in the number of nodes of the discretization grid. Finally numerical examples are reported in order to show the effectiveness of this technique.</p>","Computer Science"
"WOS:000073557700004","Loosely-specified query processing in large-scale information systems","1998","
<p>Challenging issues for processing queries specified over large-scale information spaces (e.g., Digital Libraries or the World Wide Web) include the diversity of the information sources in terms of their structures, query interfaces and search capabilities, as well as the dynamics of sources continuously being added, removed or upgraded. In this paper, we give an innovative solution for query planning in such environments. The foundation of our solution is the Dynamic Information Integration Model (DIIM) which supports the specification of not only content but also capabilities of resources without requiring the establishment of a uniform integration schema. Besides the development of the DIIM model, contributions of this paper include: (1) the introduction of the notion of fully specified queries that are semantically equivalent to a loosely-specified query; (2) a translation algorithm of a loosely-specified query into a set of semantically equivalent feasible query plans that are consistent with the binding patterns of query templates of the individual sources (capability descriptions in DIIM) and with interrelationships between information sources (expressed as join constraints in DIIM); and (3) a search restriction algorithm for optimizing query processing by pruning the search space into the relevant subspace of a query. The plans obtained by the proposed query planning process which is composed of the search restriction and translation algorithms can be shown to correspond to query plans semantically equivalent to the initial loosely-specified input query.</p>","Computer Science, Information Systems"
"WOS:000073557700004","Loosely-specified query processing in large-scale information systems","1998","
<p>Challenging issues for processing queries specified over large-scale information spaces (e.g., Digital Libraries or the World Wide Web) include the diversity of the information sources in terms of their structures, query interfaces and search capabilities, as well as the dynamics of sources continuously being added, removed or upgraded. In this paper, we give an innovative solution for query planning in such environments. The foundation of our solution is the Dynamic Information Integration Model (DIIM) which supports the specification of not only content but also capabilities of resources without requiring the establishment of a uniform integration schema. Besides the development of the DIIM model, contributions of this paper include: (1) the introduction of the notion of fully specified queries that are semantically equivalent to a loosely-specified query; (2) a translation algorithm of a loosely-specified query into a set of semantically equivalent feasible query plans that are consistent with the binding patterns of query templates of the individual sources (capability descriptions in DIIM) and with interrelationships between information sources (expressed as join constraints in DIIM); and (3) a search restriction algorithm for optimizing query processing by pruning the search space into the relevant subspace of a query. The plans obtained by the proposed query planning process which is composed of the search restriction and translation algorithms can be shown to correspond to query plans semantically equivalent to the initial loosely-specified input query.</p>","Computer Science"
"WOS:000076335600008","Results and experience with four years of development work on a filmless department in Mjolby health care centre","1998","
<p>Since the end of 1994, the radiology department in Mjolby has been running their activity without X-ray films. When the film developing machine at the department was discarded in April 1995, after six months of test-running of the new system, the film epoch was definitely abandoned. The practical experiences from working with digital images are all good. The background to this change lays in a four years development work, which has been supported financially by the Ostergotland County Council, Swedish National Board for Industrial and Technical Development (NUTEK), and a regional foundation for development of new ways to organise and perform work (Arbetslivsfonden). (C) 1998 Elsevier Science Ireland Ltd. All rights reserved.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000076335600008","Results and experience with four years of development work on a filmless department in Mjolby health care centre","1998","
<p>Since the end of 1994, the radiology department in Mjolby has been running their activity without X-ray films. When the film developing machine at the department was discarded in April 1995, after six months of test-running of the new system, the film epoch was definitely abandoned. The practical experiences from working with digital images are all good. The background to this change lays in a four years development work, which has been supported financially by the Ostergotland County Council, Swedish National Board for Industrial and Technical Development (NUTEK), and a regional foundation for development of new ways to organise and perform work (Arbetslivsfonden). (C) 1998 Elsevier Science Ireland Ltd. All rights reserved.</p>","Computer Science, Theory & Methods"
"WOS:000076335600008","Results and experience with four years of development work on a filmless department in Mjolby health care centre","1998","
<p>Since the end of 1994, the radiology department in Mjolby has been running their activity without X-ray films. When the film developing machine at the department was discarded in April 1995, after six months of test-running of the new system, the film epoch was definitely abandoned. The practical experiences from working with digital images are all good. The background to this change lays in a four years development work, which has been supported financially by the Ostergotland County Council, Swedish National Board for Industrial and Technical Development (NUTEK), and a regional foundation for development of new ways to organise and perform work (Arbetslivsfonden). (C) 1998 Elsevier Science Ireland Ltd. All rights reserved.</p>","Computer Science"
"WOS:000071831400008","Probabilistic interpretation of population codes","1998","
<p>We present a general encoding-decoding framework for interpreting the activity of a population of units. A standard population code interpretation method, the Poisson model, starts from a description as to how a single value of an underlying quantity can generate the activities of each unit in the population. In casting it in the encoding-decoding framework, we find that this model is too restrictive to describe fully the activities of units in population codes in higher processing areas, such as the medial temporal area. Under a more powerful model, the population activity can convey information not only about a single value of some quantity but also about its whole distribution, including its variance, and perhaps even the certainty the system has in the actual presence in the world of the entity generating this quantity. We propose a novel method for forming such probabilistic interpretations of population codes and compare it to the existing method.</p>","Computer Science, Artificial Intelligence"
"WOS:000071831400008","Probabilistic interpretation of population codes","1998","
<p>We present a general encoding-decoding framework for interpreting the activity of a population of units. A standard population code interpretation method, the Poisson model, starts from a description as to how a single value of an underlying quantity can generate the activities of each unit in the population. In casting it in the encoding-decoding framework, we find that this model is too restrictive to describe fully the activities of units in population codes in higher processing areas, such as the medial temporal area. Under a more powerful model, the population activity can convey information not only about a single value of some quantity but also about its whole distribution, including its variance, and perhaps even the certainty the system has in the actual presence in the world of the entity generating this quantity. We propose a novel method for forming such probabilistic interpretations of population codes and compare it to the existing method.</p>","Computer Science"
"WOS:000076853600005","Fast image restoration for reducing block artifacts based on adaptive constrained optimization","1998","
<p>In this paper we propose a fast adaptive image restoration filter using DCT-based edge-classification for reducing block artifacts in compressed images. In order to efficiently reduce block artifacts, edge direction of each block is classified by using the DCT coefficients, and an adaptive constrained least squares (CLS) filter along the edge direction is used for filtering the corresponding block. The proposed research is based on the observation that degradation due to block artifacts is a nonlinear and many-to-one mapping operator. Then, we propose an approximated version of a constrained optimization technique as a restoration process for removing the nonlinear and space-varying degradation operator. For real-time application, the proposed restoration filter is implemented in the form of a truncated FIR litter, which is suitable for postprocessing the images in real-time video systems such as HDTV or video conferencing systems. (C) 1998 Academic Press</p>","Computer Science, Information Systems"
"WOS:000076853600005","Fast image restoration for reducing block artifacts based on adaptive constrained optimization","1998","
<p>In this paper we propose a fast adaptive image restoration filter using DCT-based edge-classification for reducing block artifacts in compressed images. In order to efficiently reduce block artifacts, edge direction of each block is classified by using the DCT coefficients, and an adaptive constrained least squares (CLS) filter along the edge direction is used for filtering the corresponding block. The proposed research is based on the observation that degradation due to block artifacts is a nonlinear and many-to-one mapping operator. Then, we propose an approximated version of a constrained optimization technique as a restoration process for removing the nonlinear and space-varying degradation operator. For real-time application, the proposed restoration filter is implemented in the form of a truncated FIR litter, which is suitable for postprocessing the images in real-time video systems such as HDTV or video conferencing systems. (C) 1998 Academic Press</p>","Computer Science, Software Engineering"
"WOS:000076853600005","Fast image restoration for reducing block artifacts based on adaptive constrained optimization","1998","
<p>In this paper we propose a fast adaptive image restoration filter using DCT-based edge-classification for reducing block artifacts in compressed images. In order to efficiently reduce block artifacts, edge direction of each block is classified by using the DCT coefficients, and an adaptive constrained least squares (CLS) filter along the edge direction is used for filtering the corresponding block. The proposed research is based on the observation that degradation due to block artifacts is a nonlinear and many-to-one mapping operator. Then, we propose an approximated version of a constrained optimization technique as a restoration process for removing the nonlinear and space-varying degradation operator. For real-time application, the proposed restoration filter is implemented in the form of a truncated FIR litter, which is suitable for postprocessing the images in real-time video systems such as HDTV or video conferencing systems. (C) 1998 Academic Press</p>","Computer Science"
"WOS:000077772100004","Computer aided dynamic analysis and simulation of multibody mechanical systems in AutoCAD","1998","
<p>A general-purpose recursive multibody dynamic algorithm is written and merged into AutoCAD. This merger provides a user-friendly environment for the dynamic simulation of multibody mechanical systems. The recursive formulation provides it computationally efficient way to construct and solve equations of the motion of systems. The input data of the dynamic code can be prepared easily by using the AutoCAD drawing. The output data of the dynamic code can be used to create an animation of the dynamic simulation in AutoCAD.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000077772100004","Computer aided dynamic analysis and simulation of multibody mechanical systems in AutoCAD","1998","
<p>A general-purpose recursive multibody dynamic algorithm is written and merged into AutoCAD. This merger provides a user-friendly environment for the dynamic simulation of multibody mechanical systems. The recursive formulation provides it computationally efficient way to construct and solve equations of the motion of systems. The input data of the dynamic code can be prepared easily by using the AutoCAD drawing. The output data of the dynamic code can be used to create an animation of the dynamic simulation in AutoCAD.</p>","Computer Science, Software Engineering"
"WOS:000077772100004","Computer aided dynamic analysis and simulation of multibody mechanical systems in AutoCAD","1998","
<p>A general-purpose recursive multibody dynamic algorithm is written and merged into AutoCAD. This merger provides a user-friendly environment for the dynamic simulation of multibody mechanical systems. The recursive formulation provides it computationally efficient way to construct and solve equations of the motion of systems. The input data of the dynamic code can be prepared easily by using the AutoCAD drawing. The output data of the dynamic code can be used to create an animation of the dynamic simulation in AutoCAD.</p>","Computer Science"
"WOS:000073575700002","Development of a neural network model for rotor angle estimation","1998","
<p>This paper presents a neural network model to measure the rotor angle of the synchronous machine. The network model is established based on the mapping capabilities of multi-layer feedforward neural networks and weight estimation of the back-propagation learning rule. A neural network is then developed using simulation data, obtained by applying Park's equations for the dynamics of the synchronous machine in the d-q axis reference frame. Following successful application of the network model based on simulation data, data from laboratory measurements of terminal variables of a 5 kVA test machine is then applied to develop the neural network model for rotor angle measurement.</p>","Computer Science, Artificial Intelligence"
"WOS:000073575700002","Development of a neural network model for rotor angle estimation","1998","
<p>This paper presents a neural network model to measure the rotor angle of the synchronous machine. The network model is established based on the mapping capabilities of multi-layer feedforward neural networks and weight estimation of the back-propagation learning rule. A neural network is then developed using simulation data, obtained by applying Park's equations for the dynamics of the synchronous machine in the d-q axis reference frame. Following successful application of the network model based on simulation data, data from laboratory measurements of terminal variables of a 5 kVA test machine is then applied to develop the neural network model for rotor angle measurement.</p>","Computer Science"
"WOS:000075193800010","Self-similar texture characterization using a Fourier-domain maximum likelihood estimation method","1998","
<p>A Maximum Likelihood Estimator (MLE) has been applied to estimating the Hurst parameter H on a self-similar texture image. Much of the work done so far has concentrated on the spatial domain. In this paper, we propose an approximate MLE method for estimating H in the Fourier domain. This method saves computational time and can be applied to estimating the parameter H directly from the Fourier-domain raw data collected by the Magnetic Resonance Imaging (MRI) scanner. We use synthetic fractal datasets and a human tibia image to study the performance of our method. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science, Artificial Intelligence"
"WOS:000075193800010","Self-similar texture characterization using a Fourier-domain maximum likelihood estimation method","1998","
<p>A Maximum Likelihood Estimator (MLE) has been applied to estimating the Hurst parameter H on a self-similar texture image. Much of the work done so far has concentrated on the spatial domain. In this paper, we propose an approximate MLE method for estimating H in the Fourier domain. This method saves computational time and can be applied to estimating the parameter H directly from the Fourier-domain raw data collected by the Magnetic Resonance Imaging (MRI) scanner. We use synthetic fractal datasets and a human tibia image to study the performance of our method. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science"
"WOS:000074869600001","Mental representations of spatial language","1998","
<p>Previous studies have provided evidence of multi-level mental representations of language-conveyed spatial (scenic) information. However, the available evidence is largely inconclusive with regard to the structure of these mental representations. A laboratory experiment assesses computer-assisted problem-solving performance abilities when language-conveyed representations of spatial information are matched with the language perspective of the task and with individual cognitive sk.ills. Our findings largely validate this paradigm of ""cognitive fit"" that has been applied in non-language computer display domains, and the results suggest language-fostered ""perspective-bias"" in the formation and use of mental representations of spatial (scenic) information. (C) 1998 Acadamic Press.</p>","Computer Science, Cybernetics"
"WOS:000074869600001","Mental representations of spatial language","1998","
<p>Previous studies have provided evidence of multi-level mental representations of language-conveyed spatial (scenic) information. However, the available evidence is largely inconclusive with regard to the structure of these mental representations. A laboratory experiment assesses computer-assisted problem-solving performance abilities when language-conveyed representations of spatial information are matched with the language perspective of the task and with individual cognitive sk.ills. Our findings largely validate this paradigm of ""cognitive fit"" that has been applied in non-language computer display domains, and the results suggest language-fostered ""perspective-bias"" in the formation and use of mental representations of spatial (scenic) information. (C) 1998 Acadamic Press.</p>","Computer Science"
"WOS:000082523900024","Object oriented safety analysis of an extra high voltage substation bay","1998","
<p>Experiences of application of the object oriented approach to safety analysis of an extra high voltage substation bay are presented. As the first step the object model of the whole application is developed. Then the model is subjected to three safety analysis methods. The analyses are supported by an existing tool. The paper illustrates the application of the proposed methods and also gives some observations on the performance of the tool.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000082523900024","Object oriented safety analysis of an extra high voltage substation bay","1998","
<p>Experiences of application of the object oriented approach to safety analysis of an extra high voltage substation bay are presented. As the first step the object model of the whole application is developed. Then the model is subjected to three safety analysis methods. The analyses are supported by an existing tool. The paper illustrates the application of the proposed methods and also gives some observations on the performance of the tool.</p>","Computer Science, Theory & Methods"
"WOS:000082523900024","Object oriented safety analysis of an extra high voltage substation bay","1998","
<p>Experiences of application of the object oriented approach to safety analysis of an extra high voltage substation bay are presented. As the first step the object model of the whole application is developed. Then the model is subjected to three safety analysis methods. The analyses are supported by an existing tool. The paper illustrates the application of the proposed methods and also gives some observations on the performance of the tool.</p>","Computer Science"
"WOS:000074832200004","Practical improvements to the construction and destruction of static single assignment form","1998","
<p>Static Single Assignment (SSA) form is a program representation that is becoming increasingly popular for compiler-based code optimization. In this paper, we address three problems that have arisen in our use of SSA form. Two are variations to the SSA construction algorithms presented by Cytron et al,(1) The first variation is a version of SSA form that we call 'semi-pruned' SSA, It offers an attractive trade-off between the cost of global data-how analysis required to build 'pruned' SSA and the large number of unused phi-functions found in minimal SSA, The second variation speeds up the program renaming process by efficiently manipulating the stacks of names used during renaming. Our improvement reduces the number of pushes performed, in addition to more efficiently locating the stacks that should be popped. To convert code in SSA form back into an executable form, the compiler must use an algorithm that replaces phi-functions with appropriately-placed copy instructions. The algorithm given by Cytron et al, for inserting copies produces incorrect results in some situations; particularly in cases like instruction scheduling, where the compiler may not be able to split 'critical edges', and in the aftermath of optimizations that aggressively rewrite the name space, like some forms of global value numbering.(2) We present a new algorithm for inserting copy instructions to replace phi-functions. It fixes the problems that we have encountered with the original copy insertion algorithm. We present experimental results that demonstrate the effectiveness of the first two improvements not only during the construction of SSA form, but also in the time saved by subsequent optimization passes that use a smaller representation of the program. (C) 1998 John Wiley & Sons, Ltd.</p>","Computer Science, Software Engineering"
"WOS:000074832200004","Practical improvements to the construction and destruction of static single assignment form","1998","
<p>Static Single Assignment (SSA) form is a program representation that is becoming increasingly popular for compiler-based code optimization. In this paper, we address three problems that have arisen in our use of SSA form. Two are variations to the SSA construction algorithms presented by Cytron et al,(1) The first variation is a version of SSA form that we call 'semi-pruned' SSA, It offers an attractive trade-off between the cost of global data-how analysis required to build 'pruned' SSA and the large number of unused phi-functions found in minimal SSA, The second variation speeds up the program renaming process by efficiently manipulating the stacks of names used during renaming. Our improvement reduces the number of pushes performed, in addition to more efficiently locating the stacks that should be popped. To convert code in SSA form back into an executable form, the compiler must use an algorithm that replaces phi-functions with appropriately-placed copy instructions. The algorithm given by Cytron et al, for inserting copies produces incorrect results in some situations; particularly in cases like instruction scheduling, where the compiler may not be able to split 'critical edges', and in the aftermath of optimizations that aggressively rewrite the name space, like some forms of global value numbering.(2) We present a new algorithm for inserting copy instructions to replace phi-functions. It fixes the problems that we have encountered with the original copy insertion algorithm. We present experimental results that demonstrate the effectiveness of the first two improvements not only during the construction of SSA form, but also in the time saved by subsequent optimization passes that use a smaller representation of the program. (C) 1998 John Wiley & Sons, Ltd.</p>","Computer Science"
"WOS:000074840000017","Determining data independence on a digitized membrane in three dimensions","1998","
<p>A method for determining whether structures distributed along a cell's membrane represent a random spatial distribution is presented in this paper. Two three-dimensional (3-D) images are acquired from one cell by wide-field digital imaging of cells which have been labeled with two different fluorescent antibodies. Prior to spatial analysis, a constrained regularized least squares restoration of the images is performed. This is followed by registration via fiducial markers (dual-labeled beads). A deformable model is then used to map data near the surface to the surface. Finally, each resulting data set is analyzed to determine whether it is spatially random. To do this, we generalize the test for complete spatial randomness of points in a plane, to test voxels distributed along a voxelized membrane in three dimensions. We also test whether the distribution of one protein is independent of the distribution of a second protein. The method is applied to compare the distribution of the protein kinase C to that of vinculin. Vinculin is a protein which anchors intracellular filaments to the cell's plasma membrane. It is also used as a (sparse) membrane marker for the deformable model. Protein kinase C facilitates molecular motors inside the cell. These may be associated with actin and myosin filaments.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000074840000017","Determining data independence on a digitized membrane in three dimensions","1998","
<p>A method for determining whether structures distributed along a cell's membrane represent a random spatial distribution is presented in this paper. Two three-dimensional (3-D) images are acquired from one cell by wide-field digital imaging of cells which have been labeled with two different fluorescent antibodies. Prior to spatial analysis, a constrained regularized least squares restoration of the images is performed. This is followed by registration via fiducial markers (dual-labeled beads). A deformable model is then used to map data near the surface to the surface. Finally, each resulting data set is analyzed to determine whether it is spatially random. To do this, we generalize the test for complete spatial randomness of points in a plane, to test voxels distributed along a voxelized membrane in three dimensions. We also test whether the distribution of one protein is independent of the distribution of a second protein. The method is applied to compare the distribution of the protein kinase C to that of vinculin. Vinculin is a protein which anchors intracellular filaments to the cell's plasma membrane. It is also used as a (sparse) membrane marker for the deformable model. Protein kinase C facilitates molecular motors inside the cell. These may be associated with actin and myosin filaments.</p>","Computer Science"
"WOS:000075870700004","IT and changing professional identity: Micro-studies and macro-theory","1998","
<p>This article explores linkages between the use of information technology and changes in the self-identity of professional groups, in terms of how they see and describe themselves in relation to their work and that of others. Three micro-level studies are described, involving the work of loan managers in a particular bank, insurance brokers in the London Insurance Market, and professional salespeople in a pharmaceutical company. Results from the case studies are analyzed using concepts from the macro-level social theory of the sociologist Anthony Giddens. It is argued that such theory can help to us to generalize the results from micro-level studies and, conversely, that micro-studies of IT and social transformation are needed to add in the IT dimension to macro-level theories. The article offers a modest starting point for the investigation of IT and social transformation across multiple levels of analysis.</p>","Computer Science, Information Systems"
"WOS:000075870700004","IT and changing professional identity: Micro-studies and macro-theory","1998","
<p>This article explores linkages between the use of information technology and changes in the self-identity of professional groups, in terms of how they see and describe themselves in relation to their work and that of others. Three micro-level studies are described, involving the work of loan managers in a particular bank, insurance brokers in the London Insurance Market, and professional salespeople in a pharmaceutical company. Results from the case studies are analyzed using concepts from the macro-level social theory of the sociologist Anthony Giddens. It is argued that such theory can help to us to generalize the results from micro-level studies and, conversely, that micro-studies of IT and social transformation are needed to add in the IT dimension to macro-level theories. The article offers a modest starting point for the investigation of IT and social transformation across multiple levels of analysis.</p>","Computer Science"
"WOS:000083171400009","Generalizing updates: From models to programs","1998","
<p>Recently the field of theory update has seen some improvement, in what concerns model updating, by allowing updates to be specified by so-called revision programs. The updating of theory models is governed by their update rules and also by inertia applied to those literals not directly affected by the update program. Though this is important, it remains necessary to tackle as well the updating of programs specifying theories. Some results have been obtained on the issue of updating a logic program which encodes a set of models, to obtain a new program whose models are the desired updates of the initial models. But here the program only prays the role of a means to encode the models.</p>
<p>A logic program encodes much more than a set of models: it encodes knowledge in the form of the relationships between the elements of those models. In this paper we advocate that the principle of inertia is advantageously applied to the rules of the initial program rather than to the individual literals in a model. Indeed, we show how this concept of program update generalizes model or interpretation updates. Furthermore, it allows us to conceive what it is to update one program by another, a crucial notion for opening up a whole new range of applications concerning the evolution of knowledge bases. We will consider the updating of normal programs as well as these extended with explicit negation, under the stable semantics.</p>","Computer Science, Artificial Intelligence"
"WOS:000083171400009","Generalizing updates: From models to programs","1998","
<p>Recently the field of theory update has seen some improvement, in what concerns model updating, by allowing updates to be specified by so-called revision programs. The updating of theory models is governed by their update rules and also by inertia applied to those literals not directly affected by the update program. Though this is important, it remains necessary to tackle as well the updating of programs specifying theories. Some results have been obtained on the issue of updating a logic program which encodes a set of models, to obtain a new program whose models are the desired updates of the initial models. But here the program only prays the role of a means to encode the models.</p>
<p>A logic program encodes much more than a set of models: it encodes knowledge in the form of the relationships between the elements of those models. In this paper we advocate that the principle of inertia is advantageously applied to the rules of the initial program rather than to the individual literals in a model. Indeed, we show how this concept of program update generalizes model or interpretation updates. Furthermore, it allows us to conceive what it is to update one program by another, a crucial notion for opening up a whole new range of applications concerning the evolution of knowledge bases. We will consider the updating of normal programs as well as these extended with explicit negation, under the stable semantics.</p>","Computer Science, Theory & Methods"
"WOS:000083171400009","Generalizing updates: From models to programs","1998","
<p>Recently the field of theory update has seen some improvement, in what concerns model updating, by allowing updates to be specified by so-called revision programs. The updating of theory models is governed by their update rules and also by inertia applied to those literals not directly affected by the update program. Though this is important, it remains necessary to tackle as well the updating of programs specifying theories. Some results have been obtained on the issue of updating a logic program which encodes a set of models, to obtain a new program whose models are the desired updates of the initial models. But here the program only prays the role of a means to encode the models.</p>
<p>A logic program encodes much more than a set of models: it encodes knowledge in the form of the relationships between the elements of those models. In this paper we advocate that the principle of inertia is advantageously applied to the rules of the initial program rather than to the individual literals in a model. Indeed, we show how this concept of program update generalizes model or interpretation updates. Furthermore, it allows us to conceive what it is to update one program by another, a crucial notion for opening up a whole new range of applications concerning the evolution of knowledge bases. We will consider the updating of normal programs as well as these extended with explicit negation, under the stable semantics.</p>","Computer Science"
"WOS:000077981600009","Using roadmaps to classify regions of space for autonomous robot navigation","1998","
<p>The roadmap approach to robot path planning is one of the earliest methods. Since then, many different algorithms for building roadmaps have been proposed and widely implemented in mobile robots but their use has always been limited to planning in static, totally known environments. In this paper we combine the use of dynamic analogical representations of the environment with an efficient roadmap extraction method, to guide the robot navigation and to classify the different regions of space in which the robot moves. The paper presents the general reference architecture for the robotic system and then focuses on the algorithms for the construction of the roadmap, the classification of the regions of space and their use in robot navigation. Experimental results indicate the applicability and robustness of this approach in real situations. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science, Artificial Intelligence"
"WOS:000077981600009","Using roadmaps to classify regions of space for autonomous robot navigation","1998","
<p>The roadmap approach to robot path planning is one of the earliest methods. Since then, many different algorithms for building roadmaps have been proposed and widely implemented in mobile robots but their use has always been limited to planning in static, totally known environments. In this paper we combine the use of dynamic analogical representations of the environment with an efficient roadmap extraction method, to guide the robot navigation and to classify the different regions of space in which the robot moves. The paper presents the general reference architecture for the robotic system and then focuses on the algorithms for the construction of the roadmap, the classification of the regions of space and their use in robot navigation. Experimental results indicate the applicability and robustness of this approach in real situations. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science"
"WOS:000072805800003","On fuzzy nonlinear regression for image enhancement","1998","
<p>Nonlinear regression analysis with respect to fuzzy characteristic sets, or fuzzy nonlinear regression, is a potentially useful and previously unexplored digital signal processing tool. Here, the fuzzy regression model is used in the image enhancement problem. Given a noisy image, the noise is eliminated by computing a regression-the ""closest"" image to the input image that has membership in the characteristic set. The known properties of the original, uncorrupted imagery (e.g., smoothness) are used to define membership in the characteristic set. With conventional crisp characteristic sets that enforce the characteristic property in a global sense, the local image structure may be sacrificed. In this paper, a method to compute fuzzy nonlinear regressions for the piecewise constant characteristic property is given. Solutions are produced by minimizing an energy functional that penalizes deviation from the sensed (corrupted) image and deviation from piece-wise constancy. The construction of the energy functional, the analytical selection of the functional parameters, the minimization technique used (generalized deterministic annealing), and the fuzzy membership function are detailed. Finally, image enhancement examples are provided for remotely sensed imagery.</p>","Computer Science, Artificial Intelligence"
"WOS:000072805800003","On fuzzy nonlinear regression for image enhancement","1998","
<p>Nonlinear regression analysis with respect to fuzzy characteristic sets, or fuzzy nonlinear regression, is a potentially useful and previously unexplored digital signal processing tool. Here, the fuzzy regression model is used in the image enhancement problem. Given a noisy image, the noise is eliminated by computing a regression-the ""closest"" image to the input image that has membership in the characteristic set. The known properties of the original, uncorrupted imagery (e.g., smoothness) are used to define membership in the characteristic set. With conventional crisp characteristic sets that enforce the characteristic property in a global sense, the local image structure may be sacrificed. In this paper, a method to compute fuzzy nonlinear regressions for the piecewise constant characteristic property is given. Solutions are produced by minimizing an energy functional that penalizes deviation from the sensed (corrupted) image and deviation from piece-wise constancy. The construction of the energy functional, the analytical selection of the functional parameters, the minimization technique used (generalized deterministic annealing), and the fuzzy membership function are detailed. Finally, image enhancement examples are provided for remotely sensed imagery.</p>","Computer Science, Software Engineering"
"WOS:000072805800003","On fuzzy nonlinear regression for image enhancement","1998","
<p>Nonlinear regression analysis with respect to fuzzy characteristic sets, or fuzzy nonlinear regression, is a potentially useful and previously unexplored digital signal processing tool. Here, the fuzzy regression model is used in the image enhancement problem. Given a noisy image, the noise is eliminated by computing a regression-the ""closest"" image to the input image that has membership in the characteristic set. The known properties of the original, uncorrupted imagery (e.g., smoothness) are used to define membership in the characteristic set. With conventional crisp characteristic sets that enforce the characteristic property in a global sense, the local image structure may be sacrificed. In this paper, a method to compute fuzzy nonlinear regressions for the piecewise constant characteristic property is given. Solutions are produced by minimizing an energy functional that penalizes deviation from the sensed (corrupted) image and deviation from piece-wise constancy. The construction of the energy functional, the analytical selection of the functional parameters, the minimization technique used (generalized deterministic annealing), and the fuzzy membership function are detailed. Finally, image enhancement examples are provided for remotely sensed imagery.</p>","Computer Science"
"WOS:000076948200001","Generalization and specialization strategies for learning re languages","1998","
<p>Overgeneralization is a major issue in the identification Of grammars for formal languages from positive data. Different formulations of generalization and specialization strategies have been proposed to address this problem, and recently there has been a flurry of activity investigating such strategies in the context of indexed families of recursive languages. The present paper studies the power of these strategies to learn recursively enumerable languages from positive data. In particular, the power of strong-monotonic, monotonic, and weak-monotonic (together with their dual notions modeling specialization) strategies are investigated for identification of r.e. languages. These investigations turn out to be different from the previous investigations on learning indexed families of recursive languages and at times require new proof techniques. A complete picture is provided for the relative power of each of the strategies considered. An interesting consequence is that the power of weak-monotonic strategies is equivalent to that of conservative strategies. This result parallels the scenario for indexed classes of recursive languages. It is also shown that any identifiable collection of r.e. languages can also be identified by a strategy that exhibits the dual of weak-monotonic property. An immediate consequence of the proof of this result is that if attention is restricted to infinite re. languages, then conservative strategies can identify every identifiable collection.</p>","Computer Science, Artificial Intelligence"
"WOS:000076948200001","Generalization and specialization strategies for learning re languages","1998","
<p>Overgeneralization is a major issue in the identification Of grammars for formal languages from positive data. Different formulations of generalization and specialization strategies have been proposed to address this problem, and recently there has been a flurry of activity investigating such strategies in the context of indexed families of recursive languages. The present paper studies the power of these strategies to learn recursively enumerable languages from positive data. In particular, the power of strong-monotonic, monotonic, and weak-monotonic (together with their dual notions modeling specialization) strategies are investigated for identification of r.e. languages. These investigations turn out to be different from the previous investigations on learning indexed families of recursive languages and at times require new proof techniques. A complete picture is provided for the relative power of each of the strategies considered. An interesting consequence is that the power of weak-monotonic strategies is equivalent to that of conservative strategies. This result parallels the scenario for indexed classes of recursive languages. It is also shown that any identifiable collection of r.e. languages can also be identified by a strategy that exhibits the dual of weak-monotonic property. An immediate consequence of the proof of this result is that if attention is restricted to infinite re. languages, then conservative strategies can identify every identifiable collection.</p>","Computer Science"
"WOS:000080385000003","Convergence assessment techniques for Markov chain Monte Carlo","1998","
<p>MCMC methods have effectively revolutionised the field of Bayesian statistics over the past few years. Such methods provide invaluable tools to overcome problems with analytic intractability inherent in adopting the Bayesian approach to statistical modelling.</p>
<p>However, any inference based upon MCMC output relies critically upon the assumption that the Markov chain being simulated has achieved a steady state or ""converged"". Many techniques have been developed for trying to determine whether or not a particular Markov chain has converged, and this paper aims to review these methods with an emphasis on the mathematics underpinning these techniques, in an attempt to summarise the current ""state-of-play"" for convergence assessment techniques and to motivate directions for future research in this area.</p>","Computer Science, Theory & Methods"
"WOS:000080385000003","Convergence assessment techniques for Markov chain Monte Carlo","1998","
<p>MCMC methods have effectively revolutionised the field of Bayesian statistics over the past few years. Such methods provide invaluable tools to overcome problems with analytic intractability inherent in adopting the Bayesian approach to statistical modelling.</p>
<p>However, any inference based upon MCMC output relies critically upon the assumption that the Markov chain being simulated has achieved a steady state or ""converged"". Many techniques have been developed for trying to determine whether or not a particular Markov chain has converged, and this paper aims to review these methods with an emphasis on the mathematics underpinning these techniques, in an attempt to summarise the current ""state-of-play"" for convergence assessment techniques and to motivate directions for future research in this area.</p>","Computer Science"
"WOS:000072772100001","Model reference adaptive fuzzy control: A linguistic space approach","1998","
<p>In this paper, a model reference adaptive fuzzy controller with the adaptivity achieved by tuning both the scaling factors and the control rules is presented. The proposed tuning strategy includes two parts: (i) by referring a reference trajectory created in the linguistic space to tune the scaling factors; and (2) by applying an auxiliary correction matrix generated by specific tuning rules to compensate the control rules. Since, only the control signal, the output error and change in output error of the plant are needed in the construction of the reference trajectory and the tuning rules, the tuning strategy has the advantage of real-time property. Simulation results with a damping process and an oscillating process as the controlled plants show the performance and adaptivity of the proposed controller. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science, Theory & Methods"
"WOS:000072772100001","Model reference adaptive fuzzy control: A linguistic space approach","1998","
<p>In this paper, a model reference adaptive fuzzy controller with the adaptivity achieved by tuning both the scaling factors and the control rules is presented. The proposed tuning strategy includes two parts: (i) by referring a reference trajectory created in the linguistic space to tune the scaling factors; and (2) by applying an auxiliary correction matrix generated by specific tuning rules to compensate the control rules. Since, only the control signal, the output error and change in output error of the plant are needed in the construction of the reference trajectory and the tuning rules, the tuning strategy has the advantage of real-time property. Simulation results with a damping process and an oscillating process as the controlled plants show the performance and adaptivity of the proposed controller. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science"
"WOS:000073558000009","High-efficiency cell technologies for multi-crystalline silicon solar cells","1998","
<p>Solar cells using multi-crystalline silicon substrates have been developed for cost reduction. The cells using only conventional low cost processes like thermal diffusion and screen printing electrodes could not show sufficient conversion efficiency. High efficiency technologies have been extensively studied for the practical utilization. As a result, a record high conversion efficiency of 17.2% was obtained for a 10 X 10cm(2) cast silicon substrate.</p>
<p>This paper describes high efficiency technologies of the multi-crystalline solar cells and its progress.</p>","Computer Science, Hardware & Architecture"
"WOS:000073558000009","High-efficiency cell technologies for multi-crystalline silicon solar cells","1998","
<p>Solar cells using multi-crystalline silicon substrates have been developed for cost reduction. The cells using only conventional low cost processes like thermal diffusion and screen printing electrodes could not show sufficient conversion efficiency. High efficiency technologies have been extensively studied for the practical utilization. As a result, a record high conversion efficiency of 17.2% was obtained for a 10 X 10cm(2) cast silicon substrate.</p>
<p>This paper describes high efficiency technologies of the multi-crystalline solar cells and its progress.</p>","Computer Science"
"WOS:000071805500002","Semantic indexing and searching using a Hopfield net","1998","
<p>This paper presents a neural network approach to document semantic indexing. A Hopfield net algorithm was used to simulate human associative memory for concept exploration in the domain of computer science and engineering. INSPEC, a collection of more than 320,000 document abstracts from leading journals, was used as the document testbed. Benchmark tests confirmed that three parameters (maximum number of activated nodes, epsilon-maximum allowable error, and maximum number of iterations) were useful in positively influencing network convergence behavior without negatively impacting central processing unit performance. Another series of benchmark tests was performed to determine the effectiveness of various filtering techniques in reducing the negative impact of noisy input terms, Preliminary user tests confirmed our expectation that the Hopfield net algorithm is potentially useful as an associative memory technique to improve document recall and precision by solving discrepancies between indexer vocabularies and end-user vocabularies.</p>","Computer Science, Information Systems"
"WOS:000071805500002","Semantic indexing and searching using a Hopfield net","1998","
<p>This paper presents a neural network approach to document semantic indexing. A Hopfield net algorithm was used to simulate human associative memory for concept exploration in the domain of computer science and engineering. INSPEC, a collection of more than 320,000 document abstracts from leading journals, was used as the document testbed. Benchmark tests confirmed that three parameters (maximum number of activated nodes, epsilon-maximum allowable error, and maximum number of iterations) were useful in positively influencing network convergence behavior without negatively impacting central processing unit performance. Another series of benchmark tests was performed to determine the effectiveness of various filtering techniques in reducing the negative impact of noisy input terms, Preliminary user tests confirmed our expectation that the Hopfield net algorithm is potentially useful as an associative memory technique to improve document recall and precision by solving discrepancies between indexer vocabularies and end-user vocabularies.</p>","Computer Science"
"WOS:000075629600001","On the data expansion of the Huffman compression algorithm","1998","
<p>While compressing a file with a Huffman code, it is possible that the size of the file grows temporarily. This happens when the source letters with low frequencies (to which long codewords are assigned) are encoded first. The maximum data expansion is the average growth in bits per source letter resulting from the encoding of a source letter with a long codeword. It is a measure of the worst case temporary growth of the file. In this paper we study the maximum data expansion of Huffman codes. We provide some new properties of the maximum data expansion delta of Huffman codes and using these properties we prove that delta < 1.256.</p>","Computer Science, Hardware & Architecture"
"WOS:000075629600001","On the data expansion of the Huffman compression algorithm","1998","
<p>While compressing a file with a Huffman code, it is possible that the size of the file grows temporarily. This happens when the source letters with low frequencies (to which long codewords are assigned) are encoded first. The maximum data expansion is the average growth in bits per source letter resulting from the encoding of a source letter with a long codeword. It is a measure of the worst case temporary growth of the file. In this paper we study the maximum data expansion of Huffman codes. We provide some new properties of the maximum data expansion delta of Huffman codes and using these properties we prove that delta < 1.256.</p>","Computer Science, Information Systems"
"WOS:000075629600001","On the data expansion of the Huffman compression algorithm","1998","
<p>While compressing a file with a Huffman code, it is possible that the size of the file grows temporarily. This happens when the source letters with low frequencies (to which long codewords are assigned) are encoded first. The maximum data expansion is the average growth in bits per source letter resulting from the encoding of a source letter with a long codeword. It is a measure of the worst case temporary growth of the file. In this paper we study the maximum data expansion of Huffman codes. We provide some new properties of the maximum data expansion delta of Huffman codes and using these properties we prove that delta < 1.256.</p>","Computer Science, Software Engineering"
"WOS:000075629600001","On the data expansion of the Huffman compression algorithm","1998","
<p>While compressing a file with a Huffman code, it is possible that the size of the file grows temporarily. This happens when the source letters with low frequencies (to which long codewords are assigned) are encoded first. The maximum data expansion is the average growth in bits per source letter resulting from the encoding of a source letter with a long codeword. It is a measure of the worst case temporary growth of the file. In this paper we study the maximum data expansion of Huffman codes. We provide some new properties of the maximum data expansion delta of Huffman codes and using these properties we prove that delta < 1.256.</p>","Computer Science, Theory & Methods"
"WOS:000075629600001","On the data expansion of the Huffman compression algorithm","1998","
<p>While compressing a file with a Huffman code, it is possible that the size of the file grows temporarily. This happens when the source letters with low frequencies (to which long codewords are assigned) are encoded first. The maximum data expansion is the average growth in bits per source letter resulting from the encoding of a source letter with a long codeword. It is a measure of the worst case temporary growth of the file. In this paper we study the maximum data expansion of Huffman codes. We provide some new properties of the maximum data expansion delta of Huffman codes and using these properties we prove that delta < 1.256.</p>","Computer Science"
"WOS:000078121700009","Software reliability modeling: An approach to early reliability prediction","1998","
<p>Models for predicting soft;ware reliability in the early phases of development are of paramount importance since they provide early identification of cost overruns, software development process issues, optimal development strategies, etc. A few models geared towards early reliability prediction, applicable to well defined domains, have been developed during the 1990's. However, many questions related to early prediction are still open, and more research in this area is needed, particularly for developing a generic approach to early reliability prediction.</p>
<p>This paper presents an approach to predicting software reliability based on a systematic identification of soft-ware process failure modes and their likelihoods. A direct consequence of the approach and its supporting data collection efforts is the identification of weak areas in the software development process. A Bayes framework for the quantification of software process failure mode probabilities can be useful since it allows use of historical data that are only partially relevant do the software at hand. The key characteristics of the approach should apply to other software development life-cycles & phases. However, it is unclear how difficult the implementation of the approach would be, and how accurate the predictions would be. Further research will help answer these questions.</p>","Computer Science, Hardware & Architecture"
"WOS:000078121700009","Software reliability modeling: An approach to early reliability prediction","1998","
<p>Models for predicting soft;ware reliability in the early phases of development are of paramount importance since they provide early identification of cost overruns, software development process issues, optimal development strategies, etc. A few models geared towards early reliability prediction, applicable to well defined domains, have been developed during the 1990's. However, many questions related to early prediction are still open, and more research in this area is needed, particularly for developing a generic approach to early reliability prediction.</p>
<p>This paper presents an approach to predicting software reliability based on a systematic identification of soft-ware process failure modes and their likelihoods. A direct consequence of the approach and its supporting data collection efforts is the identification of weak areas in the software development process. A Bayes framework for the quantification of software process failure mode probabilities can be useful since it allows use of historical data that are only partially relevant do the software at hand. The key characteristics of the approach should apply to other software development life-cycles & phases. However, it is unclear how difficult the implementation of the approach would be, and how accurate the predictions would be. Further research will help answer these questions.</p>","Computer Science, Software Engineering"
"WOS:000078121700009","Software reliability modeling: An approach to early reliability prediction","1998","
<p>Models for predicting soft;ware reliability in the early phases of development are of paramount importance since they provide early identification of cost overruns, software development process issues, optimal development strategies, etc. A few models geared towards early reliability prediction, applicable to well defined domains, have been developed during the 1990's. However, many questions related to early prediction are still open, and more research in this area is needed, particularly for developing a generic approach to early reliability prediction.</p>
<p>This paper presents an approach to predicting software reliability based on a systematic identification of soft-ware process failure modes and their likelihoods. A direct consequence of the approach and its supporting data collection efforts is the identification of weak areas in the software development process. A Bayes framework for the quantification of software process failure mode probabilities can be useful since it allows use of historical data that are only partially relevant do the software at hand. The key characteristics of the approach should apply to other software development life-cycles & phases. However, it is unclear how difficult the implementation of the approach would be, and how accurate the predictions would be. Further research will help answer these questions.</p>","Computer Science"
"WOS:000075747300005","Dynamic instability analysis of a laminated composite circular cylindrical shell","1998","
<p>The dynamic instability of laminated composite circular cylindrical shells subjected to periodic load is studied using a C degrees shear flexible two-noded axisymmetric shell element. The formulation includes the effects of inplane and rotary terms. The boundaries of the principal instability regions are conveniently represented in the non-dimensional excitation frequency-non-dimensional load amplitude plane. The effects of various parameters such as ply-angle, thickness, aspect-ratio, axial and circumferential wave numbers on dynamic stability are brought out. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000075747300005","Dynamic instability analysis of a laminated composite circular cylindrical shell","1998","
<p>The dynamic instability of laminated composite circular cylindrical shells subjected to periodic load is studied using a C degrees shear flexible two-noded axisymmetric shell element. The formulation includes the effects of inplane and rotary terms. The boundaries of the principal instability regions are conveniently represented in the non-dimensional excitation frequency-non-dimensional load amplitude plane. The effects of various parameters such as ply-angle, thickness, aspect-ratio, axial and circumferential wave numbers on dynamic stability are brought out. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science"
"WOS:000072427100005","Active structures considering energy dissipation through damping and plastic yielding","1998","
<p>The optimal design of the conventional members of a building equipped with either passive or active control systems is presented for earthquake excitations. The optimized structure with structural control systems is known as an active structure. The influence of the structure's inherent structural damping on the optimal design is included. The energy imparted to a structure designed so that its structural members would yield in a major earthquake is dissipated by damping and yielding. The inherent damping and yielding energy due to the conventional structural members, as well as the external damping energy from passive or active structural control are considered. The optimal design of viscoelastic dampers in terms of their number, location and cross-sectional area to achieve an increased effective damping is examined. Elastic and inelastic analyses of conventional and active structures are performed in which the number of yielding events, and the energy dissipation through damping and yielding are compared for different earthquakes. The structural control systems are found to assist in reducing the peak structural response and the number of yielding events. (C) 1998 Elsevier Science Ltd.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000072427100005","Active structures considering energy dissipation through damping and plastic yielding","1998","
<p>The optimal design of the conventional members of a building equipped with either passive or active control systems is presented for earthquake excitations. The optimized structure with structural control systems is known as an active structure. The influence of the structure's inherent structural damping on the optimal design is included. The energy imparted to a structure designed so that its structural members would yield in a major earthquake is dissipated by damping and yielding. The inherent damping and yielding energy due to the conventional structural members, as well as the external damping energy from passive or active structural control are considered. The optimal design of viscoelastic dampers in terms of their number, location and cross-sectional area to achieve an increased effective damping is examined. Elastic and inelastic analyses of conventional and active structures are performed in which the number of yielding events, and the energy dissipation through damping and yielding are compared for different earthquakes. The structural control systems are found to assist in reducing the peak structural response and the number of yielding events. (C) 1998 Elsevier Science Ltd.</p>","Computer Science"
"WOS:000073237800004","Early computer graphics developments in the architecture, engineering, and construction industry","1998","
<p>Despite 30 years' experimentation and 20 years' availability of commercial products, the architecture, engineering, and construction industry in the mid-1990s had yet to achieve an effective integration of computer-based techniques into its business processes. Business processes in all industries are resistant to change, and people tend to use new tools in the same way they used their old ones: computers as pencils. In the architecture, engineering, and construction industry within the United States, this tendency has been aggravated by the segmentation of the work process into myriad specialties, frequently performed by separate companies, with the information flow obstructed by professional licensing, regulation, contracts, the profit motive, and even the training of design professionals. However, a number of developments-the emergence of object technology; industry standardization initiatives; widespread adoption of Internet technologies; and competitive pressures-are converging to create both the feasibility of and the necessity for rethinking and restructuring the industry. This article focuses on computer graphics precedents related to the architecture, engineering, and construction industry.</p>","Computer Science, Theory & Methods"
"WOS:000073237800004","Early computer graphics developments in the architecture, engineering, and construction industry","1998","
<p>Despite 30 years' experimentation and 20 years' availability of commercial products, the architecture, engineering, and construction industry in the mid-1990s had yet to achieve an effective integration of computer-based techniques into its business processes. Business processes in all industries are resistant to change, and people tend to use new tools in the same way they used their old ones: computers as pencils. In the architecture, engineering, and construction industry within the United States, this tendency has been aggravated by the segmentation of the work process into myriad specialties, frequently performed by separate companies, with the information flow obstructed by professional licensing, regulation, contracts, the profit motive, and even the training of design professionals. However, a number of developments-the emergence of object technology; industry standardization initiatives; widespread adoption of Internet technologies; and competitive pressures-are converging to create both the feasibility of and the necessity for rethinking and restructuring the industry. This article focuses on computer graphics precedents related to the architecture, engineering, and construction industry.</p>","Computer Science"
"WOS:000076442900014","A survey of software reuse libraries","1998","
<p>The study of storage and retrieval methods of software assets in software libraries gives rise to a number of paradoxes: While this subject has been under investigation for nearly two decades, it still remains an active area of research in software reuse and software engineering; this can be explained by the observation that new technologies (such as the internet, the world wide web, object-oriented programming) keep opening new opportunities for better asset packaging, better library organizations, and larger scale libraries - thereby posing new technical challenges. Also, while many sophisticated solutions have been proposed to this problem, the state of the practice in software reuse is characterized by the use of ad-hoc, low-tech methods; this can be explained by the observation that most existing solutions are either too ineffective to be useful or too intractable to be usable. Finally, while it is difficult to imagine a successful software reuse program without a sophisticated, well-tuned, systematic procedure for software component storage and retrieval, it seems many successful software reuse experiments rely on trivial methods of component storage and retrieval; this can be explained by the observation that, in the current state of the practice, software libraries are not the bottleneck of the software reuse process. This paper presents a survey of methods of storage and retrieval of software assets in software libraries. In addition to a review of existing research efforts, the paper makes two contributions. First, a definition of (presumably) orthogonal attributes of storage and retrieval methods; these attributes are used, in turn, to classify existing methods into six broad classes. Second, a definition of (presumably) orthogonal assessment criteria, which include technical, managerial and human factors; these criteria afford us an exhaustive and uniform basis for assessing and comparing individual methods and classes of methods.</p>","Computer Science, Software Engineering"
"WOS:000076442900014","A survey of software reuse libraries","1998","
<p>The study of storage and retrieval methods of software assets in software libraries gives rise to a number of paradoxes: While this subject has been under investigation for nearly two decades, it still remains an active area of research in software reuse and software engineering; this can be explained by the observation that new technologies (such as the internet, the world wide web, object-oriented programming) keep opening new opportunities for better asset packaging, better library organizations, and larger scale libraries - thereby posing new technical challenges. Also, while many sophisticated solutions have been proposed to this problem, the state of the practice in software reuse is characterized by the use of ad-hoc, low-tech methods; this can be explained by the observation that most existing solutions are either too ineffective to be useful or too intractable to be usable. Finally, while it is difficult to imagine a successful software reuse program without a sophisticated, well-tuned, systematic procedure for software component storage and retrieval, it seems many successful software reuse experiments rely on trivial methods of component storage and retrieval; this can be explained by the observation that, in the current state of the practice, software libraries are not the bottleneck of the software reuse process. This paper presents a survey of methods of storage and retrieval of software assets in software libraries. In addition to a review of existing research efforts, the paper makes two contributions. First, a definition of (presumably) orthogonal attributes of storage and retrieval methods; these attributes are used, in turn, to classify existing methods into six broad classes. Second, a definition of (presumably) orthogonal assessment criteria, which include technical, managerial and human factors; these criteria afford us an exhaustive and uniform basis for assessing and comparing individual methods and classes of methods.</p>","Computer Science"
"WOS:000076244000003","Perceiving and reasoning about a changing world","1998","
<p>A rational agent (artificial or otherwise) residing in a complex changing environment must gather information perceptually, update that information as the world changes, and combing that information with causal information to reason about the changing world. Using the system of defeasible reasoning that is incorporated into the OSCAR architecture for rational agents, a set of reason-schemas is proposed for enabling an agent to perform some of the requisite reasoning. Along the way, solutions are proposed for the Frame Problem, the Qualification Problem, and the Ramification Problem. The principles and reasoning described have all been implemented in OSCAR.</p>","Computer Science, Artificial Intelligence"
"WOS:000076244000003","Perceiving and reasoning about a changing world","1998","
<p>A rational agent (artificial or otherwise) residing in a complex changing environment must gather information perceptually, update that information as the world changes, and combing that information with causal information to reason about the changing world. Using the system of defeasible reasoning that is incorporated into the OSCAR architecture for rational agents, a set of reason-schemas is proposed for enabling an agent to perform some of the requisite reasoning. Along the way, solutions are proposed for the Frame Problem, the Qualification Problem, and the Ramification Problem. The principles and reasoning described have all been implemented in OSCAR.</p>","Computer Science"
"WOS:000077156600001","Lossy compression of noisy images","1998","
<p>Noise degrades the performance of any image compression algorithm, This paper studies the effect of noise bn lossy image compression. The effect of Gaussian, Poisson, and film-grain noise on compression is studied. To reduce the effect of the noise on compression, the distortion is measured with respect to the original image not to the input of the coder. Results of noisy source coding are then used to design the optimal coder. In the minimum-mean-square-error (MMSE) sense, this is equivalent to an MMSE estimator followed by an MMSE coder. The coders for the Poisson noise and the film-grain noise cases are derived and their performance is studied. The effect of this preprocessing step is studied using standard coders, e,g,, JPEG, also. As will be demonstrated, higher quality is achieved at lower bit rates.</p>","Computer Science, Artificial Intelligence"
"WOS:000077156600001","Lossy compression of noisy images","1998","
<p>Noise degrades the performance of any image compression algorithm, This paper studies the effect of noise bn lossy image compression. The effect of Gaussian, Poisson, and film-grain noise on compression is studied. To reduce the effect of the noise on compression, the distortion is measured with respect to the original image not to the input of the coder. Results of noisy source coding are then used to design the optimal coder. In the minimum-mean-square-error (MMSE) sense, this is equivalent to an MMSE estimator followed by an MMSE coder. The coders for the Poisson noise and the film-grain noise cases are derived and their performance is studied. The effect of this preprocessing step is studied using standard coders, e,g,, JPEG, also. As will be demonstrated, higher quality is achieved at lower bit rates.</p>","Computer Science"
"WOS:000078245100004","The logical foundations of goal-regression planning in autonomous agents","1998","
<p>This paper addresses the logical foundations of goal-regression planning in autonomous rational agents. It focuses mainly on three problems. The first is that goals and subgoals will often be conjunctions, and to apply goal-regression planning to a conjunction we usually have to plan separately for the conjuncts and then combine the resulting subplans. A logical problem arises from the fact that the subplans may destructively interfere with each other. This problem has been partially solved in the AI literature (e.g., in SNLP and UCPOP), but the solutions proposed there work only when a restrictive assumption is satisfied. This assumption pertains to the computability of threats. It is argued that this assumption may fail for an autonomous rational agent operating in a complex environment. Relaxing this assumption leads to a theory of defeasible planning. The theory is formulated precisely and an implementation in the OSCAR architecture is discussed.</p>
<p>The second problem is that goal-regression planning proceeds in terms of reasoning that runs afoul of the Frame Problem. It is argued that a previously proposed solution to the Frame Problem legitimizes goal-regression planning, but also has the consequence that some restrictions must be imposed on the logical form of goals and subgoals amenable to such planning. These restrictions have to do with temporal-projectibility.</p>
<p>The third problem is that the theory of goal-repression planning found in the Al literature imposes restrictive syntactical constraints on goals and subgoals and on the relation of logical consequence. Relaxing these restrictions leads to a generalization of the notion of a threat, related to collective defeat in defeasible reasoning. Relaxing the restrictions also has the consequence that the previously adequate definition of ""expectable-result"" no longer guarantees closure under logical consequence, and must he revised accordingly. That in turn leads to the need for an additional rule for goal regression planning. Roughly, the rule allows us to plan for the achievement of a goal by searching for plans that will achieve states that ""cause"" the goal. Such a rule was not previously necessary, but becomes necessary when the syntactical constraints are relaxed.</p>
<p>The fnal result is a general semantics for goal-regression planning and a set of procedures that is provably sound and complete. It is shown that this semantics can easily handle concurrent actions, quantified preconditions and effects, creation and destruction of objects, and causal connections embodying complex temporal relationships. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science, Artificial Intelligence"
"WOS:000078245100004","The logical foundations of goal-regression planning in autonomous agents","1998","
<p>This paper addresses the logical foundations of goal-regression planning in autonomous rational agents. It focuses mainly on three problems. The first is that goals and subgoals will often be conjunctions, and to apply goal-regression planning to a conjunction we usually have to plan separately for the conjuncts and then combine the resulting subplans. A logical problem arises from the fact that the subplans may destructively interfere with each other. This problem has been partially solved in the AI literature (e.g., in SNLP and UCPOP), but the solutions proposed there work only when a restrictive assumption is satisfied. This assumption pertains to the computability of threats. It is argued that this assumption may fail for an autonomous rational agent operating in a complex environment. Relaxing this assumption leads to a theory of defeasible planning. The theory is formulated precisely and an implementation in the OSCAR architecture is discussed.</p>
<p>The second problem is that goal-regression planning proceeds in terms of reasoning that runs afoul of the Frame Problem. It is argued that a previously proposed solution to the Frame Problem legitimizes goal-regression planning, but also has the consequence that some restrictions must be imposed on the logical form of goals and subgoals amenable to such planning. These restrictions have to do with temporal-projectibility.</p>
<p>The third problem is that the theory of goal-repression planning found in the Al literature imposes restrictive syntactical constraints on goals and subgoals and on the relation of logical consequence. Relaxing these restrictions leads to a generalization of the notion of a threat, related to collective defeat in defeasible reasoning. Relaxing the restrictions also has the consequence that the previously adequate definition of ""expectable-result"" no longer guarantees closure under logical consequence, and must he revised accordingly. That in turn leads to the need for an additional rule for goal regression planning. Roughly, the rule allows us to plan for the achievement of a goal by searching for plans that will achieve states that ""cause"" the goal. Such a rule was not previously necessary, but becomes necessary when the syntactical constraints are relaxed.</p>
<p>The fnal result is a general semantics for goal-regression planning and a set of procedures that is provably sound and complete. It is shown that this semantics can easily handle concurrent actions, quantified preconditions and effects, creation and destruction of objects, and causal connections embodying complex temporal relationships. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science"
"WOS:000075088400003","Aspects of network training and validation on noisy data - Part 1. Training aspects","1998","
<p>Multi-layered feed-forward (MLF) neural networks are commonly trained by the generalized Delta learning rule. The training method has shown to be robust and easy to implement for various chemical problems in analytical chemistry. However, the slow and unpredictable learning behavior puts considerable limitations to the interpretation and validation of neural network models. In this two-part paper, both training and validation aspects are addressed. The first part of this paper focuses to the generalized Delta learning rule and some important aspects of network training. It is shown that the use of quasi-Newton training on unfolded MLF networks, accelerates the training time to an order of magnitude. The learning behavior of MLF-networks trained by the generalised Delta rule and the quasi-Newton learning rule are compared by means of simulated and real data from analytical chemistry. Some conclusions about the general applicability of the discussed methods are drawn. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science, Artificial Intelligence"
"WOS:000075088400003","Aspects of network training and validation on noisy data - Part 1. Training aspects","1998","
<p>Multi-layered feed-forward (MLF) neural networks are commonly trained by the generalized Delta learning rule. The training method has shown to be robust and easy to implement for various chemical problems in analytical chemistry. However, the slow and unpredictable learning behavior puts considerable limitations to the interpretation and validation of neural network models. In this two-part paper, both training and validation aspects are addressed. The first part of this paper focuses to the generalized Delta learning rule and some important aspects of network training. It is shown that the use of quasi-Newton training on unfolded MLF networks, accelerates the training time to an order of magnitude. The learning behavior of MLF-networks trained by the generalised Delta rule and the quasi-Newton learning rule are compared by means of simulated and real data from analytical chemistry. Some conclusions about the general applicability of the discussed methods are drawn. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science"
"WOS:000077700700022","An iterative algorithm for X-ray CT fluoroscopy","1998","
<p>X-ray computed tomography fluoroscopy (CTF) enables image guidance of interventions, synchronization of scanning with contrast bolus arrival, and motion analysis. However, filtered backprojection (FB), the current method for CTP image reconstruction, is subject to motion and metal artifacts from implants, needles, or other surgical instruments. Reduced target lesion conspicuity may result from increased image noise associated with reduced tube current. In this report, we adapt the row-action expectation-maximization (EM) algorithm for CTF. Because time-dependent variation in images is localized during CTF, the row-action EM-like algorithm allows rapid convergence. More importantly, this iterative CTF algorithm has fewer metal artifacts and better low-contrast performance than FB.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000077700700022","An iterative algorithm for X-ray CT fluoroscopy","1998","
<p>X-ray computed tomography fluoroscopy (CTF) enables image guidance of interventions, synchronization of scanning with contrast bolus arrival, and motion analysis. However, filtered backprojection (FB), the current method for CTP image reconstruction, is subject to motion and metal artifacts from implants, needles, or other surgical instruments. Reduced target lesion conspicuity may result from increased image noise associated with reduced tube current. In this report, we adapt the row-action expectation-maximization (EM) algorithm for CTF. Because time-dependent variation in images is localized during CTF, the row-action EM-like algorithm allows rapid convergence. More importantly, this iterative CTF algorithm has fewer metal artifacts and better low-contrast performance than FB.</p>","Computer Science"
"WOS:000077859900003","The RDS Business Reference Suite (TM) - Special emphasis on TableBase (TM) and Contemporary Women's Issues (R)","1998","
<p>This article completes the authors' presentation of electronic databases that every college or university library should review. The RDS Business Reference Suite(TM) from Responsive Database Services offers Business & Industry(R) (B&I(TM)), TableBase(TM), and Business & Management Practices(TM) (BaMP(TM)). Contemporary Women's issues(R) (CWI(TM)), powerful addition to any college or university electronic collection, will also be featured. The databases are available in a variety of formats. Two of the four databases will be emphasized: (1) TableBase(TM), which is unique in its field because it offers statistical data (such as marker share, rankings, forecasts, product and industry sales, capital expenditures, production)from ninety industries internationally in tables, and (2) Contemporary Women's Issues(R), which includes primary source material and statistics regarding women's studies that are hard to find in a single source The University of San Francisco subscribes to CWI(TM).</p>","Computer Science, Information Systems"
"WOS:000077859900003","The RDS Business Reference Suite (TM) - Special emphasis on TableBase (TM) and Contemporary Women's Issues (R)","1998","
<p>This article completes the authors' presentation of electronic databases that every college or university library should review. The RDS Business Reference Suite(TM) from Responsive Database Services offers Business & Industry(R) (B&I(TM)), TableBase(TM), and Business & Management Practices(TM) (BaMP(TM)). Contemporary Women's issues(R) (CWI(TM)), powerful addition to any college or university electronic collection, will also be featured. The databases are available in a variety of formats. Two of the four databases will be emphasized: (1) TableBase(TM), which is unique in its field because it offers statistical data (such as marker share, rankings, forecasts, product and industry sales, capital expenditures, production)from ninety industries internationally in tables, and (2) Contemporary Women's Issues(R), which includes primary source material and statistics regarding women's studies that are hard to find in a single source The University of San Francisco subscribes to CWI(TM).</p>","Computer Science, Software Engineering"
"WOS:000077859900003","The RDS Business Reference Suite (TM) - Special emphasis on TableBase (TM) and Contemporary Women's Issues (R)","1998","
<p>This article completes the authors' presentation of electronic databases that every college or university library should review. The RDS Business Reference Suite(TM) from Responsive Database Services offers Business & Industry(R) (B&I(TM)), TableBase(TM), and Business & Management Practices(TM) (BaMP(TM)). Contemporary Women's issues(R) (CWI(TM)), powerful addition to any college or university electronic collection, will also be featured. The databases are available in a variety of formats. Two of the four databases will be emphasized: (1) TableBase(TM), which is unique in its field because it offers statistical data (such as marker share, rankings, forecasts, product and industry sales, capital expenditures, production)from ninety industries internationally in tables, and (2) Contemporary Women's Issues(R), which includes primary source material and statistics regarding women's studies that are hard to find in a single source The University of San Francisco subscribes to CWI(TM).</p>","Computer Science"
"WOS:000082482800126","NAS integer sort on multi-threaded shared memory machines","1998","
<p>Multi-threaded shared memory machines, like the commercial Tera MTA or the experimental SB-PRAM, have an extremely good performance on the Integer Sort benchmark of the NAS Parallel Benchmark Suite and are expected to scale. The number of CPU cycles is an order of magnitude lower than the numbers reported of general purpose distributed memory or shared memory machines; even Vector computers are slower. The reasons for this behavior are investigated. It turns out that both machines can take advantage of a fetch-and-add operation and that due to multi-threading no time is lost waiting for memory accesses to complete. Except for non-scalable vector computers, the Gray T3E, which supports fetch-and-add but not multi-threading, is the only parallel computer that could challenge these machines.</p>","Computer Science, Hardware & Architecture"
"WOS:000082482800126","NAS integer sort on multi-threaded shared memory machines","1998","
<p>Multi-threaded shared memory machines, like the commercial Tera MTA or the experimental SB-PRAM, have an extremely good performance on the Integer Sort benchmark of the NAS Parallel Benchmark Suite and are expected to scale. The number of CPU cycles is an order of magnitude lower than the numbers reported of general purpose distributed memory or shared memory machines; even Vector computers are slower. The reasons for this behavior are investigated. It turns out that both machines can take advantage of a fetch-and-add operation and that due to multi-threading no time is lost waiting for memory accesses to complete. Except for non-scalable vector computers, the Gray T3E, which supports fetch-and-add but not multi-threading, is the only parallel computer that could challenge these machines.</p>","Computer Science, Information Systems"
"WOS:000082482800126","NAS integer sort on multi-threaded shared memory machines","1998","
<p>Multi-threaded shared memory machines, like the commercial Tera MTA or the experimental SB-PRAM, have an extremely good performance on the Integer Sort benchmark of the NAS Parallel Benchmark Suite and are expected to scale. The number of CPU cycles is an order of magnitude lower than the numbers reported of general purpose distributed memory or shared memory machines; even Vector computers are slower. The reasons for this behavior are investigated. It turns out that both machines can take advantage of a fetch-and-add operation and that due to multi-threading no time is lost waiting for memory accesses to complete. Except for non-scalable vector computers, the Gray T3E, which supports fetch-and-add but not multi-threading, is the only parallel computer that could challenge these machines.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000082482800126","NAS integer sort on multi-threaded shared memory machines","1998","
<p>Multi-threaded shared memory machines, like the commercial Tera MTA or the experimental SB-PRAM, have an extremely good performance on the Integer Sort benchmark of the NAS Parallel Benchmark Suite and are expected to scale. The number of CPU cycles is an order of magnitude lower than the numbers reported of general purpose distributed memory or shared memory machines; even Vector computers are slower. The reasons for this behavior are investigated. It turns out that both machines can take advantage of a fetch-and-add operation and that due to multi-threading no time is lost waiting for memory accesses to complete. Except for non-scalable vector computers, the Gray T3E, which supports fetch-and-add but not multi-threading, is the only parallel computer that could challenge these machines.</p>","Computer Science, Software Engineering"
"WOS:000082482800126","NAS integer sort on multi-threaded shared memory machines","1998","
<p>Multi-threaded shared memory machines, like the commercial Tera MTA or the experimental SB-PRAM, have an extremely good performance on the Integer Sort benchmark of the NAS Parallel Benchmark Suite and are expected to scale. The number of CPU cycles is an order of magnitude lower than the numbers reported of general purpose distributed memory or shared memory machines; even Vector computers are slower. The reasons for this behavior are investigated. It turns out that both machines can take advantage of a fetch-and-add operation and that due to multi-threading no time is lost waiting for memory accesses to complete. Except for non-scalable vector computers, the Gray T3E, which supports fetch-and-add but not multi-threading, is the only parallel computer that could challenge these machines.</p>","Computer Science, Theory & Methods"
"WOS:000082482800126","NAS integer sort on multi-threaded shared memory machines","1998","
<p>Multi-threaded shared memory machines, like the commercial Tera MTA or the experimental SB-PRAM, have an extremely good performance on the Integer Sort benchmark of the NAS Parallel Benchmark Suite and are expected to scale. The number of CPU cycles is an order of magnitude lower than the numbers reported of general purpose distributed memory or shared memory machines; even Vector computers are slower. The reasons for this behavior are investigated. It turns out that both machines can take advantage of a fetch-and-add operation and that due to multi-threading no time is lost waiting for memory accesses to complete. Except for non-scalable vector computers, the Gray T3E, which supports fetch-and-add but not multi-threading, is the only parallel computer that could challenge these machines.</p>","Computer Science"
"WOS:000078841400027","Deleting redundancy in proof reconstruction","1998","
<p>We present a framework for eliminating redundancies during the reconstruction of sequent proofs from matrix proofs. We show that search-free proof reconstruction requires knowledge from the proof search process. We relate different levels of proof knowledge to reconstruction knowledge and analyze which redundancies can be deleted by using such knowledge. Our framework is uniformly applicable to classical logic and all non-classical logics which have a matrix characterization of validity and enables us to build adequate conversion procedures for each logic.</p>","Computer Science, Information Systems"
"WOS:000078841400027","Deleting redundancy in proof reconstruction","1998","
<p>We present a framework for eliminating redundancies during the reconstruction of sequent proofs from matrix proofs. We show that search-free proof reconstruction requires knowledge from the proof search process. We relate different levels of proof knowledge to reconstruction knowledge and analyze which redundancies can be deleted by using such knowledge. Our framework is uniformly applicable to classical logic and all non-classical logics which have a matrix characterization of validity and enables us to build adequate conversion procedures for each logic.</p>","Computer Science"
"WOS:000073558000003","Solar cells and environment","1998","
<p>Photovoltaic (PV) system discharges no carbon dioxide, which is the cause for the greenhouse effect? because PV system converts dean energy from sun into electricity. There is discharge of carbon dioxide during the production of PV system, because electricity from thermal power generation is used. The discharge quantity is equivalent to 20.1g-C/kWh of carbon as carbon dioxide, when PV system is used for 20 years. The discharge from thermal power generation is 194.4g-C/kWh. As a result, replacing thermal system with PV system can reduce 174.3g-C/kWh.</p>
<p>For example, 3kW PV roof top system can reduce about 550kg carbon dioxide every year. This amount is equivalent to reduction of 45.1% of discharge (1.22t) from a Japanese typical family of four.</p>","Computer Science, Hardware & Architecture"
"WOS:000073558000003","Solar cells and environment","1998","
<p>Photovoltaic (PV) system discharges no carbon dioxide, which is the cause for the greenhouse effect? because PV system converts dean energy from sun into electricity. There is discharge of carbon dioxide during the production of PV system, because electricity from thermal power generation is used. The discharge quantity is equivalent to 20.1g-C/kWh of carbon as carbon dioxide, when PV system is used for 20 years. The discharge from thermal power generation is 194.4g-C/kWh. As a result, replacing thermal system with PV system can reduce 174.3g-C/kWh.</p>
<p>For example, 3kW PV roof top system can reduce about 550kg carbon dioxide every year. This amount is equivalent to reduction of 45.1% of discharge (1.22t) from a Japanese typical family of four.</p>","Computer Science"
"WOS:000073729400007","The impact of technology on cooperative work groups","1998","
<p>The impact of technology on group process and performance operates in dynamic interdependence with key features of the group composition, task, and situation. Today it is possible to merge an understanding of technology with an understanding of group dynamics. This can be the basis for a new way of conducting business, where information and time are focal points of business strategy.</p>","Computer Science, Information Systems"
"WOS:000073729400007","The impact of technology on cooperative work groups","1998","
<p>The impact of technology on group process and performance operates in dynamic interdependence with key features of the group composition, task, and situation. Today it is possible to merge an understanding of technology with an understanding of group dynamics. This can be the basis for a new way of conducting business, where information and time are focal points of business strategy.</p>","Computer Science"
"WOS:000075188400003","A geometric algorithm for single selective disassembly using the wave propagation abstraction","1998","
<p>Disassembling a selected component from an assembly, defined as selective disassembly, is important for applications such as maintenance, recycling and reuse. This paper presents a geometric algorithm to solve the following selective disassembly problem: given an assembly and a selected component to be disassembled, determine the disassembly sequence with minimum component removals (motions), defined as an optimum sequence. We propose an abstraction ""wave propagation"" that analyzes the assembly from the selected component outwards, and orders the components for selective disassembly. The main contributions of this research are: (1) determining an optimal disassembly sequence; (2) reducing the search space by analyzing a subset of components in the assembly; and (3) providing a polynomial average complexity algorithm. The proposed selective disassembly approach applies to both two dimensional and three-dimensional assemblies. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science, Software Engineering"
"WOS:000075188400003","A geometric algorithm for single selective disassembly using the wave propagation abstraction","1998","
<p>Disassembling a selected component from an assembly, defined as selective disassembly, is important for applications such as maintenance, recycling and reuse. This paper presents a geometric algorithm to solve the following selective disassembly problem: given an assembly and a selected component to be disassembled, determine the disassembly sequence with minimum component removals (motions), defined as an optimum sequence. We propose an abstraction ""wave propagation"" that analyzes the assembly from the selected component outwards, and orders the components for selective disassembly. The main contributions of this research are: (1) determining an optimal disassembly sequence; (2) reducing the search space by analyzing a subset of components in the assembly; and (3) providing a polynomial average complexity algorithm. The proposed selective disassembly approach applies to both two dimensional and three-dimensional assemblies. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science"
"WOS:000073111300010","The problem of local synthesis of quasioptimal control in a stochastic nonlinear dynamic system","1998","
<p>The problem of determination of optimal control in a stochastic nonlinear system is considered on the basis of the local quadratic criterion. An alternative method of synthesis of parameters of control vector with the use of a given structure of dynamic system is proposed.</p>","Computer Science, Artificial Intelligence"
"WOS:000073111300010","The problem of local synthesis of quasioptimal control in a stochastic nonlinear dynamic system","1998","
<p>The problem of determination of optimal control in a stochastic nonlinear system is considered on the basis of the local quadratic criterion. An alternative method of synthesis of parameters of control vector with the use of a given structure of dynamic system is proposed.</p>","Computer Science, Cybernetics"
"WOS:000073111300010","The problem of local synthesis of quasioptimal control in a stochastic nonlinear dynamic system","1998","
<p>The problem of determination of optimal control in a stochastic nonlinear system is considered on the basis of the local quadratic criterion. An alternative method of synthesis of parameters of control vector with the use of a given structure of dynamic system is proposed.</p>","Computer Science, Theory & Methods"
"WOS:000073111300010","The problem of local synthesis of quasioptimal control in a stochastic nonlinear dynamic system","1998","
<p>The problem of determination of optimal control in a stochastic nonlinear system is considered on the basis of the local quadratic criterion. An alternative method of synthesis of parameters of control vector with the use of a given structure of dynamic system is proposed.</p>","Computer Science"
"WOS:000075073900009","A new highly convergent Monte Carlo method for matrix computations","1998","
<p>In this paper a second degree iterative Monte Carlo method for solving systems of linear algebraic equations and matrix inversion is presented. Comparisons are made with iterative Monte Carlo methods with degree one. It is shown that the mean value of the number of chains N, and the chain length T, required to reach given precision can be reduced. The following estimate on N is obtained: N = N-c/(c(N) + bN(c)(1/2))(2), where N-c is the number of chains in the usual degree one method. In addition it is shown that b>0 and that N < N-c/c(N)(2). This result shows that for our method the number of realizations N can be at least c(N)(2) times less than the number of realizations N-c of the existing Monte Carlo method. For parallel implementation, i.e. regular arrays or MIMD distributed memory architectures, these results imply faster algorithms and the reduction of the size of the arrays. This leads also in applying such methods to the problems with smaller sizes, since until now Monte Carlo methods are applicable for large scale problems and when the component of the solution vector or element or row of the inverse matrix has to be found. (C) 1998 IMACS/Elsevier Science B.V.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000075073900009","A new highly convergent Monte Carlo method for matrix computations","1998","
<p>In this paper a second degree iterative Monte Carlo method for solving systems of linear algebraic equations and matrix inversion is presented. Comparisons are made with iterative Monte Carlo methods with degree one. It is shown that the mean value of the number of chains N, and the chain length T, required to reach given precision can be reduced. The following estimate on N is obtained: N = N-c/(c(N) + bN(c)(1/2))(2), where N-c is the number of chains in the usual degree one method. In addition it is shown that b>0 and that N < N-c/c(N)(2). This result shows that for our method the number of realizations N can be at least c(N)(2) times less than the number of realizations N-c of the existing Monte Carlo method. For parallel implementation, i.e. regular arrays or MIMD distributed memory architectures, these results imply faster algorithms and the reduction of the size of the arrays. This leads also in applying such methods to the problems with smaller sizes, since until now Monte Carlo methods are applicable for large scale problems and when the component of the solution vector or element or row of the inverse matrix has to be found. (C) 1998 IMACS/Elsevier Science B.V.</p>","Computer Science, Software Engineering"
"WOS:000075073900009","A new highly convergent Monte Carlo method for matrix computations","1998","
<p>In this paper a second degree iterative Monte Carlo method for solving systems of linear algebraic equations and matrix inversion is presented. Comparisons are made with iterative Monte Carlo methods with degree one. It is shown that the mean value of the number of chains N, and the chain length T, required to reach given precision can be reduced. The following estimate on N is obtained: N = N-c/(c(N) + bN(c)(1/2))(2), where N-c is the number of chains in the usual degree one method. In addition it is shown that b>0 and that N < N-c/c(N)(2). This result shows that for our method the number of realizations N can be at least c(N)(2) times less than the number of realizations N-c of the existing Monte Carlo method. For parallel implementation, i.e. regular arrays or MIMD distributed memory architectures, these results imply faster algorithms and the reduction of the size of the arrays. This leads also in applying such methods to the problems with smaller sizes, since until now Monte Carlo methods are applicable for large scale problems and when the component of the solution vector or element or row of the inverse matrix has to be found. (C) 1998 IMACS/Elsevier Science B.V.</p>","Computer Science"
"WOS:000085483300019","Steganalysis of images created using current steganography software","1998","
<p>Steganography is the art of passing information in a manner that the very existence of the message is unknown. The goal of steganography is to avoid drawing suspicion to the transmission of a hidden message. If suspicion is raised, then this goal is defeated. Steganalysis is the art of discovering and rendering useless such covert messages. In this paper, we identify characteristics in current steganography software that direct the steganalyst to the existence of a hidden message and introduce the ground work of a tool for automatically detecting the existence of hidden messages in images.</p>","Computer Science, Theory & Methods"
"WOS:000085483300019","Steganalysis of images created using current steganography software","1998","
<p>Steganography is the art of passing information in a manner that the very existence of the message is unknown. The goal of steganography is to avoid drawing suspicion to the transmission of a hidden message. If suspicion is raised, then this goal is defeated. Steganalysis is the art of discovering and rendering useless such covert messages. In this paper, we identify characteristics in current steganography software that direct the steganalyst to the existence of a hidden message and introduce the ground work of a tool for automatically detecting the existence of hidden messages in images.</p>","Computer Science"
"WOS:000075284000001","Improved solutions for the traveling purchaser problem","1998","
<p>The traveling purchaser problem (TPP) is an interesting generalization of the well-known traveling salesman problem (TSP), in which a list of commodity items have to be purchased at some markets selling various commodities with different prices, and the total travel and purchase costs must be minimized. Applications include the purchase of raw materials for the manufacturing factories in which the total cost has to be minimized, and the scheduling of jobs over some machines with different set-up and job processing costs in which the total coats for completing the jobs has to be minimized. The TPP has been shown to be computationally intractable. Therefore, many heuristic solution procedures, including the Search algorithm, the Generalized-Savings algorithm, the Tour-Reduction algorithm, and the Commodity-Adding algorithm have been proposed to solve the TPP approximately. In this paper, we consider some variations of these algorithms to improve the solutions. The proposed variations are compared with the existing solution procedures. The results indicate that the proposed variations significantly improve the existing solutions. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000075284000001","Improved solutions for the traveling purchaser problem","1998","
<p>The traveling purchaser problem (TPP) is an interesting generalization of the well-known traveling salesman problem (TSP), in which a list of commodity items have to be purchased at some markets selling various commodities with different prices, and the total travel and purchase costs must be minimized. Applications include the purchase of raw materials for the manufacturing factories in which the total cost has to be minimized, and the scheduling of jobs over some machines with different set-up and job processing costs in which the total coats for completing the jobs has to be minimized. The TPP has been shown to be computationally intractable. Therefore, many heuristic solution procedures, including the Search algorithm, the Generalized-Savings algorithm, the Tour-Reduction algorithm, and the Commodity-Adding algorithm have been proposed to solve the TPP approximately. In this paper, we consider some variations of these algorithms to improve the solutions. The proposed variations are compared with the existing solution procedures. The results indicate that the proposed variations significantly improve the existing solutions. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science"
"WOS:000074766300011","Minimizing average completion time in the presence of release dates","1998","
<p>A natural and basic problem in scheduling theory is to provide good average quality of service to a stream of jobs that arrive over time. In this paper we consider the problem of scheduling n jobs that are released over time in order to minimize the average completion time of the set of jobs. In contrast to the problem of minimizing average completion time when all jobs are available at time 0, all the problems that we consider are NP-hard, and essentially nothing was known about constructing good approximations ill polynomial time. We give the first constant-factor approximation algorithms for several variants of the single and parallel machine models. Many of the algorithms are based on interesting algorithmic and structural relationships between preemptive and nonpreemptive schedules and linear programming relaxations of both, Many of the algorithms generalize to the minimization of average weighted completion time as well. (C) 1998 The Mathematical Programming Society, Inc.Published by Elsevier Science B.V.</p>","Computer Science, Software Engineering"
"WOS:000074766300011","Minimizing average completion time in the presence of release dates","1998","
<p>A natural and basic problem in scheduling theory is to provide good average quality of service to a stream of jobs that arrive over time. In this paper we consider the problem of scheduling n jobs that are released over time in order to minimize the average completion time of the set of jobs. In contrast to the problem of minimizing average completion time when all jobs are available at time 0, all the problems that we consider are NP-hard, and essentially nothing was known about constructing good approximations ill polynomial time. We give the first constant-factor approximation algorithms for several variants of the single and parallel machine models. Many of the algorithms are based on interesting algorithmic and structural relationships between preemptive and nonpreemptive schedules and linear programming relaxations of both, Many of the algorithms generalize to the minimization of average weighted completion time as well. (C) 1998 The Mathematical Programming Society, Inc.Published by Elsevier Science B.V.</p>","Computer Science"
"WOS:000073360600068","A publishing system to support secondary school teaching and learning","1998","
<p>We describe a publishing system which: supports a large number of geographically dispersed publishers; scales to very large information systems; supports publishers with minimal Internet knowledge: and reduces the infrastructure requirements of publishers. (C) 1998 Published by Elsevier Science B.V. All rights reserved.</p>","Computer Science, Information Systems"
"WOS:000073360600068","A publishing system to support secondary school teaching and learning","1998","
<p>We describe a publishing system which: supports a large number of geographically dispersed publishers; scales to very large information systems; supports publishers with minimal Internet knowledge: and reduces the infrastructure requirements of publishers. (C) 1998 Published by Elsevier Science B.V. All rights reserved.</p>","Computer Science"
"WOS:000171768600054","Strategies for problem list implementation in a complex clinical enterprise","1998","
<p>Although the Institute of Medicine states that a patient problem list should have a prominent place in the computer-based patient record, the design and function of the problem list is not a matter of universal agreement. Developer experience with implementation has been inconsistent, in part because of confusion on data standards, uncertain user acceptance of data entry, and minimal rewards for the clinician. I propose that necessary features of the problem list include: 1)clinical focus, 2) codification of problems, 3) support for problem resolution, 4) historicity of problems, 5)support for multiple clinical views, 6) integration of maintenance functions with workflow, 7) support for administrative reporting, and 8) integration with useful clinical tools. I describe the strategies that we employed to meet these goals while implementing the problem list in a computerized patient record serving a large, complex clinical enterprise. I further report the successful achievement of those goals based upon audits six months after implementation.</p>","Computer Science, Information Systems"
"WOS:000171768600054","Strategies for problem list implementation in a complex clinical enterprise","1998","
<p>Although the Institute of Medicine states that a patient problem list should have a prominent place in the computer-based patient record, the design and function of the problem list is not a matter of universal agreement. Developer experience with implementation has been inconsistent, in part because of confusion on data standards, uncertain user acceptance of data entry, and minimal rewards for the clinician. I propose that necessary features of the problem list include: 1)clinical focus, 2) codification of problems, 3) support for problem resolution, 4) historicity of problems, 5)support for multiple clinical views, 6) integration of maintenance functions with workflow, 7) support for administrative reporting, and 8) integration with useful clinical tools. I describe the strategies that we employed to meet these goals while implementing the problem list in a computerized patient record serving a large, complex clinical enterprise. I further report the successful achievement of those goals based upon audits six months after implementation.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000171768600054","Strategies for problem list implementation in a complex clinical enterprise","1998","
<p>Although the Institute of Medicine states that a patient problem list should have a prominent place in the computer-based patient record, the design and function of the problem list is not a matter of universal agreement. Developer experience with implementation has been inconsistent, in part because of confusion on data standards, uncertain user acceptance of data entry, and minimal rewards for the clinician. I propose that necessary features of the problem list include: 1)clinical focus, 2) codification of problems, 3) support for problem resolution, 4) historicity of problems, 5)support for multiple clinical views, 6) integration of maintenance functions with workflow, 7) support for administrative reporting, and 8) integration with useful clinical tools. I describe the strategies that we employed to meet these goals while implementing the problem list in a computerized patient record serving a large, complex clinical enterprise. I further report the successful achievement of those goals based upon audits six months after implementation.</p>","Computer Science"
"WOS:000075063000005","Probe orientation for coordinate measuring machine systems using design models","1998","
<p>The rapidly increasing usage of coordinate measuring machines in computer-integrated manufacturing environments underscores the need for a more science-based approach to inspection than the current ad hoc approach now in place. In this context we address one element of the inspection process, namely, the location of sample points. We present a method for evaluating the location of sample points using the workpiece model from the design. Accessibility analysis is used to determine possible interference between the workpiece model and probe model. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000075063000005","Probe orientation for coordinate measuring machine systems using design models","1998","
<p>The rapidly increasing usage of coordinate measuring machines in computer-integrated manufacturing environments underscores the need for a more science-based approach to inspection than the current ad hoc approach now in place. In this context we address one element of the inspection process, namely, the location of sample points. We present a method for evaluating the location of sample points using the workpiece model from the design. Accessibility analysis is used to determine possible interference between the workpiece model and probe model. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science"
"WOS:000073101600001","Primary-fuzzy-sets-based normal fuzzy reasoning methodology and its applications","1998","
<p>Based on more useful and more heuristic primary fuzzy sets contained by conventionally used fuzzy sets, a novel normal fuzzy reasoning methodology is proposed in order to overcome the weakness of many conventional fuzzy systems. Analysis indicated that the compositional rule of inference Zadeh [IEEE Transactions on Systems, Man, and Cybernetics, 3, 28-44 (1973)] and related defuzzification schemes are not effective since they may generate unreasonable results such as the constant-speed problem and the unstable cart-pole problem. The new normal fuzzy system is a generalized framework of many conventional fuzzy systems such as Takagi-Sugeno's fuzzy system Sugeno and Yasukawa [IEEE Transactions on Fuzzy Systems, 1, 7-30 (1993)] and Takagi and Sugeno [IEEE Transactions on Systems, Man, and Cybernetics, SMC-15, 116-132 (1985)], Wang's fuzzy system Wang [Adaptive Fuzzy Systems and Control Design and Stability Analysis, Prentice-Hall, Englewood Cliffs, 1994], Lin's fuzzy system Lin [Neural Fuzzy Control Systems with Structure and Parameter Learning, World Scientific, Singapore, 1994] and the fuzzy system using Yager's level set method Figueirdo et al. [IEEE Transactions on Fuzzy Systems, 1, 156-159 (1993)]. According to the normal fuzzy reasoning methodology, Takagi-Sugeno's fuzzy system can be constructed directly from high-level fuzzy rules without using the complex parameter estimation algorithm Lin [Neural Fuzzy Control Systems with Structure and Parameter Learning, World Scientific, Singapore, 1994] and Sugeno and Yasukawa [IEEE Transactions on Fuzzy Systems, 1, 7-30 (1993)]. Simulations of the constant-speed problems, the fuzzy multiplications and the cart-pole system strongly indicated that the new normal fuzzy system is more effective and more robust than many conventional fuzzy systems. Therefore, the normal fuzzy reasoning methodology can be used to efficiently construct robust fuzzy systems. (C) 1998 John Wiley & Sons, Inc.</p>","Computer Science, Artificial Intelligence"
"WOS:000073101600001","Primary-fuzzy-sets-based normal fuzzy reasoning methodology and its applications","1998","
<p>Based on more useful and more heuristic primary fuzzy sets contained by conventionally used fuzzy sets, a novel normal fuzzy reasoning methodology is proposed in order to overcome the weakness of many conventional fuzzy systems. Analysis indicated that the compositional rule of inference Zadeh [IEEE Transactions on Systems, Man, and Cybernetics, 3, 28-44 (1973)] and related defuzzification schemes are not effective since they may generate unreasonable results such as the constant-speed problem and the unstable cart-pole problem. The new normal fuzzy system is a generalized framework of many conventional fuzzy systems such as Takagi-Sugeno's fuzzy system Sugeno and Yasukawa [IEEE Transactions on Fuzzy Systems, 1, 7-30 (1993)] and Takagi and Sugeno [IEEE Transactions on Systems, Man, and Cybernetics, SMC-15, 116-132 (1985)], Wang's fuzzy system Wang [Adaptive Fuzzy Systems and Control Design and Stability Analysis, Prentice-Hall, Englewood Cliffs, 1994], Lin's fuzzy system Lin [Neural Fuzzy Control Systems with Structure and Parameter Learning, World Scientific, Singapore, 1994] and the fuzzy system using Yager's level set method Figueirdo et al. [IEEE Transactions on Fuzzy Systems, 1, 156-159 (1993)]. According to the normal fuzzy reasoning methodology, Takagi-Sugeno's fuzzy system can be constructed directly from high-level fuzzy rules without using the complex parameter estimation algorithm Lin [Neural Fuzzy Control Systems with Structure and Parameter Learning, World Scientific, Singapore, 1994] and Sugeno and Yasukawa [IEEE Transactions on Fuzzy Systems, 1, 7-30 (1993)]. Simulations of the constant-speed problems, the fuzzy multiplications and the cart-pole system strongly indicated that the new normal fuzzy system is more effective and more robust than many conventional fuzzy systems. Therefore, the normal fuzzy reasoning methodology can be used to efficiently construct robust fuzzy systems. (C) 1998 John Wiley & Sons, Inc.</p>","Computer Science"
"WOS:000071961900006","Value sets of some polynomials over finite fields GF(2(2m))","1998","
<p>This paper shows that there is a connection between the crosscorrelation functions of certain binary m-sequences and the value sets of the polynomials x(k)(1 + x)(2m-1) for k is an element of {+/-1, +/-2,4}, where x is in the finite field GF(2(2m)). In particular, the size of such value sets is determined by using finite field theory and known results about crosscorrelation functions.</p>","Computer Science, Theory & Methods"
"WOS:000071961900006","Value sets of some polynomials over finite fields GF(2(2m))","1998","
<p>This paper shows that there is a connection between the crosscorrelation functions of certain binary m-sequences and the value sets of the polynomials x(k)(1 + x)(2m-1) for k is an element of {+/-1, +/-2,4}, where x is in the finite field GF(2(2m)). In particular, the size of such value sets is determined by using finite field theory and known results about crosscorrelation functions.</p>","Computer Science"
"WOS:000073892800001","Coordinated path planning for multiple robots","1998","
<p>We present a new approach to the multi-robot path planning problem, where a number of robots are to change their positions through feasible motions in the same static environment. Rather than the usual decoupled planning, we use a coordinated approach. As a result we can show that the method is probabilistically complete, that is, any solvable problem will be solved within a finite amount of time. A data-structure storing multi-robot motion is built in two steps. First, a roadmap is constructed for just one robot. For this we use the probabilistic path planner, which guarantees that the approach can be easily applied to different robot types. In the second step, a number of these simple roadmaps are combined into a roadmap for the composite robot. This data-structure can be used for retrieving multi-robot paths. We have applied the method to car-like robots, and simulation results are presented which show that problems involving up to five car-like robots in complex environments are solved successfully in computation times in the order of seconds, after a preprocessing step (the construction of the data-structure) that consumes, at most, a few minutes. Such a preprocessing step however needs to be performed just once, for a given static environment. (C) 1998 Elsevier Science B.V.</p>","Computer Science, Artificial Intelligence"
"WOS:000073892800001","Coordinated path planning for multiple robots","1998","
<p>We present a new approach to the multi-robot path planning problem, where a number of robots are to change their positions through feasible motions in the same static environment. Rather than the usual decoupled planning, we use a coordinated approach. As a result we can show that the method is probabilistically complete, that is, any solvable problem will be solved within a finite amount of time. A data-structure storing multi-robot motion is built in two steps. First, a roadmap is constructed for just one robot. For this we use the probabilistic path planner, which guarantees that the approach can be easily applied to different robot types. In the second step, a number of these simple roadmaps are combined into a roadmap for the composite robot. This data-structure can be used for retrieving multi-robot paths. We have applied the method to car-like robots, and simulation results are presented which show that problems involving up to five car-like robots in complex environments are solved successfully in computation times in the order of seconds, after a preprocessing step (the construction of the data-structure) that consumes, at most, a few minutes. Such a preprocessing step however needs to be performed just once, for a given static environment. (C) 1998 Elsevier Science B.V.</p>","Computer Science"
"WOS:000084730100043","Learning first-order rules from image applied to glaucoma diagnosis","1998","
<p>Computer-based diagnosis from image data is important for medicine. In particular, for the glaucoma, diagnosis ive target here, the ocular fundus image can be easily obtained and can be used to automatically identify whether an eye is glaucomatous or not. However, the image has a two-dimensional distribution, and it is difficult to feature the whole image through some real-valued parameters in general. This paper proposes a machine learning method using a set of expert's decision cases that identify local abnormalities of an image. This method finds regularities between an image set and the decision cases using Inductive Logic Programming (ILP). Unlike decision-tree learning and neural networks, ILP allows relational learning between concepts. Learned rules are abstract enough to absorb noisy data obtained directly from image analysis. We applied the method to detecting early glaucomatous eyes. Our ILP system, GKS produced 30 rules from 2000 positive and negative examples that were obtained by segmenting 39 glaucomatous eyes. A 10-fold cross validation assessment shows about 80% sensitivity and 65% accuracy of the rules, resulting in the high performance comparable with human-level classification.</p>","Computer Science, Artificial Intelligence"
"WOS:000084730100043","Learning first-order rules from image applied to glaucoma diagnosis","1998","
<p>Computer-based diagnosis from image data is important for medicine. In particular, for the glaucoma, diagnosis ive target here, the ocular fundus image can be easily obtained and can be used to automatically identify whether an eye is glaucomatous or not. However, the image has a two-dimensional distribution, and it is difficult to feature the whole image through some real-valued parameters in general. This paper proposes a machine learning method using a set of expert's decision cases that identify local abnormalities of an image. This method finds regularities between an image set and the decision cases using Inductive Logic Programming (ILP). Unlike decision-tree learning and neural networks, ILP allows relational learning between concepts. Learned rules are abstract enough to absorb noisy data obtained directly from image analysis. We applied the method to detecting early glaucomatous eyes. Our ILP system, GKS produced 30 rules from 2000 positive and negative examples that were obtained by segmenting 39 glaucomatous eyes. A 10-fold cross validation assessment shows about 80% sensitivity and 65% accuracy of the rules, resulting in the high performance comparable with human-level classification.</p>","Computer Science"
"WOS:000074927200003","An adaptive knowledge-acquisition system using generic genetic programming","1998","
<p>The knowledge-acquisition bottleneck greatly obstructs the development of knowledge-based systems. One popular approach to knowledge acquisition uses inductive concept learning to derive knowledge from examples stored in databases. However, existing learning systems cannot improve themselves automatically. This paper describes an adaptive knowledge-acquisition system that can learn first-order logical relations and improve itself automatically. The system is composed of an external interface, a biases base, a knowledge base of background knowledge, an example database, an empirical ILP learner, a meta-level learner, and a learning controller. In this system, the empirical ILP learner performs top-down search in the hypothesis space defined by the concept description language, the language bias, and the background knowledge. The search is directed by search biases which can be induced and refined by the meta-level learner based on generic genetic programming.</p>
<p>It has been demonstrated that the adaptive knowledge-acquisition system performs better than FOIL on inducing logical relations from perfect or noisy training examples. The result implies that the search bias evolved by evolutionary learning is better than that of FOIL which is designed by a top researcher in the field. Consequently, generic genetic programming is a promising technique for implementing a meta-level learning system. The result is very encouraging as it suggests that the process of natural selection and evolution can successfully evolve a high-performance learning system. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science, Artificial Intelligence"
"WOS:000074927200003","An adaptive knowledge-acquisition system using generic genetic programming","1998","
<p>The knowledge-acquisition bottleneck greatly obstructs the development of knowledge-based systems. One popular approach to knowledge acquisition uses inductive concept learning to derive knowledge from examples stored in databases. However, existing learning systems cannot improve themselves automatically. This paper describes an adaptive knowledge-acquisition system that can learn first-order logical relations and improve itself automatically. The system is composed of an external interface, a biases base, a knowledge base of background knowledge, an example database, an empirical ILP learner, a meta-level learner, and a learning controller. In this system, the empirical ILP learner performs top-down search in the hypothesis space defined by the concept description language, the language bias, and the background knowledge. The search is directed by search biases which can be induced and refined by the meta-level learner based on generic genetic programming.</p>
<p>It has been demonstrated that the adaptive knowledge-acquisition system performs better than FOIL on inducing logical relations from perfect or noisy training examples. The result implies that the search bias evolved by evolutionary learning is better than that of FOIL which is designed by a top researcher in the field. Consequently, generic genetic programming is a promising technique for implementing a meta-level learning system. The result is very encouraging as it suggests that the process of natural selection and evolution can successfully evolve a high-performance learning system. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science"
"WOS:000085482900017","Evaluating a multithreaded runtime system for concurrent object-oriented languages","1998","
<p>Traditionally, the use of multithreading capabilities of operating systems has been considered inadequate for implementing concurrent object-oriented languages because of their inefficiency and nonportability. However, current operating systems encourage programmers to use threads to manage concurrent activities, since they offer a number of advantages such as multiprocessing capabilities and thread communication through shared memory. To explore these issues, we have developed Lince, a multithreaded runtime system for concurrent objects. We describe Lince and its design philosophy and analyze its performance. The use of popular threads packages allows us to simplify system design and enhance portability. The overhead of using threads for implementing concurrent objects is negligible for medium and coarse grain applications, although it can be too expensive for those requiring many fine-grained objects.</p>","Computer Science, Theory & Methods"
"WOS:000085482900017","Evaluating a multithreaded runtime system for concurrent object-oriented languages","1998","
<p>Traditionally, the use of multithreading capabilities of operating systems has been considered inadequate for implementing concurrent object-oriented languages because of their inefficiency and nonportability. However, current operating systems encourage programmers to use threads to manage concurrent activities, since they offer a number of advantages such as multiprocessing capabilities and thread communication through shared memory. To explore these issues, we have developed Lince, a multithreaded runtime system for concurrent objects. We describe Lince and its design philosophy and analyze its performance. The use of popular threads packages allows us to simplify system design and enhance portability. The overhead of using threads for implementing concurrent objects is negligible for medium and coarse grain applications, although it can be too expensive for those requiring many fine-grained objects.</p>","Computer Science"
"WOS:000071685700005","It's time for full life-cycle languages","1998","
<p>Object-orientation is supposed to offer a seamless transition in the development of systems, Ensuring this seamless transition requires using the same language throughout the process to avoid the need for translations between phases or activities. In this article we present an extension to Eiffel that we propose as a full life-cycle language, This language is capable of being a ""lingua franca"" between different analysis and design methodologies, their notations, and the CASE tools that support them. The extensions we propose adding to Eiffel explicitly represent: relations between objects, the description of supraclass constructs, states and transitions to describe the dynamic behavior of objects, description of use cases, and incrementing the specifying and documenting qualities of Eiffel.</p>","Computer Science, Software Engineering"
"WOS:000071685700005","It's time for full life-cycle languages","1998","
<p>Object-orientation is supposed to offer a seamless transition in the development of systems, Ensuring this seamless transition requires using the same language throughout the process to avoid the need for translations between phases or activities. In this article we present an extension to Eiffel that we propose as a full life-cycle language, This language is capable of being a ""lingua franca"" between different analysis and design methodologies, their notations, and the CASE tools that support them. The extensions we propose adding to Eiffel explicitly represent: relations between objects, the description of supraclass constructs, states and transitions to describe the dynamic behavior of objects, description of use cases, and incrementing the specifying and documenting qualities of Eiffel.</p>","Computer Science, Theory & Methods"
"WOS:000071685700005","It's time for full life-cycle languages","1998","
<p>Object-orientation is supposed to offer a seamless transition in the development of systems, Ensuring this seamless transition requires using the same language throughout the process to avoid the need for translations between phases or activities. In this article we present an extension to Eiffel that we propose as a full life-cycle language, This language is capable of being a ""lingua franca"" between different analysis and design methodologies, their notations, and the CASE tools that support them. The extensions we propose adding to Eiffel explicitly represent: relations between objects, the description of supraclass constructs, states and transitions to describe the dynamic behavior of objects, description of use cases, and incrementing the specifying and documenting qualities of Eiffel.</p>","Computer Science"
"WOS:000073236200001","Equational unification, word unification, and 2nd-order equational unification","1998","
<p>For finite convergent term-rewriting systems it is shown that the equational unification problem is recursively independent of the equational matching problem, the word matching problem, and the 2nd-order equational matching problem. Apart from the latter these results are derived by considering term-rewriting systems on signatures that contain unary function symbols only (i.e., string-rewriting systems). Also for this special case 2nd-order equational matching is shown to be reducible to Ist-order equational matching. In addition, we present some new decidability results for simultaneous equational matching and unification. Finally, we compare the word unification problem to the 2nd-order equational unification problem. (C) 1998-Elsevier Science B.V. All rights reserved.</p>","Computer Science, Theory & Methods"
"WOS:000073236200001","Equational unification, word unification, and 2nd-order equational unification","1998","
<p>For finite convergent term-rewriting systems it is shown that the equational unification problem is recursively independent of the equational matching problem, the word matching problem, and the 2nd-order equational matching problem. Apart from the latter these results are derived by considering term-rewriting systems on signatures that contain unary function symbols only (i.e., string-rewriting systems). Also for this special case 2nd-order equational matching is shown to be reducible to Ist-order equational matching. In addition, we present some new decidability results for simultaneous equational matching and unification. Finally, we compare the word unification problem to the 2nd-order equational unification problem. (C) 1998-Elsevier Science B.V. All rights reserved.</p>","Computer Science"
"WOS:000075392100003","IBM's Enterprise Server for Java","1998","
<p>IBM is exploiting Enterprise JavaBeans(TM) in a family of compatible Java(TM) application servers conforming to IBM's Enterprise Server for Java (ESJ) specification. The ESJ provides a common set of dynamic, adaptive system services to meet today's (and tomorrow's) middleware requirements. ESJ will provide a standard programming model and set of services across major server platforms so that implementations of ESJ are differentiated not by function but by qualify of service. Finally, ESJ increases productivity by enabling programmers to focus on business logic rather than on infrastructure details. This paper introduces the design of ESJ, including the attributes of the common execution environment, its interaction with other middleware, and its cleientserver capabilities. It provides an appreciation of the value of ESJ to application developers as a means of achieving cross-platform consistency, lower costs, and faster time to market. It also outlines the features that make ESJ the server technology base for wide-scale reuse through the ""write once, run anywhere"" promise of Java.</p>","Computer Science, Information Systems"
"WOS:000075392100003","IBM's Enterprise Server for Java","1998","
<p>IBM is exploiting Enterprise JavaBeans(TM) in a family of compatible Java(TM) application servers conforming to IBM's Enterprise Server for Java (ESJ) specification. The ESJ provides a common set of dynamic, adaptive system services to meet today's (and tomorrow's) middleware requirements. ESJ will provide a standard programming model and set of services across major server platforms so that implementations of ESJ are differentiated not by function but by qualify of service. Finally, ESJ increases productivity by enabling programmers to focus on business logic rather than on infrastructure details. This paper introduces the design of ESJ, including the attributes of the common execution environment, its interaction with other middleware, and its cleientserver capabilities. It provides an appreciation of the value of ESJ to application developers as a means of achieving cross-platform consistency, lower costs, and faster time to market. It also outlines the features that make ESJ the server technology base for wide-scale reuse through the ""write once, run anywhere"" promise of Java.</p>","Computer Science, Software Engineering"
"WOS:000075392100003","IBM's Enterprise Server for Java","1998","
<p>IBM is exploiting Enterprise JavaBeans(TM) in a family of compatible Java(TM) application servers conforming to IBM's Enterprise Server for Java (ESJ) specification. The ESJ provides a common set of dynamic, adaptive system services to meet today's (and tomorrow's) middleware requirements. ESJ will provide a standard programming model and set of services across major server platforms so that implementations of ESJ are differentiated not by function but by qualify of service. Finally, ESJ increases productivity by enabling programmers to focus on business logic rather than on infrastructure details. This paper introduces the design of ESJ, including the attributes of the common execution environment, its interaction with other middleware, and its cleientserver capabilities. It provides an appreciation of the value of ESJ to application developers as a means of achieving cross-platform consistency, lower costs, and faster time to market. It also outlines the features that make ESJ the server technology base for wide-scale reuse through the ""write once, run anywhere"" promise of Java.</p>","Computer Science, Theory & Methods"
"WOS:000075392100003","IBM's Enterprise Server for Java","1998","
<p>IBM is exploiting Enterprise JavaBeans(TM) in a family of compatible Java(TM) application servers conforming to IBM's Enterprise Server for Java (ESJ) specification. The ESJ provides a common set of dynamic, adaptive system services to meet today's (and tomorrow's) middleware requirements. ESJ will provide a standard programming model and set of services across major server platforms so that implementations of ESJ are differentiated not by function but by qualify of service. Finally, ESJ increases productivity by enabling programmers to focus on business logic rather than on infrastructure details. This paper introduces the design of ESJ, including the attributes of the common execution environment, its interaction with other middleware, and its cleientserver capabilities. It provides an appreciation of the value of ESJ to application developers as a means of achieving cross-platform consistency, lower costs, and faster time to market. It also outlines the features that make ESJ the server technology base for wide-scale reuse through the ""write once, run anywhere"" promise of Java.</p>","Computer Science"
"WOS:000072532500007","Parameterizing arbitrary shapes via Fourier descriptors for evidence-gathering extraction","1998","
<p>According to the formulation of the Hough Transform, it is possible to extract any shape that can be represented by an analytic equation with a number of free parameters, Nevertheless, the extraction of arbitrary shapes has centered on nonanalytic representations based on a table which specifies the position of edge points relative to a fixed reference point. In this paper we develop a novel approach for arbitrary shape extraction which combines the analytic representation of shapes with the generality of the characterization by Fourier descriptors. The formulation is based on a definition of the Hough Transform obtained by considering the parametric representation of shapes and extends the descriptional power of the Hough Transform beyond simple shapes, thus avoiding the use of tables, Since we use an analytic representation of shapes, the developed technique inherits the robustness of the original formulation of the Hough Transform. Based on the developed formulation, and by using different strategies of parameter space decomposition, various methods of shape extraction are presented. In these methods the parameter space is reduced by using gradient direction information as well as the positions of grouped edge points. Different methods represent a compromise between speed, noise sensitivity, simplicity, and generality, Some examples of the extraction process on a selection of synthetic and real images are presented, showing the successful extraction of target shapes from noisy data. (C) 1998 Academic Press.</p>","Computer Science, Artificial Intelligence"
"WOS:000072532500007","Parameterizing arbitrary shapes via Fourier descriptors for evidence-gathering extraction","1998","
<p>According to the formulation of the Hough Transform, it is possible to extract any shape that can be represented by an analytic equation with a number of free parameters, Nevertheless, the extraction of arbitrary shapes has centered on nonanalytic representations based on a table which specifies the position of edge points relative to a fixed reference point. In this paper we develop a novel approach for arbitrary shape extraction which combines the analytic representation of shapes with the generality of the characterization by Fourier descriptors. The formulation is based on a definition of the Hough Transform obtained by considering the parametric representation of shapes and extends the descriptional power of the Hough Transform beyond simple shapes, thus avoiding the use of tables, Since we use an analytic representation of shapes, the developed technique inherits the robustness of the original formulation of the Hough Transform. Based on the developed formulation, and by using different strategies of parameter space decomposition, various methods of shape extraction are presented. In these methods the parameter space is reduced by using gradient direction information as well as the positions of grouped edge points. Different methods represent a compromise between speed, noise sensitivity, simplicity, and generality, Some examples of the extraction process on a selection of synthetic and real images are presented, showing the successful extraction of target shapes from noisy data. (C) 1998 Academic Press.</p>","Computer Science"
"WOS:000076676800015","Creativity and artificial intelligence","1998","
<p>Creativity is a fundamental feature of human intelligence, and a challenge for AT. AI techniques can be used to create new ideas in three ways: by producing novel combinations of familiar ideas; by exploring the potential of conceptual spaces; and by making transformations that enable the generation of previously impossible ideas. Al will have less difficulty in modelling the generation of new ideas than in automating their evaluation. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science, Artificial Intelligence"
"WOS:000076676800015","Creativity and artificial intelligence","1998","
<p>Creativity is a fundamental feature of human intelligence, and a challenge for AT. AI techniques can be used to create new ideas in three ways: by producing novel combinations of familiar ideas; by exploring the potential of conceptual spaces; and by making transformations that enable the generation of previously impossible ideas. Al will have less difficulty in modelling the generation of new ideas than in automating their evaluation. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science"
"WOS:000171768600154","Reproducibility of interpreting ""and"" and ""or"" in terminology systems","1998","
<p>High quality terminologies are a fundamental requirement in a range of health care applications, To ensure high quality terminologies we should reflect about the understandability, reproducibility and utility criteria within a terminology. This paper describes efforts to improve the understandability of SNOMED. We describe the problem related to the grammatical conjunctions ""and"" and ""or"" and how we applied basic semantic rules defined by the SNOMED Editorial Board. The results show that the meaning of ""and"" and ""or"" in SNOMED can be made explicit in almost all cases and can be done in a reasonable, reliable, and reproducible manner.</p>","Computer Science, Information Systems"
"WOS:000171768600154","Reproducibility of interpreting ""and"" and ""or"" in terminology systems","1998","
<p>High quality terminologies are a fundamental requirement in a range of health care applications, To ensure high quality terminologies we should reflect about the understandability, reproducibility and utility criteria within a terminology. This paper describes efforts to improve the understandability of SNOMED. We describe the problem related to the grammatical conjunctions ""and"" and ""or"" and how we applied basic semantic rules defined by the SNOMED Editorial Board. The results show that the meaning of ""and"" and ""or"" in SNOMED can be made explicit in almost all cases and can be done in a reasonable, reliable, and reproducible manner.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000171768600154","Reproducibility of interpreting ""and"" and ""or"" in terminology systems","1998","
<p>High quality terminologies are a fundamental requirement in a range of health care applications, To ensure high quality terminologies we should reflect about the understandability, reproducibility and utility criteria within a terminology. This paper describes efforts to improve the understandability of SNOMED. We describe the problem related to the grammatical conjunctions ""and"" and ""or"" and how we applied basic semantic rules defined by the SNOMED Editorial Board. The results show that the meaning of ""and"" and ""or"" in SNOMED can be made explicit in almost all cases and can be done in a reasonable, reliable, and reproducible manner.</p>","Computer Science"
"WOS:000073868200008","Improving functional density using run-time circuit reconfiguration","1998","
<p>The ability to provide flexibility and allow fine-grain circuit specialization make field programmable gate arrays (FPGA's) ideal candidates for computing elements within application-specific architectures. The benefits of gate-level specialization and reconfigurability can be extended by reconfiguring circuit resources at run-time. This technique, termed run-time reconfiguration (RTR), allows the exploitation of dynamic conditions or temporal locality within application-specific problems. For several applications, this technique has been shown to reduce the hardware resources required for computation. The use of this technique on conventional FPGA's, however, requires additional time for circuit reconfiguration. A functional density metric is introduced that balances the advantages of RTR against its associated reconfiguration costs. This metric is used to justify runtime reconfiguration against other more conventional approaches. Several run-time reconfigured applications are presented and analyzed using this approach.</p>","Computer Science, Hardware & Architecture"
"WOS:000073868200008","Improving functional density using run-time circuit reconfiguration","1998","
<p>The ability to provide flexibility and allow fine-grain circuit specialization make field programmable gate arrays (FPGA's) ideal candidates for computing elements within application-specific architectures. The benefits of gate-level specialization and reconfigurability can be extended by reconfiguring circuit resources at run-time. This technique, termed run-time reconfiguration (RTR), allows the exploitation of dynamic conditions or temporal locality within application-specific problems. For several applications, this technique has been shown to reduce the hardware resources required for computation. The use of this technique on conventional FPGA's, however, requires additional time for circuit reconfiguration. A functional density metric is introduced that balances the advantages of RTR against its associated reconfiguration costs. This metric is used to justify runtime reconfiguration against other more conventional approaches. Several run-time reconfigured applications are presented and analyzed using this approach.</p>","Computer Science"
"WOS:000077390700010","Data-parallel tomographic reconstruction: A comparison of filtered backprojection and direct Fourier reconstruction","1998","
<p>We consider the parallelization of two standard 2D reconstruction algorithms, filtered backprojection and direct Fourier reconstruction, using the data-parallel programming style. The algorithms are implemented on a Connection Machine CM-5 with 16 processors and a peak performance of 2 Gflop/s. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science, Theory & Methods"
"WOS:000077390700010","Data-parallel tomographic reconstruction: A comparison of filtered backprojection and direct Fourier reconstruction","1998","
<p>We consider the parallelization of two standard 2D reconstruction algorithms, filtered backprojection and direct Fourier reconstruction, using the data-parallel programming style. The algorithms are implemented on a Connection Machine CM-5 with 16 processors and a peak performance of 2 Gflop/s. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science"
"WOS:000072883500003","The fuzzy Riemann-Stieltjes integral","1998","
<p>In the classcial sense, the expectation is sometimes defined as Riemann-Stieltjes integral. In this paper, we propose the concept of fuzzy Riemann-Stieltjes integral to make the expectation of fuzzy random variable applicable to statistical analysis for imprecise data.</p>","Computer Science, Artificial Intelligence"
"WOS:000072883500003","The fuzzy Riemann-Stieltjes integral","1998","
<p>In the classcial sense, the expectation is sometimes defined as Riemann-Stieltjes integral. In this paper, we propose the concept of fuzzy Riemann-Stieltjes integral to make the expectation of fuzzy random variable applicable to statistical analysis for imprecise data.</p>","Computer Science"
"WOS:000073602700002","Architecture of the San Francisco frameworks","1998","
<p>The San Francisco(TM) project is an IBM initiative, with over 130 independent software vendors, to provide business process components that can form the basis of rapidly developed distributed solutions for mission-critical business applications. This paper describes the original objectives of the San Francisco project and discusses the methodology, skills, and architecture that were used to achieve those objectives. The paper includes discussion of the importance of design patterns, extension points, and a well-defined programming model used in the San Francisco components. Most topics are touched on briefly to give an overview; some knowledge of object-oriented development techniques is assumed.</p>","Computer Science, Information Systems"
"WOS:000073602700002","Architecture of the San Francisco frameworks","1998","
<p>The San Francisco(TM) project is an IBM initiative, with over 130 independent software vendors, to provide business process components that can form the basis of rapidly developed distributed solutions for mission-critical business applications. This paper describes the original objectives of the San Francisco project and discusses the methodology, skills, and architecture that were used to achieve those objectives. The paper includes discussion of the importance of design patterns, extension points, and a well-defined programming model used in the San Francisco components. Most topics are touched on briefly to give an overview; some knowledge of object-oriented development techniques is assumed.</p>","Computer Science, Software Engineering"
"WOS:000073602700002","Architecture of the San Francisco frameworks","1998","
<p>The San Francisco(TM) project is an IBM initiative, with over 130 independent software vendors, to provide business process components that can form the basis of rapidly developed distributed solutions for mission-critical business applications. This paper describes the original objectives of the San Francisco project and discusses the methodology, skills, and architecture that were used to achieve those objectives. The paper includes discussion of the importance of design patterns, extension points, and a well-defined programming model used in the San Francisco components. Most topics are touched on briefly to give an overview; some knowledge of object-oriented development techniques is assumed.</p>","Computer Science, Theory & Methods"
"WOS:000073602700002","Architecture of the San Francisco frameworks","1998","
<p>The San Francisco(TM) project is an IBM initiative, with over 130 independent software vendors, to provide business process components that can form the basis of rapidly developed distributed solutions for mission-critical business applications. This paper describes the original objectives of the San Francisco project and discusses the methodology, skills, and architecture that were used to achieve those objectives. The paper includes discussion of the importance of design patterns, extension points, and a well-defined programming model used in the San Francisco components. Most topics are touched on briefly to give an overview; some knowledge of object-oriented development techniques is assumed.</p>","Computer Science"
"WOS:000078563200005","A safe and scalable payment infrastructure for trade of electronic content","1998","
<p>BARTER (a Backbone ARchitecture for Trade of ElectRonic content) is a payment infrastructure that facilitates digital content trade over an open network. BARTER is designed to operate over a large-scale, global and heterogeneous communication network. The BARTER protocols address two vital requirements, neglected from existing electronic commerce systems: scalability and transactional efficiency. These protocols possess strong properties such as delivery atomicity agreement validation and the ability to resolve several classes of disputes. BARTER's novelty is twofold: First, BARTER servers are not required to perform expensive cryptographic operations such as commitment verification; commitments are cross-verified by the parties themselves, thus reducing the overhead of online transaction processing by orders of magnitude. Consequently, BARTER can serve as an efficient online/offline clearing infrastructure. Second, BARTER integrates scalability considerations into several system components (the authentication subsystem, the account management subsystem, and the maintenance of global data) that are likely to suffer service degradation in a world-wide setting. In addressing these issues, BARTER takes into account the inherent asynchronous, unreliable, insecure and failure-prone environment assumptions. We contend that by employing service distribution, BARTER is expected to scale well, meeting the demands of a world-wide setting, over which it is intended to operate.</p>","Computer Science, Information Systems"
"WOS:000078563200005","A safe and scalable payment infrastructure for trade of electronic content","1998","
<p>BARTER (a Backbone ARchitecture for Trade of ElectRonic content) is a payment infrastructure that facilitates digital content trade over an open network. BARTER is designed to operate over a large-scale, global and heterogeneous communication network. The BARTER protocols address two vital requirements, neglected from existing electronic commerce systems: scalability and transactional efficiency. These protocols possess strong properties such as delivery atomicity agreement validation and the ability to resolve several classes of disputes. BARTER's novelty is twofold: First, BARTER servers are not required to perform expensive cryptographic operations such as commitment verification; commitments are cross-verified by the parties themselves, thus reducing the overhead of online transaction processing by orders of magnitude. Consequently, BARTER can serve as an efficient online/offline clearing infrastructure. Second, BARTER integrates scalability considerations into several system components (the authentication subsystem, the account management subsystem, and the maintenance of global data) that are likely to suffer service degradation in a world-wide setting. In addressing these issues, BARTER takes into account the inherent asynchronous, unreliable, insecure and failure-prone environment assumptions. We contend that by employing service distribution, BARTER is expected to scale well, meeting the demands of a world-wide setting, over which it is intended to operate.</p>","Computer Science"
"WOS:000077969400013","A fault-tolerant parallel heuristic for assignment problems","1998","
<p>This paper presents a new approach for parallel heuristic algorithms based on adaptive parallelism. Adaptive parallelism was used to dynamically adjust the parallelism degree of the application with respect to the system load. This approach demonstrates that high-performance computing using a hundred of heterogeneous workstations combined with massively parallel machines is feasible to solve large optimization problems with respect to the personal character of workstations. The fault-tolerant algorithm allows a minimal loss of computation in case of failures. The proposed algorithm exploits the properties of this class of applications in order to reduce the complexity of the algorithm in terms of the checkpoint files size and the control messages exchanged. The parallel heuristic algorithm combines different search strategies: simulated annealing and tabu search. Encouraging results have been obtained in solving the quadratic assignment problem. We have improved the best known solutions for some large real-world problems. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science, Theory & Methods"
"WOS:000077969400013","A fault-tolerant parallel heuristic for assignment problems","1998","
<p>This paper presents a new approach for parallel heuristic algorithms based on adaptive parallelism. Adaptive parallelism was used to dynamically adjust the parallelism degree of the application with respect to the system load. This approach demonstrates that high-performance computing using a hundred of heterogeneous workstations combined with massively parallel machines is feasible to solve large optimization problems with respect to the personal character of workstations. The fault-tolerant algorithm allows a minimal loss of computation in case of failures. The proposed algorithm exploits the properties of this class of applications in order to reduce the complexity of the algorithm in terms of the checkpoint files size and the control messages exchanged. The parallel heuristic algorithm combines different search strategies: simulated annealing and tabu search. Encouraging results have been obtained in solving the quadratic assignment problem. We have improved the best known solutions for some large real-world problems. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science"
"WOS:000074647700004","An implementation of Kripke-Kleene semantics","1998","
<p>An implementation of Kripke-Kleene (3-value) Semantics for logic programming is introduced in this paper. The aim of applying Kripke-Kleene Semantics to logic programming is to overcome indetermination occurred in the programs. In this paper, however, we analyse different situations in which indetermination may occur and classify them as indetermination and contradiction and finally propose a solution to this. (C) 1998 Elsevier Science Inc. All rights reserved.</p>","Computer Science, Information Systems"
"WOS:000074647700004","An implementation of Kripke-Kleene semantics","1998","
<p>An implementation of Kripke-Kleene (3-value) Semantics for logic programming is introduced in this paper. The aim of applying Kripke-Kleene Semantics to logic programming is to overcome indetermination occurred in the programs. In this paper, however, we analyse different situations in which indetermination may occur and classify them as indetermination and contradiction and finally propose a solution to this. (C) 1998 Elsevier Science Inc. All rights reserved.</p>","Computer Science"
"WOS:000073566500006","Plume1.1: Deposition of sediment from a fluvial plume","1998","
<p>An ANSI-standard Fortran 77 program solves the steady, two-dimensional advection-diffusion equation describing a turbid hypopycnal plume emanating from a river mouth. The model solves for the extent of the plume as a non-dimensioned inventory of sediment mass on an axial-lateral grid. The basis for the model is a derivation by Albertson for a two-dimensional momentum-driven submerged jet. Particle settling is based on the scavenging model of Syvitski. PLUME is robust but fast enough to handle the sedimentation beneath a river plume flowing into a coastal sea on a daily basis. Designed for speed, the model may be run with daily-changing river input characteristics (flow velocity, river mouth dimensions, sediment concentrations of up to ten grain sizes) for many (up to thousands of) years. PLUME1.1 works with input from long-term field observations or from climate-hydrologic simulations. The model provides for an improved geological simulator of land-sea interaction and the delivery of sediment onto continental margins. The model can be used to simulate flow into both open-coast and semi-enclosed basins. Open-coast plume simulations include the hood-dominated Eel River margin, Northern California, where a three-day flood event could supply more sediment than the previous seven years combined. A semi-enclosed basin example is from a fjord-basin from British Columbia, where model simulations agree well with sediment concentration observations. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000073566500006","Plume1.1: Deposition of sediment from a fluvial plume","1998","
<p>An ANSI-standard Fortran 77 program solves the steady, two-dimensional advection-diffusion equation describing a turbid hypopycnal plume emanating from a river mouth. The model solves for the extent of the plume as a non-dimensioned inventory of sediment mass on an axial-lateral grid. The basis for the model is a derivation by Albertson for a two-dimensional momentum-driven submerged jet. Particle settling is based on the scavenging model of Syvitski. PLUME is robust but fast enough to handle the sedimentation beneath a river plume flowing into a coastal sea on a daily basis. Designed for speed, the model may be run with daily-changing river input characteristics (flow velocity, river mouth dimensions, sediment concentrations of up to ten grain sizes) for many (up to thousands of) years. PLUME1.1 works with input from long-term field observations or from climate-hydrologic simulations. The model provides for an improved geological simulator of land-sea interaction and the delivery of sediment onto continental margins. The model can be used to simulate flow into both open-coast and semi-enclosed basins. Open-coast plume simulations include the hood-dominated Eel River margin, Northern California, where a three-day flood event could supply more sediment than the previous seven years combined. A semi-enclosed basin example is from a fjord-basin from British Columbia, where model simulations agree well with sediment concentration observations. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science"
"WOS:000073079000007","A continuation method for (strongly) monotone variational inequalities","1998","
<p>We consider the variational inequality problem, denoted by VIP(X,F), when F is a strongly monotone function and the convex set X is described by some inequality (and possibly equality) constraints. This problem is solved by a continuation (or interior-point) method, which solves a sequence of certain perturbed variational inequality problems. These perturbed problems depend on a parameter mu > 0. It is shown that the perturbed problems have a unique solution for all values of mu > 0, and that any sequence generated by the continuation method converges to the unique solution of VIP(X,F) under a well-known linear independence constraint qualification (LICQ). We also discuss the extension of the continuation method to monotone variational inequalities and present some numerical results obtained with a suitable implementation of this method. (C) 1998 The Mathematical Programming Society, Inc. Published by Elsevier Science B.V.</p>","Computer Science, Software Engineering"
"WOS:000073079000007","A continuation method for (strongly) monotone variational inequalities","1998","
<p>We consider the variational inequality problem, denoted by VIP(X,F), when F is a strongly monotone function and the convex set X is described by some inequality (and possibly equality) constraints. This problem is solved by a continuation (or interior-point) method, which solves a sequence of certain perturbed variational inequality problems. These perturbed problems depend on a parameter mu > 0. It is shown that the perturbed problems have a unique solution for all values of mu > 0, and that any sequence generated by the continuation method converges to the unique solution of VIP(X,F) under a well-known linear independence constraint qualification (LICQ). We also discuss the extension of the continuation method to monotone variational inequalities and present some numerical results obtained with a suitable implementation of this method. (C) 1998 The Mathematical Programming Society, Inc. Published by Elsevier Science B.V.</p>","Computer Science"
"WOS:000077275100005","Effect of sequence selection on MAI suppression in limited spreading CDMA systems","1998","
<p>Increasingly, spread spectrum systems are being proposed which support a range of data rates and traffic types in a band limited environment. One way to achieve a range of data rates with fixed bandwidth is to allow the use of different processing gains. If the range of data rates supported is large, the lower bound on the processing gain can become quite small. Receivers based on LMMSE algorithms offer high efficiency for CDMA systems through interference suppression, however at small processing gains, the sequence set correlation properties prove to be a significant limitation on the system efficiency. This paper provides examples of sets of short length spreading sequences which have good auto-correlation (AC) and cross-correlation (CC) properties which are essential to allow relatively high system efficiency. A CDMA system using an LMMSE receiver is examined with selected short length sets to demonstrate the application of the sequences. The sets proposed are compared to system performance when using well known sequence families.</p>","Computer Science, Information Systems"
"WOS:000077275100005","Effect of sequence selection on MAI suppression in limited spreading CDMA systems","1998","
<p>Increasingly, spread spectrum systems are being proposed which support a range of data rates and traffic types in a band limited environment. One way to achieve a range of data rates with fixed bandwidth is to allow the use of different processing gains. If the range of data rates supported is large, the lower bound on the processing gain can become quite small. Receivers based on LMMSE algorithms offer high efficiency for CDMA systems through interference suppression, however at small processing gains, the sequence set correlation properties prove to be a significant limitation on the system efficiency. This paper provides examples of sets of short length spreading sequences which have good auto-correlation (AC) and cross-correlation (CC) properties which are essential to allow relatively high system efficiency. A CDMA system using an LMMSE receiver is examined with selected short length sets to demonstrate the application of the sequences. The sets proposed are compared to system performance when using well known sequence families.</p>","Computer Science"
"WOS:000074454800019","Dominant color transform and circular pattern vector for traffic sign detection and recognition","1998","
<p>In this paper, a new traffic sign detection algorithm and a symbol recognition algorithm are proposed. For a traffic sign detection, a dominant color transform is introduced, which serves as a tool of highlighting a dominant primary color, while discarding the other two primary colors. For a symbol recognition, the curvilinear shape distribution on a circle centered on the centroid of the symbol, called a circular pattern vector, is used as a spatial feature of the symbol. The circular pattern vector is invariant to scaling, translation, and rotation. As simulation results, the effectiveness of traffic sign detection and recognition algorithms are confirmed.</p>","Computer Science, Hardware & Architecture"
"WOS:000074454800019","Dominant color transform and circular pattern vector for traffic sign detection and recognition","1998","
<p>In this paper, a new traffic sign detection algorithm and a symbol recognition algorithm are proposed. For a traffic sign detection, a dominant color transform is introduced, which serves as a tool of highlighting a dominant primary color, while discarding the other two primary colors. For a symbol recognition, the curvilinear shape distribution on a circle centered on the centroid of the symbol, called a circular pattern vector, is used as a spatial feature of the symbol. The circular pattern vector is invariant to scaling, translation, and rotation. As simulation results, the effectiveness of traffic sign detection and recognition algorithms are confirmed.</p>","Computer Science, Information Systems"
"WOS:000074454800019","Dominant color transform and circular pattern vector for traffic sign detection and recognition","1998","
<p>In this paper, a new traffic sign detection algorithm and a symbol recognition algorithm are proposed. For a traffic sign detection, a dominant color transform is introduced, which serves as a tool of highlighting a dominant primary color, while discarding the other two primary colors. For a symbol recognition, the curvilinear shape distribution on a circle centered on the centroid of the symbol, called a circular pattern vector, is used as a spatial feature of the symbol. The circular pattern vector is invariant to scaling, translation, and rotation. As simulation results, the effectiveness of traffic sign detection and recognition algorithms are confirmed.</p>","Computer Science"
"WOS:000077650100005","Musical punctuation on the microlevel: Automatic identification and performance of small melodic units","1998","
<p>In this investigation we use the term musical punctuation for the marking of melodic structure by commas inserted at the boundaries that separate small structural units. Two models are presented that automatically try to locate the positions of such commas. They both use the score as the input and operate with a short context of maximally five notes. The first model is based on a set of subrules. One group of subrules mark possible comma positions, each provided with a weight value. Another group alters or removes these weight values according to different conditions. The second model is an artificial neural network using a similar input as that used by the rule system. The commas proposed by either model are realized in terms of micropauses and of small lengthenings of interonset durations. The models are evaluated by using a set of 52 musical excerpts, which were marked with punctuations according to the preference of an expert performer.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000077650100005","Musical punctuation on the microlevel: Automatic identification and performance of small melodic units","1998","
<p>In this investigation we use the term musical punctuation for the marking of melodic structure by commas inserted at the boundaries that separate small structural units. Two models are presented that automatically try to locate the positions of such commas. They both use the score as the input and operate with a short context of maximally five notes. The first model is based on a set of subrules. One group of subrules mark possible comma positions, each provided with a weight value. Another group alters or removes these weight values according to different conditions. The second model is an artificial neural network using a similar input as that used by the rule system. The commas proposed by either model are realized in terms of micropauses and of small lengthenings of interonset durations. The models are evaluated by using a set of 52 musical excerpts, which were marked with punctuations according to the preference of an expert performer.</p>","Computer Science"
"WOS:000075494700003","Designing a neural network using a genetic algorithm with deterministic mutation and partial fitness","1998","
<p>In this paper a method of designing a neural pattern recognition system for a rotated coin recognition problem using a genetic algorithm (GA) with deterministic mutation (DM) and partial fitness (PF) is presented. In this method, chromosomes of individuals in the GA are divided into several parts and their PF functions are evaluated for GA operations. Furthermore, the DM, which is based on neural network learning, is introduced. The DM can evolve chromosomes of individuals to increase their fitness functions in a deterministic manner. In the pattern recognition system described in this paper, the Fourier transform is used as a preprocessor which produces rotation invariant features. These features are recognized by a multilayered neural network. The GA is utilized to reduce the number of signals, Fourier spectra, and input to the neural network. This approach using the GA is a type of feature selection problem. It is shown that the present method is better than conventional GAs with respect to convergence in learning, and results in the formation of a small neural network.</p>","Computer Science, Artificial Intelligence"
"WOS:000075494700003","Designing a neural network using a genetic algorithm with deterministic mutation and partial fitness","1998","
<p>In this paper a method of designing a neural pattern recognition system for a rotated coin recognition problem using a genetic algorithm (GA) with deterministic mutation (DM) and partial fitness (PF) is presented. In this method, chromosomes of individuals in the GA are divided into several parts and their PF functions are evaluated for GA operations. Furthermore, the DM, which is based on neural network learning, is introduced. The DM can evolve chromosomes of individuals to increase their fitness functions in a deterministic manner. In the pattern recognition system described in this paper, the Fourier transform is used as a preprocessor which produces rotation invariant features. These features are recognized by a multilayered neural network. The GA is utilized to reduce the number of signals, Fourier spectra, and input to the neural network. This approach using the GA is a type of feature selection problem. It is shown that the present method is better than conventional GAs with respect to convergence in learning, and results in the formation of a small neural network.</p>","Computer Science"
"WOS:000171768600062","Problem focused knowledge navigation: Implementing the problem focused medical record and the O-HEAP note","1998","
<p>The current organization of most Computerized Medical Records (CMR) is based on the Problem Oriented Medical Record (POMR) and the SOAP (Subjective, Objective, Assessment and Plan) note(1). The organizational structure of the POMR and especially the SOAP note, does not allow for optimal use of computer capabilities in the follow up note. Since follow up visits are the most common office visit by far, this is a major flaw in the CMR2. The authors propose a Problem Focused Medical Record and the OHEAP (Orientation, History, Exam, Assessment and Plan) note to resolve this problem. OHEAP starts with a powerful orientation structure that brings forward the timeline, last Assessment and Plan, and Plan Results for each problem along with the patient's historical tables as the starting point of every follow up visit. The Assessment and Plan portion brings problem specific differential diagnoses and their workups along with other relevant tables such as expert systems, treatments, instructions, medical literature or pathways. This leads to Problem Focused Knowledge Navigation that brings powerful efficiencies to the CMR. By recognizing the true workflow in the longitudinal diagnosis and management of any medical problem, the efficiency of the CMR is maximized. OHEAP allows for optimal use of both personal and external data elements in the medical record. Its powerful orientation attributes minimize the time spent in analyzing the current status of the problem while its connections to problem specific databases help resolve the problem.</p>","Computer Science, Information Systems"
"WOS:000171768600062","Problem focused knowledge navigation: Implementing the problem focused medical record and the O-HEAP note","1998","
<p>The current organization of most Computerized Medical Records (CMR) is based on the Problem Oriented Medical Record (POMR) and the SOAP (Subjective, Objective, Assessment and Plan) note(1). The organizational structure of the POMR and especially the SOAP note, does not allow for optimal use of computer capabilities in the follow up note. Since follow up visits are the most common office visit by far, this is a major flaw in the CMR2. The authors propose a Problem Focused Medical Record and the OHEAP (Orientation, History, Exam, Assessment and Plan) note to resolve this problem. OHEAP starts with a powerful orientation structure that brings forward the timeline, last Assessment and Plan, and Plan Results for each problem along with the patient's historical tables as the starting point of every follow up visit. The Assessment and Plan portion brings problem specific differential diagnoses and their workups along with other relevant tables such as expert systems, treatments, instructions, medical literature or pathways. This leads to Problem Focused Knowledge Navigation that brings powerful efficiencies to the CMR. By recognizing the true workflow in the longitudinal diagnosis and management of any medical problem, the efficiency of the CMR is maximized. OHEAP allows for optimal use of both personal and external data elements in the medical record. Its powerful orientation attributes minimize the time spent in analyzing the current status of the problem while its connections to problem specific databases help resolve the problem.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000171768600062","Problem focused knowledge navigation: Implementing the problem focused medical record and the O-HEAP note","1998","
<p>The current organization of most Computerized Medical Records (CMR) is based on the Problem Oriented Medical Record (POMR) and the SOAP (Subjective, Objective, Assessment and Plan) note(1). The organizational structure of the POMR and especially the SOAP note, does not allow for optimal use of computer capabilities in the follow up note. Since follow up visits are the most common office visit by far, this is a major flaw in the CMR2. The authors propose a Problem Focused Medical Record and the OHEAP (Orientation, History, Exam, Assessment and Plan) note to resolve this problem. OHEAP starts with a powerful orientation structure that brings forward the timeline, last Assessment and Plan, and Plan Results for each problem along with the patient's historical tables as the starting point of every follow up visit. The Assessment and Plan portion brings problem specific differential diagnoses and their workups along with other relevant tables such as expert systems, treatments, instructions, medical literature or pathways. This leads to Problem Focused Knowledge Navigation that brings powerful efficiencies to the CMR. By recognizing the true workflow in the longitudinal diagnosis and management of any medical problem, the efficiency of the CMR is maximized. OHEAP allows for optimal use of both personal and external data elements in the medical record. Its powerful orientation attributes minimize the time spent in analyzing the current status of the problem while its connections to problem specific databases help resolve the problem.</p>","Computer Science"
"WOS:000080459500022","Spinodal decomposition in two-dimensional binary fluids","1998","
<p>We use lattice-Boltzmann simulations to examine late stage spinodal decomposition in binary fluids, showing that the scaling hypothesis for phase ordering does not hold in all cases. We also examine the effects of an applied shear on the coarsening of the spinodal pattern.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000080459500022","Spinodal decomposition in two-dimensional binary fluids","1998","
<p>We use lattice-Boltzmann simulations to examine late stage spinodal decomposition in binary fluids, showing that the scaling hypothesis for phase ordering does not hold in all cases. We also examine the effects of an applied shear on the coarsening of the spinodal pattern.</p>","Computer Science"
"WOS:000077277700021","Calculation of electron- and photon-impact ionization via a close-coupling approach","1998","
<p>We discuss the applications of the Convergent Close-Coupling method to electron-impact ionization and photon-impact double ionization of helium. (C) 1998 Elsevier Science B.V.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000077277700021","Calculation of electron- and photon-impact ionization via a close-coupling approach","1998","
<p>We discuss the applications of the Convergent Close-Coupling method to electron-impact ionization and photon-impact double ionization of helium. (C) 1998 Elsevier Science B.V.</p>","Computer Science"
"WOS:000075427400006","Estimating face-pose consistency based on synthetic view space","1998","
<p>The visual appearance of an object in space is an image configuration projected from a subset of connected faces of the object. It is believed that face perception and face integration play a key role in object recognition in human vision. This paper presents a novel approach for calculating viewpoint consistency for three-dimensional (3-D) object recognition, which utilizes the perceptual models of face grouping and face integration. In the approach, faces are used as perceptual entities in accordance with the visual perception of shape constancy and face-pose consistency. To accommodate the perceptual knowledge of face visibility of objects, a synthetic view space (SVS) is developed. SVS is an abstractive perceptual space which partitions and synthesizes the conventional metric view sphere into a synthetic view box in which only a very limited set of synthetic views (s-views) need to be considered in estimating face-pose consistency. The s-views are structurally organized in a network, the view-connectivity net (VCN), which describes all the possible connections and constraints of the s-views in SVS. VCN provides a meaningful mechanism in pruning the search space of SVS during estimating face-pose consistency, The method has been successfully used for recognizing a class of industrial parts.</p>","Computer Science, Cybernetics"
"WOS:000075427400006","Estimating face-pose consistency based on synthetic view space","1998","
<p>The visual appearance of an object in space is an image configuration projected from a subset of connected faces of the object. It is believed that face perception and face integration play a key role in object recognition in human vision. This paper presents a novel approach for calculating viewpoint consistency for three-dimensional (3-D) object recognition, which utilizes the perceptual models of face grouping and face integration. In the approach, faces are used as perceptual entities in accordance with the visual perception of shape constancy and face-pose consistency. To accommodate the perceptual knowledge of face visibility of objects, a synthetic view space (SVS) is developed. SVS is an abstractive perceptual space which partitions and synthesizes the conventional metric view sphere into a synthetic view box in which only a very limited set of synthetic views (s-views) need to be considered in estimating face-pose consistency. The s-views are structurally organized in a network, the view-connectivity net (VCN), which describes all the possible connections and constraints of the s-views in SVS. VCN provides a meaningful mechanism in pruning the search space of SVS during estimating face-pose consistency, The method has been successfully used for recognizing a class of industrial parts.</p>","Computer Science, Theory & Methods"
"WOS:000075427400006","Estimating face-pose consistency based on synthetic view space","1998","
<p>The visual appearance of an object in space is an image configuration projected from a subset of connected faces of the object. It is believed that face perception and face integration play a key role in object recognition in human vision. This paper presents a novel approach for calculating viewpoint consistency for three-dimensional (3-D) object recognition, which utilizes the perceptual models of face grouping and face integration. In the approach, faces are used as perceptual entities in accordance with the visual perception of shape constancy and face-pose consistency. To accommodate the perceptual knowledge of face visibility of objects, a synthetic view space (SVS) is developed. SVS is an abstractive perceptual space which partitions and synthesizes the conventional metric view sphere into a synthetic view box in which only a very limited set of synthetic views (s-views) need to be considered in estimating face-pose consistency. The s-views are structurally organized in a network, the view-connectivity net (VCN), which describes all the possible connections and constraints of the s-views in SVS. VCN provides a meaningful mechanism in pruning the search space of SVS during estimating face-pose consistency, The method has been successfully used for recognizing a class of industrial parts.</p>","Computer Science"
"WOS:000072705400006","Load balancing problems for multiclass jobs in distributed/parallel computer systems","1998","
<p>Load balancing problems for multiclass jobs in distributed/parallel computer systems with general network configurations are considered. We construct a general model of such a distributed/parallel computer system. The system consists of heterogeneous host computers/processors (nodes) which are interconnected by a generally configured communication/interconnection network wherein there are several classes of jobs, each of which has its distinct delay function at each host and each communication link. This model is used to formulate the multiclass job load balancing problem as a nonlinear optimization problem in which the goal is to minimize the mean response time of a job.</p>
<p>A number of simple and intuitive theoretical results on the solution of the optimization problem are derived. On the basis of these results, we propose an effective load balancing algorithm for balancing the load over an entire distributed/parallel system. The proposed algorithm has two attractive features. One is that the algorithm can be implemented in a decentralized fashion. Another feature is simple and straightforward structure. Models of nodes, communication networks, and a numerical example are illustrated. The proposed algorithm is compared with a well-known standard steepest-descent algorithm, the FD algorithm. By using numerical experiments, we show that the proposed algorithm has much faster convergence in terms of computational time than the FD algorithm.</p>","Computer Science, Hardware & Architecture"
"WOS:000072705400006","Load balancing problems for multiclass jobs in distributed/parallel computer systems","1998","
<p>Load balancing problems for multiclass jobs in distributed/parallel computer systems with general network configurations are considered. We construct a general model of such a distributed/parallel computer system. The system consists of heterogeneous host computers/processors (nodes) which are interconnected by a generally configured communication/interconnection network wherein there are several classes of jobs, each of which has its distinct delay function at each host and each communication link. This model is used to formulate the multiclass job load balancing problem as a nonlinear optimization problem in which the goal is to minimize the mean response time of a job.</p>
<p>A number of simple and intuitive theoretical results on the solution of the optimization problem are derived. On the basis of these results, we propose an effective load balancing algorithm for balancing the load over an entire distributed/parallel system. The proposed algorithm has two attractive features. One is that the algorithm can be implemented in a decentralized fashion. Another feature is simple and straightforward structure. Models of nodes, communication networks, and a numerical example are illustrated. The proposed algorithm is compared with a well-known standard steepest-descent algorithm, the FD algorithm. By using numerical experiments, we show that the proposed algorithm has much faster convergence in terms of computational time than the FD algorithm.</p>","Computer Science"
"WOS:000075093800012","Logic optimization: Redundancy addition and removal using implication relations","1998","
<p>The logic optimization based on redundancy addition and removal is one of methods which can deal with large-scale logic circuits. In this logic optimization a few redundant elements are added to a logic circuit, and then many other redundant elements which are generated by the redundancy addition are identified and removed. In this paper an optimization method based on redundancy addition and removal using implication relations is proposed. The advantage of the proposed method is to identify removable redundant elements with short time, because the proposed method directly identifies redundant elements using implication relations from two illegal signal assignments which are produced by redundancy addition. The experimental results compared this method with another method show that this method is faster than the another method without declining the optimization ability.</p>","Computer Science, Information Systems"
"WOS:000075093800012","Logic optimization: Redundancy addition and removal using implication relations","1998","
<p>The logic optimization based on redundancy addition and removal is one of methods which can deal with large-scale logic circuits. In this logic optimization a few redundant elements are added to a logic circuit, and then many other redundant elements which are generated by the redundancy addition are identified and removed. In this paper an optimization method based on redundancy addition and removal using implication relations is proposed. The advantage of the proposed method is to identify removable redundant elements with short time, because the proposed method directly identifies redundant elements using implication relations from two illegal signal assignments which are produced by redundancy addition. The experimental results compared this method with another method show that this method is faster than the another method without declining the optimization ability.</p>","Computer Science, Software Engineering"
"WOS:000075093800012","Logic optimization: Redundancy addition and removal using implication relations","1998","
<p>The logic optimization based on redundancy addition and removal is one of methods which can deal with large-scale logic circuits. In this logic optimization a few redundant elements are added to a logic circuit, and then many other redundant elements which are generated by the redundancy addition are identified and removed. In this paper an optimization method based on redundancy addition and removal using implication relations is proposed. The advantage of the proposed method is to identify removable redundant elements with short time, because the proposed method directly identifies redundant elements using implication relations from two illegal signal assignments which are produced by redundancy addition. The experimental results compared this method with another method show that this method is faster than the another method without declining the optimization ability.</p>","Computer Science"
"WOS:000074567600010","Towards computational eco-analysis of building designs","1998","
<p>This paper argues that: (a) construction and operation of buildings is internationally a major cause of resource depletion and environmental pollution, (b) traditional criteria of construction cost and energy use estimation are insufficient in view of a comprehensive environmental impact analysis of buildings, (c) LCA (life-cycle analysis), ""eco-bookkeeping"" and ""eco-balance"" approaches could be advantageously utilized in concurrence with more traditional engineering, cost estimation and energy simulation to obtain a broader building design evaluation scheme and (d) appropriate IT-based computational tools could facilitate environmentally informed, integrative decisionmaking throughout the building design, construction, operation and decommissioning phases. Toward this end, this paper specifically introduces the prototypical realization of a computational tool for the integration of environmental impact modeling approaches within an object-oriented space-based CAD environment. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000074567600010","Towards computational eco-analysis of building designs","1998","
<p>This paper argues that: (a) construction and operation of buildings is internationally a major cause of resource depletion and environmental pollution, (b) traditional criteria of construction cost and energy use estimation are insufficient in view of a comprehensive environmental impact analysis of buildings, (c) LCA (life-cycle analysis), ""eco-bookkeeping"" and ""eco-balance"" approaches could be advantageously utilized in concurrence with more traditional engineering, cost estimation and energy simulation to obtain a broader building design evaluation scheme and (d) appropriate IT-based computational tools could facilitate environmentally informed, integrative decisionmaking throughout the building design, construction, operation and decommissioning phases. Toward this end, this paper specifically introduces the prototypical realization of a computational tool for the integration of environmental impact modeling approaches within an object-oriented space-based CAD environment. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science"
"WOS:000072290500003","Parallel genetic simulated annealing: A massively parallel SIMD algorithm","1998","
<p>Many significant engineering and scientific problems involve optimization of some criteria over a combinatorial configuration space. The two methods most often used to solve these problems effectively-simulated annealing (SA) and genetic algorithms (GA)-do not easily lend themselves to massive parallel implementations. Simulated annealing is a naturally serial algorithm, while GA involves a selection process that requires global coordination. This paper introduces a new hybrid algorithm that inherits those aspects of GA that lend themselves to parallelization, and avoids serial bottle-necks of GA approaches by incorporating elements of SA to provide a completely parallel, easily scalable hybrid GA/SA method. This new method, called Genetic Simulated Annealing, does not require parallelization of any problem specific portions of a serial implementation-existing serial implementations can be incorporated as is. Results of a study on two difficult combinatorial optimization problems, a 100 city traveling salesperson problem and a 24 word, 12 bit error correcting code design problem, performed on a 16K PE MasPar MP-1, indicate advantages over previous parallel GA and SA approaches. One of the key results is that the performance of the algorithm scales up linearly with the increase of processing elements, a feature not demonstrated by any previous parallel GA or SA approaches, which enables the new algorithm to utilize massive parallel architecture with maximum effectiveness. Additionally, the algorithm does not require careful choice of central parameters, a significant advantage over SA and GA.</p>","Computer Science, Theory & Methods"
"WOS:000072290500003","Parallel genetic simulated annealing: A massively parallel SIMD algorithm","1998","
<p>Many significant engineering and scientific problems involve optimization of some criteria over a combinatorial configuration space. The two methods most often used to solve these problems effectively-simulated annealing (SA) and genetic algorithms (GA)-do not easily lend themselves to massive parallel implementations. Simulated annealing is a naturally serial algorithm, while GA involves a selection process that requires global coordination. This paper introduces a new hybrid algorithm that inherits those aspects of GA that lend themselves to parallelization, and avoids serial bottle-necks of GA approaches by incorporating elements of SA to provide a completely parallel, easily scalable hybrid GA/SA method. This new method, called Genetic Simulated Annealing, does not require parallelization of any problem specific portions of a serial implementation-existing serial implementations can be incorporated as is. Results of a study on two difficult combinatorial optimization problems, a 100 city traveling salesperson problem and a 24 word, 12 bit error correcting code design problem, performed on a 16K PE MasPar MP-1, indicate advantages over previous parallel GA and SA approaches. One of the key results is that the performance of the algorithm scales up linearly with the increase of processing elements, a feature not demonstrated by any previous parallel GA or SA approaches, which enables the new algorithm to utilize massive parallel architecture with maximum effectiveness. Additionally, the algorithm does not require careful choice of central parameters, a significant advantage over SA and GA.</p>","Computer Science"
"WOS:000082774900049","Generalization in Wilson's classifier system","1998","
<p>We analyze generalization with the XCS classifier system when the system is applied to animat problems in grid-worlds. Our aim is to give a unified view of generalization with XCS, in order to explain some of the phenomena reported in the literature. Initially, we apply XCS to two environments. Our results show that there are situations in which the generalization mechanism of XCS may prevent the system from converging to optimum. Accordingly, we study XCS's generalization mechanism analyzing the conditions under which the system may fail to evolve an optimal solution. We draw a hypothesis in order to explain the results reported so far. Our hypothesis suggests that XCS fails to learn an optimal solution when, due to the environment structure and to the exploration strategy employed, the system does not explore all the areas of the environment frequently. We thus introduce a meta exploration strategy that is used as theoretical tool to validate our hypothesis experimentally.</p>","Computer Science, Theory & Methods"
"WOS:000082774900049","Generalization in Wilson's classifier system","1998","
<p>We analyze generalization with the XCS classifier system when the system is applied to animat problems in grid-worlds. Our aim is to give a unified view of generalization with XCS, in order to explain some of the phenomena reported in the literature. Initially, we apply XCS to two environments. Our results show that there are situations in which the generalization mechanism of XCS may prevent the system from converging to optimum. Accordingly, we study XCS's generalization mechanism analyzing the conditions under which the system may fail to evolve an optimal solution. We draw a hypothesis in order to explain the results reported so far. Our hypothesis suggests that XCS fails to learn an optimal solution when, due to the environment structure and to the exploration strategy employed, the system does not explore all the areas of the environment frequently. We thus introduce a meta exploration strategy that is used as theoretical tool to validate our hypothesis experimentally.</p>","Computer Science"
"WOS:000082116300008","Combinatorial linear programming: Geometry can help","1998","
<p>We consider a class A of generalized linear programs on the d-cube (due to Matousek) and prove that Kalai's subexponential simplex algorithm RANDOM-FACET is polynomial on all actual linear programs in the class. In contrast, the subexponential analysis is known to be best possible for general instances in A. Thus, we identify a, ""geometric"" property of linear programming that goes beyond all abstract notions previously employed in generalized linear programming frameworks, and that can be exploited by the simplex method in a,nontrivial setting.</p>","Computer Science, Software Engineering"
"WOS:000082116300008","Combinatorial linear programming: Geometry can help","1998","
<p>We consider a class A of generalized linear programs on the d-cube (due to Matousek) and prove that Kalai's subexponential simplex algorithm RANDOM-FACET is polynomial on all actual linear programs in the class. In contrast, the subexponential analysis is known to be best possible for general instances in A. Thus, we identify a, ""geometric"" property of linear programming that goes beyond all abstract notions previously employed in generalized linear programming frameworks, and that can be exploited by the simplex method in a,nontrivial setting.</p>","Computer Science, Theory & Methods"
"WOS:000082116300008","Combinatorial linear programming: Geometry can help","1998","
<p>We consider a class A of generalized linear programs on the d-cube (due to Matousek) and prove that Kalai's subexponential simplex algorithm RANDOM-FACET is polynomial on all actual linear programs in the class. In contrast, the subexponential analysis is known to be best possible for general instances in A. Thus, we identify a, ""geometric"" property of linear programming that goes beyond all abstract notions previously employed in generalized linear programming frameworks, and that can be exploited by the simplex method in a,nontrivial setting.</p>","Computer Science"
"WOS:000171768600057","""Virtual"" clinical trials: Case control experiments utilizing a health services research workstation","1998","
<p>We created an interface to a growing repository of clinical and administrative information to facilitate the design and execution of case-control experiments. The system enables knowledgeable users to generate and test hypotheses regarding associations among diseases and outcomes. The intuitive interface allows the user to specify criteria for selecting cases and defining putative risks. The repository contains comprehensive administrative and selected clinical information on all ambulatory and emergency department visits as well as hospital admissions since 1994. We tested the workstation's ability to determine relationships between outpatient diagnoses including hypertension, osteoarthritis and hypercholesterolemia with the occurrence of admissions for stroke and myocardial infarction and achieved results consistent with published studies. Successful implementation of this Health Services Research Workstation will allow ""virtual"" clinical trials to validate the results of formal clinical trials on a local population and may provide meaningful analyses of data when formal clinical trials are not feasible.</p>","Computer Science, Information Systems"
"WOS:000171768600057","""Virtual"" clinical trials: Case control experiments utilizing a health services research workstation","1998","
<p>We created an interface to a growing repository of clinical and administrative information to facilitate the design and execution of case-control experiments. The system enables knowledgeable users to generate and test hypotheses regarding associations among diseases and outcomes. The intuitive interface allows the user to specify criteria for selecting cases and defining putative risks. The repository contains comprehensive administrative and selected clinical information on all ambulatory and emergency department visits as well as hospital admissions since 1994. We tested the workstation's ability to determine relationships between outpatient diagnoses including hypertension, osteoarthritis and hypercholesterolemia with the occurrence of admissions for stroke and myocardial infarction and achieved results consistent with published studies. Successful implementation of this Health Services Research Workstation will allow ""virtual"" clinical trials to validate the results of formal clinical trials on a local population and may provide meaningful analyses of data when formal clinical trials are not feasible.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000171768600057","""Virtual"" clinical trials: Case control experiments utilizing a health services research workstation","1998","
<p>We created an interface to a growing repository of clinical and administrative information to facilitate the design and execution of case-control experiments. The system enables knowledgeable users to generate and test hypotheses regarding associations among diseases and outcomes. The intuitive interface allows the user to specify criteria for selecting cases and defining putative risks. The repository contains comprehensive administrative and selected clinical information on all ambulatory and emergency department visits as well as hospital admissions since 1994. We tested the workstation's ability to determine relationships between outpatient diagnoses including hypertension, osteoarthritis and hypercholesterolemia with the occurrence of admissions for stroke and myocardial infarction and achieved results consistent with published studies. Successful implementation of this Health Services Research Workstation will allow ""virtual"" clinical trials to validate the results of formal clinical trials on a local population and may provide meaningful analyses of data when formal clinical trials are not feasible.</p>","Computer Science"
"WOS:000075656800006","Application of the extremum stack to neurological MRI","1998","
<p>The extremum stack, as proposed by Koenderink, is a multiresolution image description and segmentation scheme which examines intensity extrema (minima and maxima) as they move and merge through a series of progressively isotropically diffused images known as scale space. Such a data-driven approach is attractive because it is claimed to be a generally applicable and natural method of image segmentation, The performance of the extremum stack is evaluated here using the case of neurological magnetic resonance imaging data as a specific example, and means of improving its performance proposed, It is confirmed experimentally that the extremum stack has the desirable property of being shift-, scale-, and rotation-invariant, and produces natural results for many compact regions of anatomy, It handles elongated objects poorly, however, and subsections of regions may merge prematurely before each region is represented as a single node. It is shown that this premature merging can often be avoided by the application of either a variable conductance-diffusing preprocessing step, or more effectively, the use of an adaptive variable conductance diffusion method within the extremum stack itself in place of the isotropic Gaussian diffusion proposed by Koenderink.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000075656800006","Application of the extremum stack to neurological MRI","1998","
<p>The extremum stack, as proposed by Koenderink, is a multiresolution image description and segmentation scheme which examines intensity extrema (minima and maxima) as they move and merge through a series of progressively isotropically diffused images known as scale space. Such a data-driven approach is attractive because it is claimed to be a generally applicable and natural method of image segmentation, The performance of the extremum stack is evaluated here using the case of neurological magnetic resonance imaging data as a specific example, and means of improving its performance proposed, It is confirmed experimentally that the extremum stack has the desirable property of being shift-, scale-, and rotation-invariant, and produces natural results for many compact regions of anatomy, It handles elongated objects poorly, however, and subsections of regions may merge prematurely before each region is represented as a single node. It is shown that this premature merging can often be avoided by the application of either a variable conductance-diffusing preprocessing step, or more effectively, the use of an adaptive variable conductance diffusion method within the extremum stack itself in place of the isotropic Gaussian diffusion proposed by Koenderink.</p>","Computer Science"
"WOS:000082115900107","Interactive pre-operative selection of cutting constraints, and interactive force controlled knee surgery by a surgical robot","1998","
<p>This paper describes a low-cost computer system that takes CT images of the knee, and with three-dimensional models of knee prostheses allows a surgeon to position the prosthesis correctly pre-operatively in an interactive manner. Once in position the computer can process bone and prosthesis geometry to derive a set of constraint boundaries that constitute a safe cutting area for a force controlled robot (i.e, that avoids soft tissue such as ligaments), and provides the correct cutting planes for good prosthesis/bone alignment. This boundary information is used to program the robot, allowing a surgeon to move the robot within predefined regions to machine away bone accurately whilst preventing damage to soft tissue.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000082115900107","Interactive pre-operative selection of cutting constraints, and interactive force controlled knee surgery by a surgical robot","1998","
<p>This paper describes a low-cost computer system that takes CT images of the knee, and with three-dimensional models of knee prostheses allows a surgeon to position the prosthesis correctly pre-operatively in an interactive manner. Once in position the computer can process bone and prosthesis geometry to derive a set of constraint boundaries that constitute a safe cutting area for a force controlled robot (i.e, that avoids soft tissue such as ligaments), and provides the correct cutting planes for good prosthesis/bone alignment. This boundary information is used to program the robot, allowing a surgeon to move the robot within predefined regions to machine away bone accurately whilst preventing damage to soft tissue.</p>","Computer Science, Theory & Methods"
"WOS:000082115900107","Interactive pre-operative selection of cutting constraints, and interactive force controlled knee surgery by a surgical robot","1998","
<p>This paper describes a low-cost computer system that takes CT images of the knee, and with three-dimensional models of knee prostheses allows a surgeon to position the prosthesis correctly pre-operatively in an interactive manner. Once in position the computer can process bone and prosthesis geometry to derive a set of constraint boundaries that constitute a safe cutting area for a force controlled robot (i.e, that avoids soft tissue such as ligaments), and provides the correct cutting planes for good prosthesis/bone alignment. This boundary information is used to program the robot, allowing a surgeon to move the robot within predefined regions to machine away bone accurately whilst preventing damage to soft tissue.</p>","Computer Science"
"WOS:000072754600006","From roles to teamwork: A framework and architecture","1998","
<p>Teamwork requires organization, strategies, and coordination. The design of a multiagent system should support these conceptual properties for constructing effective teams. The advantage of a teamwork approach is the reduction in complexity of the task through distribution of responsibilities, resulting in better utilization of resources, robust behaviors, and a greater variety of behaviors against competitors. In this article a framework for building teams of responsible agents using roles, responsibilities, and strategies is described. Its application to the domain of soccer is used to design a high-performance team of soccer agents. The architecture for these agents utilizes a reactive planning system with support for teamwork. The team of soccer agents will be tested in a series of competitions against other teams in the real-time soccer simulator proposed for Robocup-97, which provides an uncertain, resource-bounded world.</p>","Computer Science, Artificial Intelligence"
"WOS:000072754600006","From roles to teamwork: A framework and architecture","1998","
<p>Teamwork requires organization, strategies, and coordination. The design of a multiagent system should support these conceptual properties for constructing effective teams. The advantage of a teamwork approach is the reduction in complexity of the task through distribution of responsibilities, resulting in better utilization of resources, robust behaviors, and a greater variety of behaviors against competitors. In this article a framework for building teams of responsible agents using roles, responsibilities, and strategies is described. Its application to the domain of soccer is used to design a high-performance team of soccer agents. The architecture for these agents utilizes a reactive planning system with support for teamwork. The team of soccer agents will be tested in a series of competitions against other teams in the real-time soccer simulator proposed for Robocup-97, which provides an uncertain, resource-bounded world.</p>","Computer Science"
"WOS:000073410500006","Performance analysis and its impact on design","1998","
<p>Gone are the days when one or two expert architects would use hunches, experience, and rules of thumb to determine a processor's features. Marketplace competition has long since forced companies to replace this ad hoc process with a targeted and highly systematic process that focuses new designs on specific workloads.</p>
<p>At the core of these processes are models of the processor's performance and its workloads. Developing and verifying these models is the domain now called performance analysis.</p>
<p>However, a systematic design process has some disadvantages because it can constrain the diversity and accuracy of the workloads we can measure. These are hot problems in performance analysis.</p>
<p>In this article, the authors describe the advances in performance analysis over the last decade, focusing on architectural performance. They predict that the industry will soon need new simulation methods to accommodate future processors.</p>
<p>Researchers have presented some of these advanced simulation methodologies at past workshops on Performance Analysis and its Impact on Design (PAID) workshops. This article - a tutorial - is the one of three articles in this issue about performance analysis. It is accompanied by articles based on presentations from last year's workshop. Although the PAID workshops focus on computer (hardware-software) systems, these techniques are in many ways universal and adaptable to the analysis of other performance-critical systems.</p>","Computer Science, Hardware & Architecture"
"WOS:000073410500006","Performance analysis and its impact on design","1998","
<p>Gone are the days when one or two expert architects would use hunches, experience, and rules of thumb to determine a processor's features. Marketplace competition has long since forced companies to replace this ad hoc process with a targeted and highly systematic process that focuses new designs on specific workloads.</p>
<p>At the core of these processes are models of the processor's performance and its workloads. Developing and verifying these models is the domain now called performance analysis.</p>
<p>However, a systematic design process has some disadvantages because it can constrain the diversity and accuracy of the workloads we can measure. These are hot problems in performance analysis.</p>
<p>In this article, the authors describe the advances in performance analysis over the last decade, focusing on architectural performance. They predict that the industry will soon need new simulation methods to accommodate future processors.</p>
<p>Researchers have presented some of these advanced simulation methodologies at past workshops on Performance Analysis and its Impact on Design (PAID) workshops. This article - a tutorial - is the one of three articles in this issue about performance analysis. It is accompanied by articles based on presentations from last year's workshop. Although the PAID workshops focus on computer (hardware-software) systems, these techniques are in many ways universal and adaptable to the analysis of other performance-critical systems.</p>","Computer Science, Software Engineering"
"WOS:000073410500006","Performance analysis and its impact on design","1998","
<p>Gone are the days when one or two expert architects would use hunches, experience, and rules of thumb to determine a processor's features. Marketplace competition has long since forced companies to replace this ad hoc process with a targeted and highly systematic process that focuses new designs on specific workloads.</p>
<p>At the core of these processes are models of the processor's performance and its workloads. Developing and verifying these models is the domain now called performance analysis.</p>
<p>However, a systematic design process has some disadvantages because it can constrain the diversity and accuracy of the workloads we can measure. These are hot problems in performance analysis.</p>
<p>In this article, the authors describe the advances in performance analysis over the last decade, focusing on architectural performance. They predict that the industry will soon need new simulation methods to accommodate future processors.</p>
<p>Researchers have presented some of these advanced simulation methodologies at past workshops on Performance Analysis and its Impact on Design (PAID) workshops. This article - a tutorial - is the one of three articles in this issue about performance analysis. It is accompanied by articles based on presentations from last year's workshop. Although the PAID workshops focus on computer (hardware-software) systems, these techniques are in many ways universal and adaptable to the analysis of other performance-critical systems.</p>","Computer Science"
"WOS:000074051400007","An efficient and robust integration technique for applied random vibration analysis","1998","
<p>The mode-based finite element formulation of the equations of motion is usually adopted for linear random vibration analysis (RVA). In general, the RVA of large systems requires a large number of numerical integrations which is very time-consuming for a reasonable level of desired accuracy. Moreover, conventional numerical integration methods may fail to converge when the integrands are highly oscillatory due to slow propagation velocities. In this paper, a robust general-purpose RVA integration technique which can overcome these drawbacks is presented. Multi-point base and nodal excitations including wave passage effect and frequency-independent spatial correlation can be taken into account in the analysis. The proposed technique is based on the closed-form solutions for polynomial-type power spectral density functions and has been verified to be efficient and accurate for many engineering problems. This paper describes the implementation details, presents two examples taken from engineering applications and demonstrates the dramatic time-saving in the computation compared to numerical integration solutions.; Response statistics, such as standard deviation of structural responses, are computed and displayed over the entire structures for these examples. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000074051400007","An efficient and robust integration technique for applied random vibration analysis","1998","
<p>The mode-based finite element formulation of the equations of motion is usually adopted for linear random vibration analysis (RVA). In general, the RVA of large systems requires a large number of numerical integrations which is very time-consuming for a reasonable level of desired accuracy. Moreover, conventional numerical integration methods may fail to converge when the integrands are highly oscillatory due to slow propagation velocities. In this paper, a robust general-purpose RVA integration technique which can overcome these drawbacks is presented. Multi-point base and nodal excitations including wave passage effect and frequency-independent spatial correlation can be taken into account in the analysis. The proposed technique is based on the closed-form solutions for polynomial-type power spectral density functions and has been verified to be efficient and accurate for many engineering problems. This paper describes the implementation details, presents two examples taken from engineering applications and demonstrates the dramatic time-saving in the computation compared to numerical integration solutions.; Response statistics, such as standard deviation of structural responses, are computed and displayed over the entire structures for these examples. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science"
"WOS:000074574900005","A dynamic programming based algorithm for the crew scheduling problem","1998","
<p>In this paper we consider the crew scheduling problem, that is the problem of assigning K crews to tasks with fixed start and finish times such that each crew does not exceed a limit on the total time it can spend working. This problem is formulated as a problem of finding K time limit constrained vertex disjoint paths which visit all vertices on a network. A lower bound for the problem is found via dynamic programming. This lower bound is improved via a Lagrangean based penalty procedure and subgradient optimisation. Computational results are given for a number of randomly generated problems involving between 50 and 500 tasks. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000074574900005","A dynamic programming based algorithm for the crew scheduling problem","1998","
<p>In this paper we consider the crew scheduling problem, that is the problem of assigning K crews to tasks with fixed start and finish times such that each crew does not exceed a limit on the total time it can spend working. This problem is formulated as a problem of finding K time limit constrained vertex disjoint paths which visit all vertices on a network. A lower bound for the problem is found via dynamic programming. This lower bound is improved via a Lagrangean based penalty procedure and subgradient optimisation. Computational results are given for a number of randomly generated problems involving between 50 and 500 tasks. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science"
"WOS:000072593500008","Randomized observational studies on the economics of therapies - Biometrical experience of two trials","1998","
<p>Economic studies in medicine are intended to investigate costs, associated with a particular problem dealing with the indication, diagnosis or therapy, for instance, whether the high costs involved in a highly intensive or innovative therapy could be balanced by the eventual savings made, due to the shorter periods of treatment. In such situations a randomized controlled trial is necessary to find out which therapy or which therapeutical strategy is least expensive in the long run. Economic studies do, however, present some specific problems. Making a list of all the cost-relevant treatment items can be very laborious, but the use of flat rates and lump sums alone cannot lead to a complete cost analysis. Often, costs between hospitals vary more than between treatment regimens.</p>
<p>Early and sudden deaths incur low costs and may bias the results. Furthermore, costs are distributed with a long and heavy upper tail including extreme outliers. This does, in fact, complicate the estimation of the sample size. In this article, these problems are outlined and, with the help of the data obtained from two randomized economic trials in health care, solutions are proposed and discussed.</p>","Computer Science, Information Systems"
"WOS:000072593500008","Randomized observational studies on the economics of therapies - Biometrical experience of two trials","1998","
<p>Economic studies in medicine are intended to investigate costs, associated with a particular problem dealing with the indication, diagnosis or therapy, for instance, whether the high costs involved in a highly intensive or innovative therapy could be balanced by the eventual savings made, due to the shorter periods of treatment. In such situations a randomized controlled trial is necessary to find out which therapy or which therapeutical strategy is least expensive in the long run. Economic studies do, however, present some specific problems. Making a list of all the cost-relevant treatment items can be very laborious, but the use of flat rates and lump sums alone cannot lead to a complete cost analysis. Often, costs between hospitals vary more than between treatment regimens.</p>
<p>Early and sudden deaths incur low costs and may bias the results. Furthermore, costs are distributed with a long and heavy upper tail including extreme outliers. This does, in fact, complicate the estimation of the sample size. In this article, these problems are outlined and, with the help of the data obtained from two randomized economic trials in health care, solutions are proposed and discussed.</p>","Computer Science"
"WOS:000074720000068","A new probe for eddy current testing of low conductivity materials","1998","
<p>In the paper a new probe for eddy current testing of low conductivity materials is presented. The new probe has been applied for eddy current testing of H2SO4 water solution. Results of measurements are presented.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000074720000068","A new probe for eddy current testing of low conductivity materials","1998","
<p>In the paper a new probe for eddy current testing of low conductivity materials is presented. The new probe has been applied for eddy current testing of H2SO4 water solution. Results of measurements are presented.</p>","Computer Science"
"WOS:000072321700002","Refinement and validation of software requirements using incremental simulation","1998","
<p>Requirements engineering refers to activities of gathering and organizing customer requirements and system specifications, making explicit representations of them, and making sure that they are valid and accounted for during the course of the design lifecycle of software. One very popular software development practice is the incremental development practice. The incremental development refers to practices that allow a program, or similarly specifications, to be developed, validated, and delivered in stages. The incremental practice is characterized by its depth-first process where focuses are given to small parts of the system in sequence to fair amounts of detail. In this paper, we present a development and validation of specifications in such an incremental style using a tool called ASADAL, a comprehensive CASE tool for real-time systems. ASADAL supports incremental and hierarchical refinements of specifications using multiple representational constructs and the evolving incomplete specifications can be formally tested with respect to critical real time properties or be simulated to determine whether the specifications capture the intended system behavior. In particular, we highlight features of ASADAL's specification simulator, called ASADAL/SIM, that plays a critical role in the incremental validation and helps users gain insights into the validity of evolving specifications. Such features include the multiple and mixed level simulation, real-value simulation, presentation and analysis of simulation data, and variety of flexible simulation control schemes. We illustrate the overall process using an example of an incremental specification development of an elevator control system.</p>","Computer Science, Information Systems"
"WOS:000072321700002","Refinement and validation of software requirements using incremental simulation","1998","
<p>Requirements engineering refers to activities of gathering and organizing customer requirements and system specifications, making explicit representations of them, and making sure that they are valid and accounted for during the course of the design lifecycle of software. One very popular software development practice is the incremental development practice. The incremental development refers to practices that allow a program, or similarly specifications, to be developed, validated, and delivered in stages. The incremental practice is characterized by its depth-first process where focuses are given to small parts of the system in sequence to fair amounts of detail. In this paper, we present a development and validation of specifications in such an incremental style using a tool called ASADAL, a comprehensive CASE tool for real-time systems. ASADAL supports incremental and hierarchical refinements of specifications using multiple representational constructs and the evolving incomplete specifications can be formally tested with respect to critical real time properties or be simulated to determine whether the specifications capture the intended system behavior. In particular, we highlight features of ASADAL's specification simulator, called ASADAL/SIM, that plays a critical role in the incremental validation and helps users gain insights into the validity of evolving specifications. Such features include the multiple and mixed level simulation, real-value simulation, presentation and analysis of simulation data, and variety of flexible simulation control schemes. We illustrate the overall process using an example of an incremental specification development of an elevator control system.</p>","Computer Science, Software Engineering"
"WOS:000072321700002","Refinement and validation of software requirements using incremental simulation","1998","
<p>Requirements engineering refers to activities of gathering and organizing customer requirements and system specifications, making explicit representations of them, and making sure that they are valid and accounted for during the course of the design lifecycle of software. One very popular software development practice is the incremental development practice. The incremental development refers to practices that allow a program, or similarly specifications, to be developed, validated, and delivered in stages. The incremental practice is characterized by its depth-first process where focuses are given to small parts of the system in sequence to fair amounts of detail. In this paper, we present a development and validation of specifications in such an incremental style using a tool called ASADAL, a comprehensive CASE tool for real-time systems. ASADAL supports incremental and hierarchical refinements of specifications using multiple representational constructs and the evolving incomplete specifications can be formally tested with respect to critical real time properties or be simulated to determine whether the specifications capture the intended system behavior. In particular, we highlight features of ASADAL's specification simulator, called ASADAL/SIM, that plays a critical role in the incremental validation and helps users gain insights into the validity of evolving specifications. Such features include the multiple and mixed level simulation, real-value simulation, presentation and analysis of simulation data, and variety of flexible simulation control schemes. We illustrate the overall process using an example of an incremental specification development of an elevator control system.</p>","Computer Science"
"WOS:000081725500006","The British national information strategy","1998","
<p>Many countries have developed Frameworks of policies to guide their transition into information societies. The approach is consistent and systematic, In Britain we have not attempted to develop policies in this way. Rather they have emerged as a result of disparate initiatives. The current policies are analysed using a matrix which identifies three Levels of policy: industrial, organisation and social; and four cross-cutting themes: information technology, information markets, human resources and legislation and regulation. Together, the various initiatives add up to a national strategy but it is one that lacks coordination and cohesion.</p>","Computer Science, Information Systems"
"WOS:000081725500006","The British national information strategy","1998","
<p>Many countries have developed Frameworks of policies to guide their transition into information societies. The approach is consistent and systematic, In Britain we have not attempted to develop policies in this way. Rather they have emerged as a result of disparate initiatives. The current policies are analysed using a matrix which identifies three Levels of policy: industrial, organisation and social; and four cross-cutting themes: information technology, information markets, human resources and legislation and regulation. Together, the various initiatives add up to a national strategy but it is one that lacks coordination and cohesion.</p>","Computer Science"
"WOS:000075048600006","A neural network study of precollicular saccadic averaging","1998","
<p>Saccadic averaging is the phenomenon that two simultaneously presented retinal inputs result in a saccade with an endpoint located on an intermediate position between the two stimuli. Recordings from neurons in the deeper layers of the superior colliculus have revealed neural correlates of saccade averaging, indicating that it takes place at this level or upstream. Recently, we proposed a neural network for internal feedback in saccades. This neural network model is different from other models in that it suggests the possibility that averaging takes place in a stage upstream of the colliculus. The network consists of output units representing the neural map of the deeper layers of the superior colliculus and hidden layers imitating areas in the posterior parietal cortex. The deeper layers of the superior colliculus represent the motor error of a desired saccade, e.g. an eye movement to a visual target. In this article we show that averaging is an emergent property of the proposed network. When two retinal targets with different intensities are simultaneously presented to the network, the activity in the output layer represents a single motor error with a weighted average value. Our goal is to understand the mechanism of weighted averaging in this neural network. It appears that averaging in the model is caused by the linear dependence of the net input, received by the hidden units, on retinal error, independent of its retinal coding format. For nonnormalized retinal error inputs, also the nonlinearity between the net input and the activity of the hidden units plays a role in the averaging process. The averaging properties of the model are in agreement with physiological experiments if the hypothetical retinal error input map is normalized. The neural network predicts that if this normalization is overruled by electrical stimulation, averaging still takes place. However, in this case - as a consequence of the feedback task - the location of the resulting saccade depends on the initial eye position and the total intensity/current applied at the two locations. This could be a way to verify the neural network model. If the assumptions for the model are valid, a physiological implication of this paper is that averaging of saccades takes place upstream of the superior colliculus.</p>","Computer Science, Cybernetics"
"WOS:000075048600006","A neural network study of precollicular saccadic averaging","1998","
<p>Saccadic averaging is the phenomenon that two simultaneously presented retinal inputs result in a saccade with an endpoint located on an intermediate position between the two stimuli. Recordings from neurons in the deeper layers of the superior colliculus have revealed neural correlates of saccade averaging, indicating that it takes place at this level or upstream. Recently, we proposed a neural network for internal feedback in saccades. This neural network model is different from other models in that it suggests the possibility that averaging takes place in a stage upstream of the colliculus. The network consists of output units representing the neural map of the deeper layers of the superior colliculus and hidden layers imitating areas in the posterior parietal cortex. The deeper layers of the superior colliculus represent the motor error of a desired saccade, e.g. an eye movement to a visual target. In this article we show that averaging is an emergent property of the proposed network. When two retinal targets with different intensities are simultaneously presented to the network, the activity in the output layer represents a single motor error with a weighted average value. Our goal is to understand the mechanism of weighted averaging in this neural network. It appears that averaging in the model is caused by the linear dependence of the net input, received by the hidden units, on retinal error, independent of its retinal coding format. For nonnormalized retinal error inputs, also the nonlinearity between the net input and the activity of the hidden units plays a role in the averaging process. The averaging properties of the model are in agreement with physiological experiments if the hypothetical retinal error input map is normalized. The neural network predicts that if this normalization is overruled by electrical stimulation, averaging still takes place. However, in this case - as a consequence of the feedback task - the location of the resulting saccade depends on the initial eye position and the total intensity/current applied at the two locations. This could be a way to verify the neural network model. If the assumptions for the model are valid, a physiological implication of this paper is that averaging of saccades takes place upstream of the superior colliculus.</p>","Computer Science"
"WOS:000075181000008","Model-based codesign","1998","
<p>Hardware-software codesign has been a research topic since the beginning of this decade, but only now are structured methods emerging that focus on automating design. Unfortunately, to date most codesign approaches simply leverage performance from individual hardware and software tools, rather than enforcing a structured integration of hardware and software systems simultaneously. A few frameworks have successfully done this integration and have the potential for significant benefits, including reduced time to market, smaller scale design, better likelihood of component reuse, and maximum use of processing power.</p>
<p>This article describes a codesign approach that lets developers create models of a formal system representation independently of the hardware and software implementation. The authors' framework, which targets embedded systems, lets developers use simulation-based modeling to explore the feasibility of virtual prototypes and then interactively map the specification onto a software-hardware architecture.</p>","Computer Science, Hardware & Architecture"
"WOS:000075181000008","Model-based codesign","1998","
<p>Hardware-software codesign has been a research topic since the beginning of this decade, but only now are structured methods emerging that focus on automating design. Unfortunately, to date most codesign approaches simply leverage performance from individual hardware and software tools, rather than enforcing a structured integration of hardware and software systems simultaneously. A few frameworks have successfully done this integration and have the potential for significant benefits, including reduced time to market, smaller scale design, better likelihood of component reuse, and maximum use of processing power.</p>
<p>This article describes a codesign approach that lets developers create models of a formal system representation independently of the hardware and software implementation. The authors' framework, which targets embedded systems, lets developers use simulation-based modeling to explore the feasibility of virtual prototypes and then interactively map the specification onto a software-hardware architecture.</p>","Computer Science, Software Engineering"
"WOS:000075181000008","Model-based codesign","1998","
<p>Hardware-software codesign has been a research topic since the beginning of this decade, but only now are structured methods emerging that focus on automating design. Unfortunately, to date most codesign approaches simply leverage performance from individual hardware and software tools, rather than enforcing a structured integration of hardware and software systems simultaneously. A few frameworks have successfully done this integration and have the potential for significant benefits, including reduced time to market, smaller scale design, better likelihood of component reuse, and maximum use of processing power.</p>
<p>This article describes a codesign approach that lets developers create models of a formal system representation independently of the hardware and software implementation. The authors' framework, which targets embedded systems, lets developers use simulation-based modeling to explore the feasibility of virtual prototypes and then interactively map the specification onto a software-hardware architecture.</p>","Computer Science"
"WOS:000073999000006","Efficient and highly accurate computation of a class of radially symmetric solutions of the Navier-Stokes equation and the heat equation in two dimensions","1998","
<p>In this paper we test several different formulas for the computation of the exact vorticity and angular velocity in certain radially symmetric solutions of the two-dimensional Navier-Stokes equation in vorticity-stream function form. The class of initial conditions for the vorticity considered here has often been used by many authors in the study of vortex methods. However, only in the case of zero viscosity has it been possible to efficiently compute the exact vorticity and velocity at later times. The expressions for the vorticity and angular velocity, given in this paper, enable us to compute these quantities both efficiently and highly accurately for nonzero viscosity. This makes it feasible to obtain reliable error measurements in the study of vortex methods for the Navier-Stokes equation. (C) 1998 Academic Press.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000073999000006","Efficient and highly accurate computation of a class of radially symmetric solutions of the Navier-Stokes equation and the heat equation in two dimensions","1998","
<p>In this paper we test several different formulas for the computation of the exact vorticity and angular velocity in certain radially symmetric solutions of the two-dimensional Navier-Stokes equation in vorticity-stream function form. The class of initial conditions for the vorticity considered here has often been used by many authors in the study of vortex methods. However, only in the case of zero viscosity has it been possible to efficiently compute the exact vorticity and velocity at later times. The expressions for the vorticity and angular velocity, given in this paper, enable us to compute these quantities both efficiently and highly accurately for nonzero viscosity. This makes it feasible to obtain reliable error measurements in the study of vortex methods for the Navier-Stokes equation. (C) 1998 Academic Press.</p>","Computer Science"
"WOS:000074567600008","A dependency network generator for standards processing","1998","
<p>This paper presents a hybrid approach to standards processing. The use of conventional printed codes of practice is inherently fraught with many problems. In particular, the problem of following the flow of information through:a standard is addressed in this paper. A standards processing environment, SADA (standards automated design assistant), is described in this paper. SADA consists of a hypertext representation of the code and a separate object-oriented processing module. Another component of the SADA model, the dependency network generator, creates dependency networks from the hypertext representation of the code. These dependency networks are used by the processing module to determine the correct processing order of the code provisions. The dependency network generator (DNG) ensures that a direct mapping exists between the hypertext representation of the code and the processing module. A brief overview of the standards processing environment is provided, followed by a detailed description of the dependency network generator (DNG). The DNG is considered crucial to the effective operation of the hybrid environment. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000074567600008","A dependency network generator for standards processing","1998","
<p>This paper presents a hybrid approach to standards processing. The use of conventional printed codes of practice is inherently fraught with many problems. In particular, the problem of following the flow of information through:a standard is addressed in this paper. A standards processing environment, SADA (standards automated design assistant), is described in this paper. SADA consists of a hypertext representation of the code and a separate object-oriented processing module. Another component of the SADA model, the dependency network generator, creates dependency networks from the hypertext representation of the code. These dependency networks are used by the processing module to determine the correct processing order of the code provisions. The dependency network generator (DNG) ensures that a direct mapping exists between the hypertext representation of the code and the processing module. A brief overview of the standards processing environment is provided, followed by a detailed description of the dependency network generator (DNG). The DNG is considered crucial to the effective operation of the hybrid environment. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science"
"WOS:000071193500030","On the constructions of constant-weight codes","1998","
<p>Two methods of constructing binary constant-weight codes from 1) codes over GF(q) and 2) constant-weight codes over GF(q) are presented. Several classes of binary optimum constant-weight codes are derived from these methods. in general, we show that binary optimum constant-weight codes, which achieve the Johnson bound, can be constructed from optimum codes over CF(q) which achieve the Plotkin bound. Finally, several classes of optimum constant-weight codes over GF(q) are constructed.</p>","Computer Science, Information Systems"
"WOS:000071193500030","On the constructions of constant-weight codes","1998","
<p>Two methods of constructing binary constant-weight codes from 1) codes over GF(q) and 2) constant-weight codes over GF(q) are presented. Several classes of binary optimum constant-weight codes are derived from these methods. in general, we show that binary optimum constant-weight codes, which achieve the Johnson bound, can be constructed from optimum codes over CF(q) which achieve the Plotkin bound. Finally, several classes of optimum constant-weight codes over GF(q) are constructed.</p>","Computer Science"
"WOS:000071350600004","A-SATCHMORE: SATCHMORE with availability checking","1998","
<p>We present an improvement of SATCHMORE, called A-SATCHMORE, by incorporating availability checking into relevancy. Because some atoms unavailable to the further computation are also marked relevant, SATCHMORE suffers from a potential explosion of the search space. Addressing this weakness of SATCHMORE, we show that an atom does not need to be marked relevant unless it is available to the further computation and no non-Horn clause needs to be selected unless all its consequent atoms are marked availably relevant, i.e., unless it is totally availably relevant. In this way, A-SATCHMORE is able to further restrict the ues of non-Horn clauses (therefore to reduce the search space) and makes the proof more goal-oriented. Our theorem prover, A-SATCHMORE, can be simply implemented in PROLOG based on SATCHMORE. We discuss how to incorporate availability checking into relevancy, describe our improvement and present the implementation. We also prove that our theorem prover is sound and complete, and provide examples to show the power of our availability approach.</p>","Computer Science, Hardware & Architecture"
"WOS:000071350600004","A-SATCHMORE: SATCHMORE with availability checking","1998","
<p>We present an improvement of SATCHMORE, called A-SATCHMORE, by incorporating availability checking into relevancy. Because some atoms unavailable to the further computation are also marked relevant, SATCHMORE suffers from a potential explosion of the search space. Addressing this weakness of SATCHMORE, we show that an atom does not need to be marked relevant unless it is available to the further computation and no non-Horn clause needs to be selected unless all its consequent atoms are marked availably relevant, i.e., unless it is totally availably relevant. In this way, A-SATCHMORE is able to further restrict the ues of non-Horn clauses (therefore to reduce the search space) and makes the proof more goal-oriented. Our theorem prover, A-SATCHMORE, can be simply implemented in PROLOG based on SATCHMORE. We discuss how to incorporate availability checking into relevancy, describe our improvement and present the implementation. We also prove that our theorem prover is sound and complete, and provide examples to show the power of our availability approach.</p>","Computer Science, Theory & Methods"
"WOS:000071350600004","A-SATCHMORE: SATCHMORE with availability checking","1998","
<p>We present an improvement of SATCHMORE, called A-SATCHMORE, by incorporating availability checking into relevancy. Because some atoms unavailable to the further computation are also marked relevant, SATCHMORE suffers from a potential explosion of the search space. Addressing this weakness of SATCHMORE, we show that an atom does not need to be marked relevant unless it is available to the further computation and no non-Horn clause needs to be selected unless all its consequent atoms are marked availably relevant, i.e., unless it is totally availably relevant. In this way, A-SATCHMORE is able to further restrict the ues of non-Horn clauses (therefore to reduce the search space) and makes the proof more goal-oriented. Our theorem prover, A-SATCHMORE, can be simply implemented in PROLOG based on SATCHMORE. We discuss how to incorporate availability checking into relevancy, describe our improvement and present the implementation. We also prove that our theorem prover is sound and complete, and provide examples to show the power of our availability approach.</p>","Computer Science"
"WOS:000082523000019","Failure detection and consensus in the crash-recovery model","1998","
<p>We study the problems of failure detection and consensus in asynchronous systems in which processes may crash and recover, and links may lose messages. We first propose new failure detectors that are particularly suitable to the crash-recovery model. We next determine under what conditions stable storage is necessary to solve consensus in this model. Using the new failure detectors, we give two consensus algorithms that match these conditions: one requires stable storage and the other does not. Both algorithms tolerate link failures and are particularly efficient in the runs that are most likely in practice - those with no failures or failure detector mistakes. In such runs, consensus is achieved within 3 delta time and with 4n messages, where delta is the maximum message delay and n is the number of processes in the system.</p>","Computer Science, Information Systems"
"WOS:000082523000019","Failure detection and consensus in the crash-recovery model","1998","
<p>We study the problems of failure detection and consensus in asynchronous systems in which processes may crash and recover, and links may lose messages. We first propose new failure detectors that are particularly suitable to the crash-recovery model. We next determine under what conditions stable storage is necessary to solve consensus in this model. Using the new failure detectors, we give two consensus algorithms that match these conditions: one requires stable storage and the other does not. Both algorithms tolerate link failures and are particularly efficient in the runs that are most likely in practice - those with no failures or failure detector mistakes. In such runs, consensus is achieved within 3 delta time and with 4n messages, where delta is the maximum message delay and n is the number of processes in the system.</p>","Computer Science, Theory & Methods"
"WOS:000082523000019","Failure detection and consensus in the crash-recovery model","1998","
<p>We study the problems of failure detection and consensus in asynchronous systems in which processes may crash and recover, and links may lose messages. We first propose new failure detectors that are particularly suitable to the crash-recovery model. We next determine under what conditions stable storage is necessary to solve consensus in this model. Using the new failure detectors, we give two consensus algorithms that match these conditions: one requires stable storage and the other does not. Both algorithms tolerate link failures and are particularly efficient in the runs that are most likely in practice - those with no failures or failure detector mistakes. In such runs, consensus is achieved within 3 delta time and with 4n messages, where delta is the maximum message delay and n is the number of processes in the system.</p>","Computer Science"
"WOS:000076451200003","Neural network simulations of the primate oculomotor system III. An one-dimensional, one-directional model of the superior colliculus","1998","
<p>This report evaluates the performance of a biologically motivated neural network model of the primate superior colliculus (SC). Consistent with known anatomy and physiology, its major features include excitatory connections between its output elements, nigral gating mechanisms, and an eye displacement feedback of reticular origin to recalculate the metrics of saccades to memorized targets in retinotopic coordinates. Despite the fact that it makes no use of eye position or eye velocity information, the model can account for the accuracy of saccades in double step stimulation experiments. Further, the model accounts for the effects of focal SC lesions. Finally, it accounts for the properties of saccades evoked in response to the electrical stimulation of the SC. These include the approximate size constancy of evoked saccades despite increases of stimulus intensity, the fact that the size of evoked saccades depends on the time that has elapsed from a previous saccade, the fact that staircases of saccades are evoked in response to prolonged stimuli, and the fact that the size of saccades evoked in response to the simultaneous stimulation of two SC sites is the average of the saccades that are evoked when the two sites are separately stimulated.</p>","Computer Science, Cybernetics"
"WOS:000076451200003","Neural network simulations of the primate oculomotor system III. An one-dimensional, one-directional model of the superior colliculus","1998","
<p>This report evaluates the performance of a biologically motivated neural network model of the primate superior colliculus (SC). Consistent with known anatomy and physiology, its major features include excitatory connections between its output elements, nigral gating mechanisms, and an eye displacement feedback of reticular origin to recalculate the metrics of saccades to memorized targets in retinotopic coordinates. Despite the fact that it makes no use of eye position or eye velocity information, the model can account for the accuracy of saccades in double step stimulation experiments. Further, the model accounts for the effects of focal SC lesions. Finally, it accounts for the properties of saccades evoked in response to the electrical stimulation of the SC. These include the approximate size constancy of evoked saccades despite increases of stimulus intensity, the fact that the size of evoked saccades depends on the time that has elapsed from a previous saccade, the fact that staircases of saccades are evoked in response to prolonged stimuli, and the fact that the size of saccades evoked in response to the simultaneous stimulation of two SC sites is the average of the saccades that are evoked when the two sites are separately stimulated.</p>","Computer Science"
"WOS:000071113100014","Optimal nonnegative color scanning filters","1998","
<p>In this correspondence, the problem of designing color scanning filters for multi-illuminant color recording is considered. The filter transmittances are determined from a minimum-mean-squared orthogonal tristimulus error criterion that minimizes the color error in estimates obtained from noisy recorded data. Nonnegativity constraints essential for physical realizability are imposed on the filter transmittances. In order to demonstrate the significant improvements obtained, the resulting filters are compared with suboptimal filters reported in earlier literature.</p>","Computer Science, Artificial Intelligence"
"WOS:000071113100014","Optimal nonnegative color scanning filters","1998","
<p>In this correspondence, the problem of designing color scanning filters for multi-illuminant color recording is considered. The filter transmittances are determined from a minimum-mean-squared orthogonal tristimulus error criterion that minimizes the color error in estimates obtained from noisy recorded data. Nonnegativity constraints essential for physical realizability are imposed on the filter transmittances. In order to demonstrate the significant improvements obtained, the resulting filters are compared with suboptimal filters reported in earlier literature.</p>","Computer Science"
"WOS:000171768600163","Knowledge discovery and data mining to assist natural language understanding","1998","
<p>As natural language processing systems become more frequent in clinical use, methods for interpreting the output of these programs become increasingly important. These methods require the effort of a domain expert, who must build specific queries and rules for interpreting the processor output. Knowledge discovery and data mining tools can be used instead of a domain expert to automatically generate these queries and rules. C5.0, a decision tree generator, was used to create a rule base for a natural language understanding system. A general-purpose natural language processor using this rule base was tested on a set of 200 chest radiograph reports. When a small set of reports, classified by physicians, was used as the training set, the generated rule base performed as well as lay persons, hut worse than physicians. When a larger set of reports, using ICD9 coding to classify the set, was used for training the system, the rule base performed worse than the physicians and lay persons. It appears that a larger, more accurate training set is needed to increase performance of the method.</p>","Computer Science, Information Systems"
"WOS:000171768600163","Knowledge discovery and data mining to assist natural language understanding","1998","
<p>As natural language processing systems become more frequent in clinical use, methods for interpreting the output of these programs become increasingly important. These methods require the effort of a domain expert, who must build specific queries and rules for interpreting the processor output. Knowledge discovery and data mining tools can be used instead of a domain expert to automatically generate these queries and rules. C5.0, a decision tree generator, was used to create a rule base for a natural language understanding system. A general-purpose natural language processor using this rule base was tested on a set of 200 chest radiograph reports. When a small set of reports, classified by physicians, was used as the training set, the generated rule base performed as well as lay persons, hut worse than physicians. When a larger set of reports, using ICD9 coding to classify the set, was used for training the system, the rule base performed worse than the physicians and lay persons. It appears that a larger, more accurate training set is needed to increase performance of the method.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000171768600163","Knowledge discovery and data mining to assist natural language understanding","1998","
<p>As natural language processing systems become more frequent in clinical use, methods for interpreting the output of these programs become increasingly important. These methods require the effort of a domain expert, who must build specific queries and rules for interpreting the processor output. Knowledge discovery and data mining tools can be used instead of a domain expert to automatically generate these queries and rules. C5.0, a decision tree generator, was used to create a rule base for a natural language understanding system. A general-purpose natural language processor using this rule base was tested on a set of 200 chest radiograph reports. When a small set of reports, classified by physicians, was used as the training set, the generated rule base performed as well as lay persons, hut worse than physicians. When a larger set of reports, using ICD9 coding to classify the set, was used for training the system, the rule base performed worse than the physicians and lay persons. It appears that a larger, more accurate training set is needed to increase performance of the method.</p>","Computer Science"
"WOS:000075007100012","Adoption of security and confidentiality features in an operational community health information network: the Comox Valley experience - case example","1998","
<p>Since 1993, a budding community health information network (CHIN) has been in operation in the Comox Valley in Canada. A general hospital and three multi-doctor clinics are linked electronically. The clinics operate without paper charts using a comprehensive clinic information system. The link is provided by RSALink, a commercial message exchange service, based on Health Link, a system developed at the University of Victoria (McDaniel et al., Can. Med. Inform. 1 (1994) 40-41; McDaniel, Dissertation, University of Victoria, Canada, 1994). Health Link is a highly adaptable message exchange service with rich functionality. Despite this, the system is used exclusively to receive laboratory results transmitted by the hospital's laboratory system (RSAStat(1)). The results are deposited in the patient data base of a commercial clinic information system (CliniCare(2)). This case is instructive because the users' selection of services available through Health Link allows us to observe the preferences in this informational sophisticated environment. Laboratory data transmission is appreciated as highly beneficial. The reliability, security and ample privacy protection and authentication features of Health Link, in contrast, are used in a black box mode and are not consciously exploited. This is consistent with our experience of the use of other systems which have operated for a substantial time, essentially without serious protection features. Our experience suggests that security and confidentiality features are exploited only to the extent that they do not require additional effort or conscientious intervention. This puts the system provider in the difficult position of either offering interactive systems that nobody will use, or providing automated features that nobody is aware of and that are therefore not used to full advantage--if at all. (C) 1998 Elsevier Science Ireland Ltd. All rights reserved.</p>","Computer Science, Information Systems"
"WOS:000075007100012","Adoption of security and confidentiality features in an operational community health information network: the Comox Valley experience - case example","1998","
<p>Since 1993, a budding community health information network (CHIN) has been in operation in the Comox Valley in Canada. A general hospital and three multi-doctor clinics are linked electronically. The clinics operate without paper charts using a comprehensive clinic information system. The link is provided by RSALink, a commercial message exchange service, based on Health Link, a system developed at the University of Victoria (McDaniel et al., Can. Med. Inform. 1 (1994) 40-41; McDaniel, Dissertation, University of Victoria, Canada, 1994). Health Link is a highly adaptable message exchange service with rich functionality. Despite this, the system is used exclusively to receive laboratory results transmitted by the hospital's laboratory system (RSAStat(1)). The results are deposited in the patient data base of a commercial clinic information system (CliniCare(2)). This case is instructive because the users' selection of services available through Health Link allows us to observe the preferences in this informational sophisticated environment. Laboratory data transmission is appreciated as highly beneficial. The reliability, security and ample privacy protection and authentication features of Health Link, in contrast, are used in a black box mode and are not consciously exploited. This is consistent with our experience of the use of other systems which have operated for a substantial time, essentially without serious protection features. Our experience suggests that security and confidentiality features are exploited only to the extent that they do not require additional effort or conscientious intervention. This puts the system provider in the difficult position of either offering interactive systems that nobody will use, or providing automated features that nobody is aware of and that are therefore not used to full advantage--if at all. (C) 1998 Elsevier Science Ireland Ltd. All rights reserved.</p>","Computer Science"
"WOS:000076039200021","A recursive algorithm based on the extended Kalman filter for the training of feedforward neural models","1998","
<p>The extended Kalman filter (EKF) is a well-known tool for the recursive parameter estimation of static and dynamic nonlinear models. In particular, the EKF has been applied to the estimation of the weights of feedforward and recurrent neural network models, i.e. to their training, and shown to be more efficient than recursive and nonrecursive first-order training algorithms; nevertheless, these first applications to the training of neural networks did not fully exploit the potentials of the EKF. In this paper, we analyze the specific influence of the EKF parameters for modeling problems, and propose a variant of this algorithm for the training of feedforward neural models which proves to be very efficient as compared to nonrecursive second-order algorithms. We test the proposed EKF algorithm on several static and dynamic modeling problems, some of them being benchmark problems, and which bring out the properties of the proposed algorithm. (C) 1998 Published by Elsevier Science B.V. All rights reserved.</p>","Computer Science, Artificial Intelligence"
"WOS:000076039200021","A recursive algorithm based on the extended Kalman filter for the training of feedforward neural models","1998","
<p>The extended Kalman filter (EKF) is a well-known tool for the recursive parameter estimation of static and dynamic nonlinear models. In particular, the EKF has been applied to the estimation of the weights of feedforward and recurrent neural network models, i.e. to their training, and shown to be more efficient than recursive and nonrecursive first-order training algorithms; nevertheless, these first applications to the training of neural networks did not fully exploit the potentials of the EKF. In this paper, we analyze the specific influence of the EKF parameters for modeling problems, and propose a variant of this algorithm for the training of feedforward neural models which proves to be very efficient as compared to nonrecursive second-order algorithms. We test the proposed EKF algorithm on several static and dynamic modeling problems, some of them being benchmark problems, and which bring out the properties of the proposed algorithm. (C) 1998 Published by Elsevier Science B.V. All rights reserved.</p>","Computer Science"
"WOS:000077272800004","Space-time properties of a storage process","1998","
<p>In a recent paper, Stadje analyzed the space-time properties of some storage processes. We give a short probabilistic proof of these results.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000077272800004","Space-time properties of a storage process","1998","
<p>In a recent paper, Stadje analyzed the space-time properties of some storage processes. We give a short probabilistic proof of these results.</p>","Computer Science"
"WOS:000074681300005","Finite element analysis of two- and three-dimensional flows around square columns in tandem arrangement","1998","
<p>Two- (2D) and three-dimensional (3D) finite element analyses for flow around two square columns in tandem arrangement were performed with various column spacings and Reynolds numbers. The computed values were compared with the wind-tunnel results in terms of the aerodynamic characteristics of the leeward column. In most 2D computations, strong vortices were formed behind the windward column, irrespective of widely changed Reynolds numbers. This was different from the experimental phenomena of equivalent spacing, so that the computed time-averaged pressure coefficients were not identical to the experimental values except when the distance between the two columns was adequately wide or narrow. On the other hand, in 3D computation, distinct differences in flow structures behind the column were observed between Reynolds numbers of 10(3) and 10(4) and the pressure coefficient in the 3D analysis with Re = 10(4) agreed well with the experimental value. Thus, the effectiveness of 3D computations and Reynolds number effects on the flow around two square columns have been confirmed. (C) 1998 John Wiley & Sons, Ltd.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000074681300005","Finite element analysis of two- and three-dimensional flows around square columns in tandem arrangement","1998","
<p>Two- (2D) and three-dimensional (3D) finite element analyses for flow around two square columns in tandem arrangement were performed with various column spacings and Reynolds numbers. The computed values were compared with the wind-tunnel results in terms of the aerodynamic characteristics of the leeward column. In most 2D computations, strong vortices were formed behind the windward column, irrespective of widely changed Reynolds numbers. This was different from the experimental phenomena of equivalent spacing, so that the computed time-averaged pressure coefficients were not identical to the experimental values except when the distance between the two columns was adequately wide or narrow. On the other hand, in 3D computation, distinct differences in flow structures behind the column were observed between Reynolds numbers of 10(3) and 10(4) and the pressure coefficient in the 3D analysis with Re = 10(4) agreed well with the experimental value. Thus, the effectiveness of 3D computations and Reynolds number effects on the flow around two square columns have been confirmed. (C) 1998 John Wiley & Sons, Ltd.</p>","Computer Science"
"WOS:000076053900003","Rewrite orderings for higher-order terms in eta-long beta-normal form and the recursive path ordering","1998","
<p>This paper extends the termination proof techniques based on rewrite orderings to a higher-order setting, by defining a recursive path ordering for simply typed higher-order terms in long beta-normal form. This ordering is powerful enough to show termination of several complex examples. (C) 1998-Elsevier Science B.V. All rights reserved.</p>","Computer Science, Theory & Methods"
"WOS:000076053900003","Rewrite orderings for higher-order terms in eta-long beta-normal form and the recursive path ordering","1998","
<p>This paper extends the termination proof techniques based on rewrite orderings to a higher-order setting, by defining a recursive path ordering for simply typed higher-order terms in long beta-normal form. This ordering is powerful enough to show termination of several complex examples. (C) 1998-Elsevier Science B.V. All rights reserved.</p>","Computer Science"
"WOS:000076584600011","Application of computer animation techniques for presenting biomechanical research results","1998","
<p>Computer animation is becoming a widely accepted method for presenting results of biomechanical analyses and simulations. After summarizing the main aspects of three-dimensional and video overlaid animation, two applications of these techniques are presented. A parameterized graphical model of the tennis racket has been developed, which can be used for visualizing the simulated ball-racket impact phase. In the second example, high-speed video sequences of recorded drop jumps are overlaid with graphical elements to compare methods for determining the jumping height and related parameters. These graphical elements represent quantities, which are calculated applying the different methods. Differences can vividly be demonstrated. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000076584600011","Application of computer animation techniques for presenting biomechanical research results","1998","
<p>Computer animation is becoming a widely accepted method for presenting results of biomechanical analyses and simulations. After summarizing the main aspects of three-dimensional and video overlaid animation, two applications of these techniques are presented. A parameterized graphical model of the tennis racket has been developed, which can be used for visualizing the simulated ball-racket impact phase. In the second example, high-speed video sequences of recorded drop jumps are overlaid with graphical elements to compare methods for determining the jumping height and related parameters. These graphical elements represent quantities, which are calculated applying the different methods. Differences can vividly be demonstrated. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science"
"WOS:000075068500007","Boosting complete techniques thanks to local search methods","1998","
<p>In this paper, an efficient heuristic allowing one to localize inconsistent kernels in propositional knowledge-bases is described. Then, it is shown that local search techniques can boost the performance of logically complete methods for SAT. More precisely, local search techniques can be used to guide the branching strategy of logically complete techniques like Davis and Putnam's one, giving rise to significant performance improvements, in particular when addressing locally inconsistent problems. Moreover, this approach appears very competitive in the context of consistent SAT instances, too.</p>","Computer Science, Artificial Intelligence"
"WOS:000075068500007","Boosting complete techniques thanks to local search methods","1998","
<p>In this paper, an efficient heuristic allowing one to localize inconsistent kernels in propositional knowledge-bases is described. Then, it is shown that local search techniques can boost the performance of logically complete methods for SAT. More precisely, local search techniques can be used to guide the branching strategy of logically complete techniques like Davis and Putnam's one, giving rise to significant performance improvements, in particular when addressing locally inconsistent problems. Moreover, this approach appears very competitive in the context of consistent SAT instances, too.</p>","Computer Science"
"WOS:000076676800011","Inheritance comes of age: applying nonmonotonic techniques to problems in industry","1998","
<p>Nonmonotonic reasoning is virtually absent from industry and has been so since its inception; the result is that the field is becoming increasingly marginalized within Al. We argue that this is largely because researchers in the area focus exclusively on commonsense problems which are irrelevant to industry and because few efficient algorithms or tools have been developed. A sensible strategy is thus to focus on industry problems and to develop solutions within tractable subtheories of nonmonotonic logic.</p>
<p>We examine an example of nonmonotonic reasoning in industry-inheritance of business rules in the medical insurance domain - and show how the paradigm of inheritance with exceptions can be extended to a broader and more powerful kind of nonmonotonic reasoning. This is done by introducing formula-augmented semantic networks (FANs), semantic networks which attach well-formed formulae to nodes. The problem of inheriting well-formed formulae within this structure is explored, and an algorithm is given and discussed. Finally we discuss the underlying lessons that can be generalized to other industry problems. (C) 1998 Published by Elsevier Science B.V. All rights reserved.</p>","Computer Science, Artificial Intelligence"
"WOS:000076676800011","Inheritance comes of age: applying nonmonotonic techniques to problems in industry","1998","
<p>Nonmonotonic reasoning is virtually absent from industry and has been so since its inception; the result is that the field is becoming increasingly marginalized within Al. We argue that this is largely because researchers in the area focus exclusively on commonsense problems which are irrelevant to industry and because few efficient algorithms or tools have been developed. A sensible strategy is thus to focus on industry problems and to develop solutions within tractable subtheories of nonmonotonic logic.</p>
<p>We examine an example of nonmonotonic reasoning in industry-inheritance of business rules in the medical insurance domain - and show how the paradigm of inheritance with exceptions can be extended to a broader and more powerful kind of nonmonotonic reasoning. This is done by introducing formula-augmented semantic networks (FANs), semantic networks which attach well-formed formulae to nodes. The problem of inheriting well-formed formulae within this structure is explored, and an algorithm is given and discussed. Finally we discuss the underlying lessons that can be generalized to other industry problems. (C) 1998 Published by Elsevier Science B.V. All rights reserved.</p>","Computer Science"
"WOS:000075055300003","A comparison of algorithms used to compute hill slope as a property of the DEM","1998","
<p>The calculation of hill slope in the form of downhill gradient and aspect for a point in a digital elevation model (DEM), is a popular procedure in the hydrological, environmental and remote sensing. The most commonly used slope calculation algorithms employed on DEM topography data make use of a three by three search window, or kernel, centred on the grid point (grid cell) in question in order to calculate the gradient and aspect at that point. A comparison of eight frequently used slope calculation algorithms for digital elevation matrices has been carried out using both synthetic and real data as test surfaces. Morrison's surface III, a trigonometrically defined surface, was used as the synthetic test surface. This was differentiated analytically to give true gradient and aspect values against which to compare the results of the tested algorithms. The results of the best-performing slope algorithm on Morrison's surface were then used as the reference against which to compare the other tested algorithms on a real DEM. For both of the lest surfaces residual gradient and aspect grids were calculated by subtracting the gradient and aspect grids produced by the algorithms on test from the true/ reference gradient and aspect grids. The resulting residual gradient and aspect grids were used to calculate root-mean-square (RMS) residual error estimates that were used to rank the slope algorithms from ""best"" (lowest value of RMS residual error) to ""worst"" (largest value of RMS residual error). For Morrison's test surface, Fleming and Hoffer's method gave the ""best"" results for both gradient and aspect. Horn's method (used in ArcInfo GRID) also performed well for both gradient and aspect estimation. However, the popular maximum downward gradient method (MDG) performed poorly, coming last in the rankings. A similar pattern was seen in the gradient and aspect rankings derived using the Rhum DEM, with Horn's method performing well and the MDG method poorly. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000075055300003","A comparison of algorithms used to compute hill slope as a property of the DEM","1998","
<p>The calculation of hill slope in the form of downhill gradient and aspect for a point in a digital elevation model (DEM), is a popular procedure in the hydrological, environmental and remote sensing. The most commonly used slope calculation algorithms employed on DEM topography data make use of a three by three search window, or kernel, centred on the grid point (grid cell) in question in order to calculate the gradient and aspect at that point. A comparison of eight frequently used slope calculation algorithms for digital elevation matrices has been carried out using both synthetic and real data as test surfaces. Morrison's surface III, a trigonometrically defined surface, was used as the synthetic test surface. This was differentiated analytically to give true gradient and aspect values against which to compare the results of the tested algorithms. The results of the best-performing slope algorithm on Morrison's surface were then used as the reference against which to compare the other tested algorithms on a real DEM. For both of the lest surfaces residual gradient and aspect grids were calculated by subtracting the gradient and aspect grids produced by the algorithms on test from the true/ reference gradient and aspect grids. The resulting residual gradient and aspect grids were used to calculate root-mean-square (RMS) residual error estimates that were used to rank the slope algorithms from ""best"" (lowest value of RMS residual error) to ""worst"" (largest value of RMS residual error). For Morrison's test surface, Fleming and Hoffer's method gave the ""best"" results for both gradient and aspect. Horn's method (used in ArcInfo GRID) also performed well for both gradient and aspect estimation. However, the popular maximum downward gradient method (MDG) performed poorly, coming last in the rankings. A similar pattern was seen in the gradient and aspect rankings derived using the Rhum DEM, with Horn's method performing well and the MDG method poorly. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science"
"WOS:000071479900006","Fitting mixtures of exponentials to long-tail distributions to analyze network performance models","1998","
<p>Traffic measurements from communication networks have shown that many quantities charecterizing network performance have long-tail probability distributions, i.e., with tails that decay more slowly than exponentially. File lengths, call holding times, scene lengths in MPEG video streams, and intervals between connection requests in Internet traffic all have been found to have long tail distributions, being well described by distributions such as the Pareto and Weibull. It is known that long-tail distributions can have a dramatic effect upon performance, e.g., long-tail service-time distributions cause long-tail waiting-time distributions in queues, but it is often difficult to describe this effect in detail, because performance models with component long-tail distributions tend to be difficult to analyze. We address this problem by developing an algorithm for approximating a long-tail distribution by a hyperexponential distribution (a finite mixture of exponentials). We first prove that. in prinicple, it is possible to approximate distributions from a large class, including the Pareto and Weibull distributions, arbitrarily closely by hyperexponential distributions. Then we develop a specific fitting alogrithm. Our fitting algorithm is recursive over time scales, starting with the largest time scale. At each stage, an exponential component is fit in the largest remaining time scale and then the fitted exponential component is subtracted from the distribution. Even though a mixture of exponentials has an exponential tail, it can match a long-tail distribution in the regions of primary interest when there an enough exponential components. When a good fit is achieved, the approximating hyperexponential distribution inherits many of the difficulties of the original long-tail distribution; e.g., it is still difficult to obtain reliable estimates from simulation experiments. However, some difficulties are avoided; e.g., it is possible to solve some queueing models that could not be solved before. We give examples showing that the fitting procedure is effective, both for directly matching a long-tail distribution and for predicting the performance in a queueing model with a long-tail service-time distribution. (C) 1998 Elsevier Science B.V.</p>","Computer Science, Hardware & Architecture"
"WOS:000071479900006","Fitting mixtures of exponentials to long-tail distributions to analyze network performance models","1998","
<p>Traffic measurements from communication networks have shown that many quantities charecterizing network performance have long-tail probability distributions, i.e., with tails that decay more slowly than exponentially. File lengths, call holding times, scene lengths in MPEG video streams, and intervals between connection requests in Internet traffic all have been found to have long tail distributions, being well described by distributions such as the Pareto and Weibull. It is known that long-tail distributions can have a dramatic effect upon performance, e.g., long-tail service-time distributions cause long-tail waiting-time distributions in queues, but it is often difficult to describe this effect in detail, because performance models with component long-tail distributions tend to be difficult to analyze. We address this problem by developing an algorithm for approximating a long-tail distribution by a hyperexponential distribution (a finite mixture of exponentials). We first prove that. in prinicple, it is possible to approximate distributions from a large class, including the Pareto and Weibull distributions, arbitrarily closely by hyperexponential distributions. Then we develop a specific fitting alogrithm. Our fitting algorithm is recursive over time scales, starting with the largest time scale. At each stage, an exponential component is fit in the largest remaining time scale and then the fitted exponential component is subtracted from the distribution. Even though a mixture of exponentials has an exponential tail, it can match a long-tail distribution in the regions of primary interest when there an enough exponential components. When a good fit is achieved, the approximating hyperexponential distribution inherits many of the difficulties of the original long-tail distribution; e.g., it is still difficult to obtain reliable estimates from simulation experiments. However, some difficulties are avoided; e.g., it is possible to solve some queueing models that could not be solved before. We give examples showing that the fitting procedure is effective, both for directly matching a long-tail distribution and for predicting the performance in a queueing model with a long-tail service-time distribution. (C) 1998 Elsevier Science B.V.</p>","Computer Science, Theory & Methods"
"WOS:000071479900006","Fitting mixtures of exponentials to long-tail distributions to analyze network performance models","1998","
<p>Traffic measurements from communication networks have shown that many quantities charecterizing network performance have long-tail probability distributions, i.e., with tails that decay more slowly than exponentially. File lengths, call holding times, scene lengths in MPEG video streams, and intervals between connection requests in Internet traffic all have been found to have long tail distributions, being well described by distributions such as the Pareto and Weibull. It is known that long-tail distributions can have a dramatic effect upon performance, e.g., long-tail service-time distributions cause long-tail waiting-time distributions in queues, but it is often difficult to describe this effect in detail, because performance models with component long-tail distributions tend to be difficult to analyze. We address this problem by developing an algorithm for approximating a long-tail distribution by a hyperexponential distribution (a finite mixture of exponentials). We first prove that. in prinicple, it is possible to approximate distributions from a large class, including the Pareto and Weibull distributions, arbitrarily closely by hyperexponential distributions. Then we develop a specific fitting alogrithm. Our fitting algorithm is recursive over time scales, starting with the largest time scale. At each stage, an exponential component is fit in the largest remaining time scale and then the fitted exponential component is subtracted from the distribution. Even though a mixture of exponentials has an exponential tail, it can match a long-tail distribution in the regions of primary interest when there an enough exponential components. When a good fit is achieved, the approximating hyperexponential distribution inherits many of the difficulties of the original long-tail distribution; e.g., it is still difficult to obtain reliable estimates from simulation experiments. However, some difficulties are avoided; e.g., it is possible to solve some queueing models that could not be solved before. We give examples showing that the fitting procedure is effective, both for directly matching a long-tail distribution and for predicting the performance in a queueing model with a long-tail service-time distribution. (C) 1998 Elsevier Science B.V.</p>","Computer Science"
"WOS:000075449700007","An analysis of lock-based and optimistic concurrency control protocols in multiprocessor real-time databases","1998","
<p>Previous studies, e.g., Haritsa et al. (Haritsa, J.R., Livny, M., Carey, M., 1990. Proceedings of Ninth ACM Symposium on Principles of Database systems) have shown that optimistic concurrency control (OCC) generally performs better than lock-based protocols in disk-based real-time database systems (RTDBS). In this paper we compare the two concurrency control protocols in both disk-based and memory-resident multiprocessor RTDBS. Based on simulation, we analyze the intrinsic behaviors of the two protocols. The result of our performance evaluation experiments show that different characteristics of the two environments indeed have great impact on the protocols' performance. We identify such system characteristics and expose the weaknesses of traditional OCC and lock-based protocols. To improve performance, a new protocol, called Two Phase Locking-Lock Write All (2PL-LW), is proposed. We show that 2PL-LW performs better than the traditional protocols in meeting transaction deadlines in both disk-based and memory-resident RTDBS. (C) 1998 Elsevier Science Inc. All rights reserved.</p>","Computer Science, Software Engineering"
"WOS:000075449700007","An analysis of lock-based and optimistic concurrency control protocols in multiprocessor real-time databases","1998","
<p>Previous studies, e.g., Haritsa et al. (Haritsa, J.R., Livny, M., Carey, M., 1990. Proceedings of Ninth ACM Symposium on Principles of Database systems) have shown that optimistic concurrency control (OCC) generally performs better than lock-based protocols in disk-based real-time database systems (RTDBS). In this paper we compare the two concurrency control protocols in both disk-based and memory-resident multiprocessor RTDBS. Based on simulation, we analyze the intrinsic behaviors of the two protocols. The result of our performance evaluation experiments show that different characteristics of the two environments indeed have great impact on the protocols' performance. We identify such system characteristics and expose the weaknesses of traditional OCC and lock-based protocols. To improve performance, a new protocol, called Two Phase Locking-Lock Write All (2PL-LW), is proposed. We show that 2PL-LW performs better than the traditional protocols in meeting transaction deadlines in both disk-based and memory-resident RTDBS. (C) 1998 Elsevier Science Inc. All rights reserved.</p>","Computer Science, Theory & Methods"
"WOS:000075449700007","An analysis of lock-based and optimistic concurrency control protocols in multiprocessor real-time databases","1998","
<p>Previous studies, e.g., Haritsa et al. (Haritsa, J.R., Livny, M., Carey, M., 1990. Proceedings of Ninth ACM Symposium on Principles of Database systems) have shown that optimistic concurrency control (OCC) generally performs better than lock-based protocols in disk-based real-time database systems (RTDBS). In this paper we compare the two concurrency control protocols in both disk-based and memory-resident multiprocessor RTDBS. Based on simulation, we analyze the intrinsic behaviors of the two protocols. The result of our performance evaluation experiments show that different characteristics of the two environments indeed have great impact on the protocols' performance. We identify such system characteristics and expose the weaknesses of traditional OCC and lock-based protocols. To improve performance, a new protocol, called Two Phase Locking-Lock Write All (2PL-LW), is proposed. We show that 2PL-LW performs better than the traditional protocols in meeting transaction deadlines in both disk-based and memory-resident RTDBS. (C) 1998 Elsevier Science Inc. All rights reserved.</p>","Computer Science"
"WOS:000077805200003","Pattern classification using genetic algorithms: Determination of H","1998","
<p>A methodology based on the concept of a variable string length GA (VGA) is developed for determining automatically the number of hyperplanes for modeling the class boundaries in a GA-classifier. The genetic operators and fitness function are defined to take care of the variability in chromosome length. It is proved that the method is able to arrive at the optimal number of misclassifications after a sufficiently large number of iterations, and will need a minimal number of hyperplanes for this purpose. Experimental results on different artificial and real life data sets demonstrate that the classifier. using the concept of a variable length chromosome, can automatically determine an appropriate value of the number of hyperplanes, and also provide performance better than that of the fixed length version. Its comparison with another approach using a VGA is provided. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science, Artificial Intelligence"
"WOS:000077805200003","Pattern classification using genetic algorithms: Determination of H","1998","
<p>A methodology based on the concept of a variable string length GA (VGA) is developed for determining automatically the number of hyperplanes for modeling the class boundaries in a GA-classifier. The genetic operators and fitness function are defined to take care of the variability in chromosome length. It is proved that the method is able to arrive at the optimal number of misclassifications after a sufficiently large number of iterations, and will need a minimal number of hyperplanes for this purpose. Experimental results on different artificial and real life data sets demonstrate that the classifier. using the concept of a variable length chromosome, can automatically determine an appropriate value of the number of hyperplanes, and also provide performance better than that of the fixed length version. Its comparison with another approach using a VGA is provided. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science"
"WOS:000072211400017","New finite-element-based fluctuation-splitting kinetic schemes","1998","
<p>Starting from the gas kinetic model, a new class of schemes for hyperbolic systems of conservation laws is presented. The flow solvers are based on the Boltzmann equations. The numerical discretization is based on the upwind cell vertex fluctuation-splitting model. The method is truly multidimensional in the sense that the splitting is independent of a particular normal direction; the geometry of the mesh does not influence the upwinding. Numerical results for inviscid flow test cases are presented to indicate the robustness and accuracy of the schemes. (C) 1998 John Wiley & Sons, Ltd.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000072211400017","New finite-element-based fluctuation-splitting kinetic schemes","1998","
<p>Starting from the gas kinetic model, a new class of schemes for hyperbolic systems of conservation laws is presented. The flow solvers are based on the Boltzmann equations. The numerical discretization is based on the upwind cell vertex fluctuation-splitting model. The method is truly multidimensional in the sense that the splitting is independent of a particular normal direction; the geometry of the mesh does not influence the upwinding. Numerical results for inviscid flow test cases are presented to indicate the robustness and accuracy of the schemes. (C) 1998 John Wiley & Sons, Ltd.</p>","Computer Science"
"WOS:000082479300009","A cookbook for optimal interprocedural program optimization","1998","
<p>In this chapter we present the interprocedural counterpart of the intraprocedural cookbook for program optimization. To this end we summarize the presentation of the Chapters 5, 6, 7, and 8 for constructing an interprocedural program optimization from the designer's point of view. As in the intraprocedural setting, the point is to provide the designer of a program optimization with concise guidelines which structure and simplify the construction process, and simultaneously hide all details of the framework which are irrelevant for its application. Following the guidelines the construction and the corresponding optimality and precision proofs of the transformations and the IDFA-algorithms, respectively can be done like in the intraprocedural setting in a cookbook style.</p>","Computer Science, Theory & Methods"
"WOS:000082479300009","A cookbook for optimal interprocedural program optimization","1998","
<p>In this chapter we present the interprocedural counterpart of the intraprocedural cookbook for program optimization. To this end we summarize the presentation of the Chapters 5, 6, 7, and 8 for constructing an interprocedural program optimization from the designer's point of view. As in the intraprocedural setting, the point is to provide the designer of a program optimization with concise guidelines which structure and simplify the construction process, and simultaneously hide all details of the framework which are irrelevant for its application. Following the guidelines the construction and the corresponding optimality and precision proofs of the transformations and the IDFA-algorithms, respectively can be done like in the intraprocedural setting in a cookbook style.</p>","Computer Science"
"WOS:000075141300005","Module packing based on the BSG-structure and IC layout applications","1998","
<p>A new method of packing the rectangles is proposed with applications to integarted circuit (IC) layout design. A special work-sheet, called the bounded-sliceline grid, is introduced. It consists of special segments that dissect the plane into rooms to which binary relations ""right-of"" and ""above"" are associated such that any two rooms are uniquely in either relation. A packing is obtained through an assignment of the modules into the rooms followed by a compaction procedure. Changing the assignments by swapping the contents of two roams, a simulated annealing strategy is implemented to search for a good packing, Empirical results show that hundreds of rectangles are packed with a quite good quality in area efficiency, A wide adaptability is demonstrated specific to IC layout design. Ideas to handle a multilayer, nonrectangular chips with L-shaped modules are suggested.</p>","Computer Science, Hardware & Architecture"
"WOS:000075141300005","Module packing based on the BSG-structure and IC layout applications","1998","
<p>A new method of packing the rectangles is proposed with applications to integarted circuit (IC) layout design. A special work-sheet, called the bounded-sliceline grid, is introduced. It consists of special segments that dissect the plane into rooms to which binary relations ""right-of"" and ""above"" are associated such that any two rooms are uniquely in either relation. A packing is obtained through an assignment of the modules into the rooms followed by a compaction procedure. Changing the assignments by swapping the contents of two roams, a simulated annealing strategy is implemented to search for a good packing, Empirical results show that hundreds of rectangles are packed with a quite good quality in area efficiency, A wide adaptability is demonstrated specific to IC layout design. Ideas to handle a multilayer, nonrectangular chips with L-shaped modules are suggested.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000075141300005","Module packing based on the BSG-structure and IC layout applications","1998","
<p>A new method of packing the rectangles is proposed with applications to integarted circuit (IC) layout design. A special work-sheet, called the bounded-sliceline grid, is introduced. It consists of special segments that dissect the plane into rooms to which binary relations ""right-of"" and ""above"" are associated such that any two rooms are uniquely in either relation. A packing is obtained through an assignment of the modules into the rooms followed by a compaction procedure. Changing the assignments by swapping the contents of two roams, a simulated annealing strategy is implemented to search for a good packing, Empirical results show that hundreds of rectangles are packed with a quite good quality in area efficiency, A wide adaptability is demonstrated specific to IC layout design. Ideas to handle a multilayer, nonrectangular chips with L-shaped modules are suggested.</p>","Computer Science"
"WOS:000082286800009","Cerebellar control of robot arms","1998","
<p>Decades of research into the structure and function of the cerebellum have led to a clear understanding of many of its cells, as well as how learning takes place. Furthermore, there are many theories on what signals the cerebellum operates on, and how it works in concert with other parts of the nervous system. Nevertheless, the application of computational cerebella, models to the control of robot dynamics remains in its infant state. To date, a few applications have been realized, but limited to the control of traditional robot structures which, strictly speaking, do not require adaptive control for the tasks that are performed since their dynamic structures are relatively simple. The currently emerging family of light-weight robots (Hirzinger; G. (1996) In Proceedings of the 2nd International Conference on Advanced Robotics, Intelligent Automation, and Active Systems, Vienna, Austria) poses a new challenge to robot control: owing to their complex dynamics, traditional methods, depending on a full analysis of the dynamics of the system, are no longer applicable since the joints influence each other's dynamics during movement. Can artificial cerebellar models compete here? In this paper, we present a succinct introduction of the cerebellum, and discuss where it could be applied to tackle problems in robotics. without conclusively answering the above question, an overview of several applications of cerebellar models to robot control is given.</p>","Computer Science, Artificial Intelligence"
"WOS:000082286800009","Cerebellar control of robot arms","1998","
<p>Decades of research into the structure and function of the cerebellum have led to a clear understanding of many of its cells, as well as how learning takes place. Furthermore, there are many theories on what signals the cerebellum operates on, and how it works in concert with other parts of the nervous system. Nevertheless, the application of computational cerebella, models to the control of robot dynamics remains in its infant state. To date, a few applications have been realized, but limited to the control of traditional robot structures which, strictly speaking, do not require adaptive control for the tasks that are performed since their dynamic structures are relatively simple. The currently emerging family of light-weight robots (Hirzinger; G. (1996) In Proceedings of the 2nd International Conference on Advanced Robotics, Intelligent Automation, and Active Systems, Vienna, Austria) poses a new challenge to robot control: owing to their complex dynamics, traditional methods, depending on a full analysis of the dynamics of the system, are no longer applicable since the joints influence each other's dynamics during movement. Can artificial cerebellar models compete here? In this paper, we present a succinct introduction of the cerebellum, and discuss where it could be applied to tackle problems in robotics. without conclusively answering the above question, an overview of several applications of cerebellar models to robot control is given.</p>","Computer Science, Theory & Methods"
"WOS:000082286800009","Cerebellar control of robot arms","1998","
<p>Decades of research into the structure and function of the cerebellum have led to a clear understanding of many of its cells, as well as how learning takes place. Furthermore, there are many theories on what signals the cerebellum operates on, and how it works in concert with other parts of the nervous system. Nevertheless, the application of computational cerebella, models to the control of robot dynamics remains in its infant state. To date, a few applications have been realized, but limited to the control of traditional robot structures which, strictly speaking, do not require adaptive control for the tasks that are performed since their dynamic structures are relatively simple. The currently emerging family of light-weight robots (Hirzinger; G. (1996) In Proceedings of the 2nd International Conference on Advanced Robotics, Intelligent Automation, and Active Systems, Vienna, Austria) poses a new challenge to robot control: owing to their complex dynamics, traditional methods, depending on a full analysis of the dynamics of the system, are no longer applicable since the joints influence each other's dynamics during movement. Can artificial cerebellar models compete here? In this paper, we present a succinct introduction of the cerebellum, and discuss where it could be applied to tackle problems in robotics. without conclusively answering the above question, an overview of several applications of cerebellar models to robot control is given.</p>","Computer Science"
"WOS:000075954500007","Discrete transparent boundary conditions for wide angle parabolic equations in underwater acoustics","1998","
<p>This paper is concerned with transparent boundary conditions (TBCs) for wide angle ""parabolic"" equations (WAPEs) in the application to underwater acoustics (assuming cylindrical symmetry). Existing discretizations of these TBCs introduce slight numerical reflections at this artificial boundary and also render the overall Crank-Nicolson finite difference method only conditionally stable. Here, a novel discrete TBC is derived from the fully discretized whole-space problem that is reflection-free and yields an unconditionally stable scheme. While we shall assume a uniform discretization in range, the interior depth discretization (i.e. in the water column) may be nonuniform, and we shall discuss strategies for the ""best exterior discretization"" (i.e. in the sea bottom). The superiority of the new discrete TBC over existing discretizations is illustrated on several benchmark problems. In the literature different WAPEs (or WAPE and the standard ""parabolic"" equation) have been coupled in the water and the sea bottom. We analyze under which conditions this yields a hybrid model that is conservative for the acoustic held. (C) 1998 Academic Press.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000075954500007","Discrete transparent boundary conditions for wide angle parabolic equations in underwater acoustics","1998","
<p>This paper is concerned with transparent boundary conditions (TBCs) for wide angle ""parabolic"" equations (WAPEs) in the application to underwater acoustics (assuming cylindrical symmetry). Existing discretizations of these TBCs introduce slight numerical reflections at this artificial boundary and also render the overall Crank-Nicolson finite difference method only conditionally stable. Here, a novel discrete TBC is derived from the fully discretized whole-space problem that is reflection-free and yields an unconditionally stable scheme. While we shall assume a uniform discretization in range, the interior depth discretization (i.e. in the water column) may be nonuniform, and we shall discuss strategies for the ""best exterior discretization"" (i.e. in the sea bottom). The superiority of the new discrete TBC over existing discretizations is illustrated on several benchmark problems. In the literature different WAPEs (or WAPE and the standard ""parabolic"" equation) have been coupled in the water and the sea bottom. We analyze under which conditions this yields a hybrid model that is conservative for the acoustic held. (C) 1998 Academic Press.</p>","Computer Science"
"WOS:000073237800005","History and technology of computer fonts","1998","
<p>Computer fonts have become widely popularized through the use of the Macintosh and Microsoft Windows software systems. This technology has been evolving for more than 30 years and has a number of underlying technologies that support the current high level of utility and quality.</p>","Computer Science, Theory & Methods"
"WOS:000073237800005","History and technology of computer fonts","1998","
<p>Computer fonts have become widely popularized through the use of the Macintosh and Microsoft Windows software systems. This technology has been evolving for more than 30 years and has a number of underlying technologies that support the current high level of utility and quality.</p>","Computer Science"
"WOS:000074405900008","SplitsTree: analyzing and visualizing evolutionary data","1998","
<p>Motivation: Real evolutionary data often contain a number of different and sometimes conflicting phylogenetic signals, and thus do not always clearly support a unique tree. To address this problem, Bandelt and Dress (Adv. Math., 92, 47-05, 1992) developed the method of split decomposition. For ideal data, this method gives rise to a tree, whereas less ideal data are represented by a tree-like network that may indicate evidence for different and conflicting phylogenies.</p>
<p>Results: SplitsTree is an interactive program, for analyzing and visualizing evolutionary data, that implements this approach. It also supports a number of distances transformations, the computation of parsimony splits, spectral analysis and bootstrapping.</p>
<p>Availability: There are two versions of SplitsTree: an interactive Macintosh version (shareware) and a command-line Unix version (public domain). Both are available from: ftp://ftp.uni-bielefeld.de/pub/math/splits/splitstree2. There is a WWW version running at: http://www.bibiserv.techfak.unibielefeld.de/splits.</p>
<p>Contact: huson@mathematik.uni-bielefeld.de.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000074405900008","SplitsTree: analyzing and visualizing evolutionary data","1998","
<p>Motivation: Real evolutionary data often contain a number of different and sometimes conflicting phylogenetic signals, and thus do not always clearly support a unique tree. To address this problem, Bandelt and Dress (Adv. Math., 92, 47-05, 1992) developed the method of split decomposition. For ideal data, this method gives rise to a tree, whereas less ideal data are represented by a tree-like network that may indicate evidence for different and conflicting phylogenies.</p>
<p>Results: SplitsTree is an interactive program, for analyzing and visualizing evolutionary data, that implements this approach. It also supports a number of distances transformations, the computation of parsimony splits, spectral analysis and bootstrapping.</p>
<p>Availability: There are two versions of SplitsTree: an interactive Macintosh version (shareware) and a command-line Unix version (public domain). Both are available from: ftp://ftp.uni-bielefeld.de/pub/math/splits/splitstree2. There is a WWW version running at: http://www.bibiserv.techfak.unibielefeld.de/splits.</p>
<p>Contact: huson@mathematik.uni-bielefeld.de.</p>","Computer Science"
"WOS:000084730000005","On Encoding p pi in m pi","1998","
<p>This paper is about the encoding of p pi, the polyadic pi-calculus, in m pi, the monadic pi-calculus. A type system for m pi processes is introduced which captures the interaction regime underlying the encoding of p pi processes respecting a sorting. A full-abstraction result is shown: two p pi processes are typed barbed congruent iff their m pi encodings are monadic-typed barbed congruent.</p>","Computer Science, Software Engineering"
"WOS:000084730000005","On Encoding p pi in m pi","1998","
<p>This paper is about the encoding of p pi, the polyadic pi-calculus, in m pi, the monadic pi-calculus. A type system for m pi processes is introduced which captures the interaction regime underlying the encoding of p pi processes respecting a sorting. A full-abstraction result is shown: two p pi processes are typed barbed congruent iff their m pi encodings are monadic-typed barbed congruent.</p>","Computer Science, Theory & Methods"
"WOS:000084730000005","On Encoding p pi in m pi","1998","
<p>This paper is about the encoding of p pi, the polyadic pi-calculus, in m pi, the monadic pi-calculus. A type system for m pi processes is introduced which captures the interaction regime underlying the encoding of p pi processes respecting a sorting. A full-abstraction result is shown: two p pi processes are typed barbed congruent iff their m pi encodings are monadic-typed barbed congruent.</p>","Computer Science"
"WOS:000074882200005","Calculation of effective diffusivities and reactivities in immobilized cell systems using finite difference methods","1998","
<p>Immobilized cell systems typically consist of a single cell type encased in a semi-rigid polymer support. The rates at which nutrient molecules diffuse and react in these materials determine the feasible longevity and the amount of desirable product generated by the cells. Finite difference techniques were developed to calculate effective diffusivities and rates of reaction of small molecules in such immobilized cell systems. The structures analyzed consist of multiple cellular inclusions distributed in a continuous phase where molecules diffuse more slowly in the cells than in the continuous phase. Diffusivities are in excellent agreement with available theoretical bounds. Under typical reactive conditions, the depth to which oxygen can penetrate ranges from 24-200 mu m, depending on the cell volume fraction, oxygen supply, and cellular uptake kinetics. Increases in the cell fraction beyond 0.55 yield minimal increases in the oxygen consumption rate, suggesting that such materials are limited by the diffusive supply of oxygen. (C) 1998 Published by Elsevier Science Ltd. All rights reserved.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000074882200005","Calculation of effective diffusivities and reactivities in immobilized cell systems using finite difference methods","1998","
<p>Immobilized cell systems typically consist of a single cell type encased in a semi-rigid polymer support. The rates at which nutrient molecules diffuse and react in these materials determine the feasible longevity and the amount of desirable product generated by the cells. Finite difference techniques were developed to calculate effective diffusivities and rates of reaction of small molecules in such immobilized cell systems. The structures analyzed consist of multiple cellular inclusions distributed in a continuous phase where molecules diffuse more slowly in the cells than in the continuous phase. Diffusivities are in excellent agreement with available theoretical bounds. Under typical reactive conditions, the depth to which oxygen can penetrate ranges from 24-200 mu m, depending on the cell volume fraction, oxygen supply, and cellular uptake kinetics. Increases in the cell fraction beyond 0.55 yield minimal increases in the oxygen consumption rate, suggesting that such materials are limited by the diffusive supply of oxygen. (C) 1998 Published by Elsevier Science Ltd. All rights reserved.</p>","Computer Science"
"WOS:000071038700004","An integrated temporal data model incorporating time series concept","1998","
<p>Most temporal data models have concentrated on describing temporal data based on versioning of objects, tuples or attributes. The concept of time series, which is often needed in temporal applications, does not fit well within these models. The goal of this paper is to propose a generalized temporal database model that integrates the modeling of both version-based and time-series based temporal data into a single conceptual framework. The concept of calendar is also integrated into our proposed model. We also discuss how a conceptual Extended-ER design in our model can be mapped to an object-oriented or relational database implementation.</p>","Computer Science, Artificial Intelligence"
"WOS:000071038700004","An integrated temporal data model incorporating time series concept","1998","
<p>Most temporal data models have concentrated on describing temporal data based on versioning of objects, tuples or attributes. The concept of time series, which is often needed in temporal applications, does not fit well within these models. The goal of this paper is to propose a generalized temporal database model that integrates the modeling of both version-based and time-series based temporal data into a single conceptual framework. The concept of calendar is also integrated into our proposed model. We also discuss how a conceptual Extended-ER design in our model can be mapped to an object-oriented or relational database implementation.</p>","Computer Science, Information Systems"
"WOS:000071038700004","An integrated temporal data model incorporating time series concept","1998","
<p>Most temporal data models have concentrated on describing temporal data based on versioning of objects, tuples or attributes. The concept of time series, which is often needed in temporal applications, does not fit well within these models. The goal of this paper is to propose a generalized temporal database model that integrates the modeling of both version-based and time-series based temporal data into a single conceptual framework. The concept of calendar is also integrated into our proposed model. We also discuss how a conceptual Extended-ER design in our model can be mapped to an object-oriented or relational database implementation.</p>","Computer Science"
"WOS:000072774800007","A method of identifying influential data in fuzzy clustering","1998","
<p>In multivariate statistical methods, it is important to identify influential observations for a reasonable interpretation of the data structure, In this paper, we propose a method for identifying influential data in the fuzzy C-means (FCM) algorithm, To investigate such data, we consider a perturbation of the data points and evaluate the effect of a perturbation. As a perturbation, we consider two cases: one is the case in which the direction of a perturbation is specified and the other is the case in which the direction of a perturbation is not specified, By computing the change in the clustering result of FCM when given data points are slightly perturbed, we can look for data points that greatly affect the result, Also, we confirm an efficacy of the proposed method by numerical examples.</p>","Computer Science, Artificial Intelligence"
"WOS:000072774800007","A method of identifying influential data in fuzzy clustering","1998","
<p>In multivariate statistical methods, it is important to identify influential observations for a reasonable interpretation of the data structure, In this paper, we propose a method for identifying influential data in the fuzzy C-means (FCM) algorithm, To investigate such data, we consider a perturbation of the data points and evaluate the effect of a perturbation. As a perturbation, we consider two cases: one is the case in which the direction of a perturbation is specified and the other is the case in which the direction of a perturbation is not specified, By computing the change in the clustering result of FCM when given data points are slightly perturbed, we can look for data points that greatly affect the result, Also, we confirm an efficacy of the proposed method by numerical examples.</p>","Computer Science"
"WOS:000074779200002","Automatic generation of reprogrammable microcoded controllers within a high-level synthesis environment","1998","
<p>Existing techniques in high-level synthesis mostly assume a simple controller model in the form of a single FSM. In reality more complex controller architectures are often used. On the other hand, in the case of programmable processors the controller architecture is largely defined by the available control-flow instructions in the instruction set. With the wider acceptance of behavioural synthesis, the application of these methods for the design of programmable controllers is of fundamental importance in embedded system technology. An important extension of an existing architectural synthesis system targeting the generation of reprogrammable microcoded controllers is described. The designer can then generate both styles of architecture, hardwired and programmable, using the same synthesis system and can quickly evaluate the trade-offs of hardware decisions.</p>","Computer Science, Hardware & Architecture"
"WOS:000074779200002","Automatic generation of reprogrammable microcoded controllers within a high-level synthesis environment","1998","
<p>Existing techniques in high-level synthesis mostly assume a simple controller model in the form of a single FSM. In reality more complex controller architectures are often used. On the other hand, in the case of programmable processors the controller architecture is largely defined by the available control-flow instructions in the instruction set. With the wider acceptance of behavioural synthesis, the application of these methods for the design of programmable controllers is of fundamental importance in embedded system technology. An important extension of an existing architectural synthesis system targeting the generation of reprogrammable microcoded controllers is described. The designer can then generate both styles of architecture, hardwired and programmable, using the same synthesis system and can quickly evaluate the trade-offs of hardware decisions.</p>","Computer Science, Theory & Methods"
"WOS:000074779200002","Automatic generation of reprogrammable microcoded controllers within a high-level synthesis environment","1998","
<p>Existing techniques in high-level synthesis mostly assume a simple controller model in the form of a single FSM. In reality more complex controller architectures are often used. On the other hand, in the case of programmable processors the controller architecture is largely defined by the available control-flow instructions in the instruction set. With the wider acceptance of behavioural synthesis, the application of these methods for the design of programmable controllers is of fundamental importance in embedded system technology. An important extension of an existing architectural synthesis system targeting the generation of reprogrammable microcoded controllers is described. The designer can then generate both styles of architecture, hardwired and programmable, using the same synthesis system and can quickly evaluate the trade-offs of hardware decisions.</p>","Computer Science"
"WOS:000074612700011","The HAGIS self-consistent nonlinear wave-particle interaction model","1998","
<p>The problem of modelling the self-consistent interaction of an energetic particle ensemble with a wave spectrum specific to magnetically confined plasmas in a torus is discussed. Particle motion in a magnetic field coordinate system, whose surfaces are perturbed by a spectrum of finite amplitude magnetohydrodynamical (MHD) waves, is described using a Hamiltonian formulation. Employing the Sf method enables the simulation particles to only represent the change in the total particle distribution function and consequently possesses significant computational advantages over standard techniques. Changes to the particle distribution function subsequently affect the wave spectrum through wave-particle interactions. The model is validated using large aspect-ratio asymptotic limits as well as through a comparison with other numerical work. A consideration of the Kinetic Toroidal Alfven Eigenmode instability driven by fusion born alpha-particles in a D-T JET plasma illustrates a use of the code and demonstrates nonlinear saturation of the instability, together with the resultant redistribution of particles both in energy and across the plasma cross section. (C) 1998 Elsevier Science B.V.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000074612700011","The HAGIS self-consistent nonlinear wave-particle interaction model","1998","
<p>The problem of modelling the self-consistent interaction of an energetic particle ensemble with a wave spectrum specific to magnetically confined plasmas in a torus is discussed. Particle motion in a magnetic field coordinate system, whose surfaces are perturbed by a spectrum of finite amplitude magnetohydrodynamical (MHD) waves, is described using a Hamiltonian formulation. Employing the Sf method enables the simulation particles to only represent the change in the total particle distribution function and consequently possesses significant computational advantages over standard techniques. Changes to the particle distribution function subsequently affect the wave spectrum through wave-particle interactions. The model is validated using large aspect-ratio asymptotic limits as well as through a comparison with other numerical work. A consideration of the Kinetic Toroidal Alfven Eigenmode instability driven by fusion born alpha-particles in a D-T JET plasma illustrates a use of the code and demonstrates nonlinear saturation of the instability, together with the resultant redistribution of particles both in energy and across the plasma cross section. (C) 1998 Elsevier Science B.V.</p>","Computer Science"
"WOS:000076917200001","Diffusion-based learning theory for organizing visuo-motor coordination","1998","
<p>A diffusion-based learning theory is presented and applied to organize the visuomotor coordination of an eye-hand system which has redundant motion degree of freedom (dof). This theory considers the spatial optimality of the coordination: to minimize the end-effector position error of the eye-hand system as well as the differentiation of the joint angles with respect to the end-effector positions over all the bounded work space. By introducing variational methods with respect to the space, we derive a partial differential equation (PDE) of the joint angles with respect to the work space. The equation includes a diffusion term. For the given boundary conditions and the initial conditions, it can be solved uniquely, and the solution is a well organized map. From the motor learning point of view, our approach contains both the aspects of supervised learning as well as self-organization. Firstly, we assume that the forward relation from the hand system's joint angles to its end-effector positions can be obtained using supervised learning, and at the boundary of the work space, the supervisor can provide correct joint information. Then, by evolving the diffusion equation, we organize the visuomotor coordination. We show the effectiveness of this approach using a 3-dof scale manipulator. The problems of how to realize the visuomotor mdp; how to utilize the resultant map in several motions; and what are the influences of the initial conditions on the map formation and the relation to the boundary conditions are also discussed using computer simulations. Our approach has three advantages: (I) it does not require too many trial motions for the eye-hand system; (2) during the map formation process, it requires only the local interactions between each node; and (3) it guarantees the final map's spatial optimality over all the bounded work space.</p>","Computer Science, Cybernetics"
"WOS:000076917200001","Diffusion-based learning theory for organizing visuo-motor coordination","1998","
<p>A diffusion-based learning theory is presented and applied to organize the visuomotor coordination of an eye-hand system which has redundant motion degree of freedom (dof). This theory considers the spatial optimality of the coordination: to minimize the end-effector position error of the eye-hand system as well as the differentiation of the joint angles with respect to the end-effector positions over all the bounded work space. By introducing variational methods with respect to the space, we derive a partial differential equation (PDE) of the joint angles with respect to the work space. The equation includes a diffusion term. For the given boundary conditions and the initial conditions, it can be solved uniquely, and the solution is a well organized map. From the motor learning point of view, our approach contains both the aspects of supervised learning as well as self-organization. Firstly, we assume that the forward relation from the hand system's joint angles to its end-effector positions can be obtained using supervised learning, and at the boundary of the work space, the supervisor can provide correct joint information. Then, by evolving the diffusion equation, we organize the visuomotor coordination. We show the effectiveness of this approach using a 3-dof scale manipulator. The problems of how to realize the visuomotor mdp; how to utilize the resultant map in several motions; and what are the influences of the initial conditions on the map formation and the relation to the boundary conditions are also discussed using computer simulations. Our approach has three advantages: (I) it does not require too many trial motions for the eye-hand system; (2) during the map formation process, it requires only the local interactions between each node; and (3) it guarantees the final map's spatial optimality over all the bounded work space.</p>","Computer Science"
"WOS:000073141600005","An algorithmic version of the blow-up lemma","1998","
<p>Recently we developed a new method in graph theory based on the regularity lemma. The method is applied to find certain spanning subgraphs in dense graphs. The other main general tool of the method, besides the regularity lemma, is the so-called blow-up lemma (Komlos, Sarkozy, and Szemeredi [Combinatorica, 17, 109-123 (1997)]. This lemma helps to find bounded degree spanning subgraphs in epsilon-regular graphs. Our original proof of the lemma is not algorithmic, it applies probabilistic methods. in this paper we provide an algorithmic version of the blow-up lemma. The desired subgraph, for an n-vertex graph, can be found in time O(nM(n)), where M(n) = O(n(2.376)) is the time needed to multiply two n by n matrices with 0,1 entires over the integers. We show that the algorithm can be parallelized and implemented in NC5. (C) 1998 John Wiley & Sons, Inc.</p>","Computer Science, Software Engineering"
"WOS:000073141600005","An algorithmic version of the blow-up lemma","1998","
<p>Recently we developed a new method in graph theory based on the regularity lemma. The method is applied to find certain spanning subgraphs in dense graphs. The other main general tool of the method, besides the regularity lemma, is the so-called blow-up lemma (Komlos, Sarkozy, and Szemeredi [Combinatorica, 17, 109-123 (1997)]. This lemma helps to find bounded degree spanning subgraphs in epsilon-regular graphs. Our original proof of the lemma is not algorithmic, it applies probabilistic methods. in this paper we provide an algorithmic version of the blow-up lemma. The desired subgraph, for an n-vertex graph, can be found in time O(nM(n)), where M(n) = O(n(2.376)) is the time needed to multiply two n by n matrices with 0,1 entires over the integers. We show that the algorithm can be parallelized and implemented in NC5. (C) 1998 John Wiley & Sons, Inc.</p>","Computer Science"
"WOS:000076103600007","The RHODOS DSM system","1998","
<p>The RHODOS Distributed Shared Memory (DSM) system forms an easy to program (using sequential programming skills without a need to learn DSM concepts) and transparent environment, and provides high performance computational services. This system also allows programmers to use either the sequential or release consistency model for the shared memory. These attributes have been achieved by integrating DSM into the RHODOS distributed operating system rather than putting it on top of an existing operating system, as have other researchers. In this paper we report on the development of a DSM system integrated into RHODOS and how it supports programmers; the programming of three applications to demonstrate ease of programming; and the results of running these three applications using the two different consistency protocols. (C) 1998 Published by Elsevier Science B.V.</p>","Computer Science, Hardware & Architecture"
"WOS:000076103600007","The RHODOS DSM system","1998","
<p>The RHODOS Distributed Shared Memory (DSM) system forms an easy to program (using sequential programming skills without a need to learn DSM concepts) and transparent environment, and provides high performance computational services. This system also allows programmers to use either the sequential or release consistency model for the shared memory. These attributes have been achieved by integrating DSM into the RHODOS distributed operating system rather than putting it on top of an existing operating system, as have other researchers. In this paper we report on the development of a DSM system integrated into RHODOS and how it supports programmers; the programming of three applications to demonstrate ease of programming; and the results of running these three applications using the two different consistency protocols. (C) 1998 Published by Elsevier Science B.V.</p>","Computer Science, Theory & Methods"
"WOS:000076103600007","The RHODOS DSM system","1998","
<p>The RHODOS Distributed Shared Memory (DSM) system forms an easy to program (using sequential programming skills without a need to learn DSM concepts) and transparent environment, and provides high performance computational services. This system also allows programmers to use either the sequential or release consistency model for the shared memory. These attributes have been achieved by integrating DSM into the RHODOS distributed operating system rather than putting it on top of an existing operating system, as have other researchers. In this paper we report on the development of a DSM system integrated into RHODOS and how it supports programmers; the programming of three applications to demonstrate ease of programming; and the results of running these three applications using the two different consistency protocols. (C) 1998 Published by Elsevier Science B.V.</p>","Computer Science"
"WOS:000075816100003","An approach for establishing enterprise data standards","1998","
<p>Avoid the common pitfalls experienced by many organizations that have established enterprise data standards by following a straightforward process. Standards should be defined around a simple framework and be borrowed from third parties or vendors wherever possible to save time. It is better to reuse, rather than reinvent or create standards. To be used successfully, standards must be easy to understand, have a proven track record, be easy to use, and be readily available. Standards should be maintained in a central repository such as Lotus Notes or on an intranet Web site so that they are readily accessible to project teams.</p>","Computer Science, Information Systems"
"WOS:000075816100003","An approach for establishing enterprise data standards","1998","
<p>Avoid the common pitfalls experienced by many organizations that have established enterprise data standards by following a straightforward process. Standards should be defined around a simple framework and be borrowed from third parties or vendors wherever possible to save time. It is better to reuse, rather than reinvent or create standards. To be used successfully, standards must be easy to understand, have a proven track record, be easy to use, and be readily available. Standards should be maintained in a central repository such as Lotus Notes or on an intranet Web site so that they are readily accessible to project teams.</p>","Computer Science"
"WOS:000072583200001","Edge and depth from focus","1998","
<p>This paper proposes a novel method to obtain the reliable edge and depth information by integrating a set of multi-focus images, i.e., a sequence of images taken by systematically varying a camera parameter focus. In previous work on depth measurement using focusing or defocusing, the accuracy depends upon the size and location of local windows where the amount of blur is measured. In contrast,no windowing is needed in our method; the blur is evaluated from the intensity change along corresponding pixels in the multi-focus images. Such a blur analysis enables us not only to detect the edge points without using spatial differentiation but also to estimate the depth with high accuracy. In addition, the analysis result is stable because the proposed method involves integral computations such as summation and least-square model fitting. This paper first discusses the fundamental properties of multi-focus images based on a step edge model. Then, two algorithms are presented: edge detection using art accumulated defocus image which represents the spatial distribution of blur, and depth estimation using a spatio-focal image which represents the intensity distribution along focus axis. The experimental results demonstrate that the highly precise measurement has been achieved: 0.5 pixel position fluctuation in edge detection and 0.2% error at 2.4 m in depth estimation.</p>","Computer Science, Artificial Intelligence"
"WOS:000072583200001","Edge and depth from focus","1998","
<p>This paper proposes a novel method to obtain the reliable edge and depth information by integrating a set of multi-focus images, i.e., a sequence of images taken by systematically varying a camera parameter focus. In previous work on depth measurement using focusing or defocusing, the accuracy depends upon the size and location of local windows where the amount of blur is measured. In contrast,no windowing is needed in our method; the blur is evaluated from the intensity change along corresponding pixels in the multi-focus images. Such a blur analysis enables us not only to detect the edge points without using spatial differentiation but also to estimate the depth with high accuracy. In addition, the analysis result is stable because the proposed method involves integral computations such as summation and least-square model fitting. This paper first discusses the fundamental properties of multi-focus images based on a step edge model. Then, two algorithms are presented: edge detection using art accumulated defocus image which represents the spatial distribution of blur, and depth estimation using a spatio-focal image which represents the intensity distribution along focus axis. The experimental results demonstrate that the highly precise measurement has been achieved: 0.5 pixel position fluctuation in edge detection and 0.2% error at 2.4 m in depth estimation.</p>","Computer Science"
"WOS:000076556500005","On the parsimony of the multi-layer perceptrons when processing encoded symbolic variables","1998","
<p>This article addresses the issue of symbolic processing with Multi-Layer Perceptrons through encoding. Given an encoding, we propose a lower bound of the number of parameters for an MLP to perform a random mapping of its input symbolic space to its output symbolic space. In the case of what we call binary encoding, the needed number of parameters may be theoretically computed. Given these two results, we show that the most efficient encodings are the ones which use one input unit per value.</p>","Computer Science, Artificial Intelligence"
"WOS:000076556500005","On the parsimony of the multi-layer perceptrons when processing encoded symbolic variables","1998","
<p>This article addresses the issue of symbolic processing with Multi-Layer Perceptrons through encoding. Given an encoding, we propose a lower bound of the number of parameters for an MLP to perform a random mapping of its input symbolic space to its output symbolic space. In the case of what we call binary encoding, the needed number of parameters may be theoretically computed. Given these two results, we show that the most efficient encodings are the ones which use one input unit per value.</p>","Computer Science"
"WOS:000074811100008","The emergence of distributed library services: A European perspective","1998","
<p>This article discusses the emergence of distributed library services drawing on recent European initiatives to describe developments. it focuses on Z39.50-based services. It outlines a descriptive framework for such services and briefly introduces developments in other domains. Projects funded through the EU Telematics Application Programme are highlighted and some recent developments in U.K. higher education are introduced.</p>","Computer Science, Information Systems"
"WOS:000074811100008","The emergence of distributed library services: A European perspective","1998","
<p>This article discusses the emergence of distributed library services drawing on recent European initiatives to describe developments. it focuses on Z39.50-based services. It outlines a descriptive framework for such services and briefly introduces developments in other domains. Projects funded through the EU Telematics Application Programme are highlighted and some recent developments in U.K. higher education are introduced.</p>","Computer Science"
"WOS:000085483000010","Parallel interactive media server systems","1998","
<p>Interactive media server systems play an important role in the envisioned 'Information society'. Powerful media server systems are one of the cornerstones of the networked society in which media servers store news information, product descriptions, customer information, video clips and many other media elements that are used to inform consumers, run businesses, or entertain people.</p>
<p>Within this paper we distinguish two types of media objects. Realtime media on the one hand and non-realtime media objects on the other hand. Whereas realtime media, e.g. audio and video streams, are mainly used in information and entertainment applications, non-realtime media is used in all general purpose applications, e.g. conventional web services. The paper presents the design of two media server systems, handling one of the two types of media objects each. The server systems described in the paper are both based on a distributed memory parallel computer system. For each of the server systems presented here, a single important question is studied in detail. This is the data layout question for non-realtime media servers and the communication scheduling problem for realtime media servers.</p>","Computer Science, Theory & Methods"
"WOS:000085483000010","Parallel interactive media server systems","1998","
<p>Interactive media server systems play an important role in the envisioned 'Information society'. Powerful media server systems are one of the cornerstones of the networked society in which media servers store news information, product descriptions, customer information, video clips and many other media elements that are used to inform consumers, run businesses, or entertain people.</p>
<p>Within this paper we distinguish two types of media objects. Realtime media on the one hand and non-realtime media objects on the other hand. Whereas realtime media, e.g. audio and video streams, are mainly used in information and entertainment applications, non-realtime media is used in all general purpose applications, e.g. conventional web services. The paper presents the design of two media server systems, handling one of the two types of media objects each. The server systems described in the paper are both based on a distributed memory parallel computer system. For each of the server systems presented here, a single important question is studied in detail. This is the data layout question for non-realtime media servers and the communication scheduling problem for realtime media servers.</p>","Computer Science"
"WOS:000074454800036","On Puiseux expansion of approximate eigenvalues and eigenvectors","1998","
<p>In [1], approximate eigenvalues and eigenvectors are defined and algorithms to compute them are described. However, the algorithms require a certain condition: the eigenvalues of M module S are all distinct, where M is a given matrix with polynomial entries and S is a maximal ideal generated by the indeterminate in M. In this paper, we deal with the construction of approximate eigenvalues and eigenvectors when the condition is not satisfied. In this case, powers of approximate eigenvalues and eigenvectors become, in general, fractions. In other words, approximate eigenvalues and eigenvectors are expressed in the form of Puiseux series. We focus on a matrix with univariate polynomial entries and give complete algorithms to compute the approximate eigenvalues and eigenvectors of the matrix.</p>","Computer Science, Hardware & Architecture"
"WOS:000074454800036","On Puiseux expansion of approximate eigenvalues and eigenvectors","1998","
<p>In [1], approximate eigenvalues and eigenvectors are defined and algorithms to compute them are described. However, the algorithms require a certain condition: the eigenvalues of M module S are all distinct, where M is a given matrix with polynomial entries and S is a maximal ideal generated by the indeterminate in M. In this paper, we deal with the construction of approximate eigenvalues and eigenvectors when the condition is not satisfied. In this case, powers of approximate eigenvalues and eigenvectors become, in general, fractions. In other words, approximate eigenvalues and eigenvectors are expressed in the form of Puiseux series. We focus on a matrix with univariate polynomial entries and give complete algorithms to compute the approximate eigenvalues and eigenvectors of the matrix.</p>","Computer Science, Information Systems"
"WOS:000074454800036","On Puiseux expansion of approximate eigenvalues and eigenvectors","1998","
<p>In [1], approximate eigenvalues and eigenvectors are defined and algorithms to compute them are described. However, the algorithms require a certain condition: the eigenvalues of M module S are all distinct, where M is a given matrix with polynomial entries and S is a maximal ideal generated by the indeterminate in M. In this paper, we deal with the construction of approximate eigenvalues and eigenvectors when the condition is not satisfied. In this case, powers of approximate eigenvalues and eigenvectors become, in general, fractions. In other words, approximate eigenvalues and eigenvectors are expressed in the form of Puiseux series. We focus on a matrix with univariate polynomial entries and give complete algorithms to compute the approximate eigenvalues and eigenvectors of the matrix.</p>","Computer Science"
"WOS:000074605400001","Combining negation as failure and embedded implications in logic programs","1998","
<p>In this paper we consider a language which combines embedded hypothetical implications and negation as failure (NAF). For this language we develop a top-down query evaluation procedure, a Kripke/Kleene fixed point semantics, and a logical interpretation by means of a completion construction. As a difference with respect to other proposals, we put no restriction on the occurrences of negation by failure; in particular, programs are not required to be stratified. The operational semantics we propose is an extension to our language of Stark's ESLDNF, and allows negative non-ground literals to be selected in a query. The fixed point semantics is a generalization of those developed by Fitting and Kunen for fat logic programs and makes use of Kleene strong three-valued logic. The completion of a program is a recursive theory interpreted in a three-valued modal logic. We prove soundness and completeness of the operational semantics with respect to both the fixed point semantics and the completion. While soundness results require no restriction, completeness results are limited by the possibility of floundering. Similarly to Stark, we prove completeness for the class of so-called epsilon-queries, which are not subjected to floundering. Since the property of being an epsilon-query is undecidable, we give a syntactical decidable condition which ensures this property. Such a condition is a non-trivial generalization of the usual allowedness condition for flat programs. (C) 1998 Elsevier Science Inc. All rights reserved.</p>","Computer Science, Theory & Methods"
"WOS:000074605400001","Combining negation as failure and embedded implications in logic programs","1998","
<p>In this paper we consider a language which combines embedded hypothetical implications and negation as failure (NAF). For this language we develop a top-down query evaluation procedure, a Kripke/Kleene fixed point semantics, and a logical interpretation by means of a completion construction. As a difference with respect to other proposals, we put no restriction on the occurrences of negation by failure; in particular, programs are not required to be stratified. The operational semantics we propose is an extension to our language of Stark's ESLDNF, and allows negative non-ground literals to be selected in a query. The fixed point semantics is a generalization of those developed by Fitting and Kunen for fat logic programs and makes use of Kleene strong three-valued logic. The completion of a program is a recursive theory interpreted in a three-valued modal logic. We prove soundness and completeness of the operational semantics with respect to both the fixed point semantics and the completion. While soundness results require no restriction, completeness results are limited by the possibility of floundering. Similarly to Stark, we prove completeness for the class of so-called epsilon-queries, which are not subjected to floundering. Since the property of being an epsilon-query is undecidable, we give a syntactical decidable condition which ensures this property. Such a condition is a non-trivial generalization of the usual allowedness condition for flat programs. (C) 1998 Elsevier Science Inc. All rights reserved.</p>","Computer Science"
"WOS:000075656800001","Application of reconstruction-based scatter compensation to thallium-201 SPECT: Implementations for reduced reconstructed image noise","1998","
<p>Scatter compensation in T1-201 single photon emission computed tomography (SPECT) presents an interesting challenge because of the multiple emission energies and relatively large proportion of scattered photons, In this paper, we present a simulation study investigating reconstructed image noise levels arising from various implementations of iterative reconstruction-based scatter compensation (RBSC) in T1-201 SPECT, A two-stage analysis was used to study single and multiple energy window implementations of reconstruction-based scatter compensation, and RBSC was compared to the upper limits on performance for other approaches to handling scatter. In the first stage, singular value decomposition of the system transfer matrix was used to analyze noise levels in a manner independent of the choice of reconstruction algorithm, providing results valid across a wide range of regularizations, In the second stage, the data were reconstructed using maximum-likelihood expectation-maximization, and the noise properties of the resultant images were analyzed. The best RBSC performance was obtained using multiple energy windows, one for each emission photopeak? and RBSC outperformed the upper limit on subtraction-based compensation methods. Implementing RBSC with the correct choice of energy window acquisition scheme is a promising method for performing scatter compensation for T1-201 SPECT.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000075656800001","Application of reconstruction-based scatter compensation to thallium-201 SPECT: Implementations for reduced reconstructed image noise","1998","
<p>Scatter compensation in T1-201 single photon emission computed tomography (SPECT) presents an interesting challenge because of the multiple emission energies and relatively large proportion of scattered photons, In this paper, we present a simulation study investigating reconstructed image noise levels arising from various implementations of iterative reconstruction-based scatter compensation (RBSC) in T1-201 SPECT, A two-stage analysis was used to study single and multiple energy window implementations of reconstruction-based scatter compensation, and RBSC was compared to the upper limits on performance for other approaches to handling scatter. In the first stage, singular value decomposition of the system transfer matrix was used to analyze noise levels in a manner independent of the choice of reconstruction algorithm, providing results valid across a wide range of regularizations, In the second stage, the data were reconstructed using maximum-likelihood expectation-maximization, and the noise properties of the resultant images were analyzed. The best RBSC performance was obtained using multiple energy windows, one for each emission photopeak? and RBSC outperformed the upper limit on subtraction-based compensation methods. Implementing RBSC with the correct choice of energy window acquisition scheme is a promising method for performing scatter compensation for T1-201 SPECT.</p>","Computer Science"
"WOS:000082725700016","3D modeling from captured images using bi-directional ray traversal method","1998","
<p>In general, modeling techniques are classified into the creative approach and the reproductive approach. We discuss the latter approach to reconstruct existing objects. Lately, computers and networks are significantly improved, so that application algorithms are also expected to be improved as well. For instance, capturing and modeling existing objects without special equipment is really desirable. We propose a method to reconstruct whole appearances of existing objects with voxel space from 2D images that are captured using ordinary video cameras. Our approach does not involve extracting and tracking feature points of target objects unlike some conventional methods. Our method identifies the voxel properties so as to keep the appearance from frame to frame consistent. We call the way of expression screened voxel expressions. The voxel properties are represented by state variables that represent shape and color, and they are calculated by backward and forward ray shooting. During the calculation each voxel has several candidate colors and the correct one is only determined at the end. In this paper, we will discuss the feature value assessment using some experiments. We will also explain the basic methodology of our approach.</p>","Computer Science, Artificial Intelligence"
"WOS:000082725700016","3D modeling from captured images using bi-directional ray traversal method","1998","
<p>In general, modeling techniques are classified into the creative approach and the reproductive approach. We discuss the latter approach to reconstruct existing objects. Lately, computers and networks are significantly improved, so that application algorithms are also expected to be improved as well. For instance, capturing and modeling existing objects without special equipment is really desirable. We propose a method to reconstruct whole appearances of existing objects with voxel space from 2D images that are captured using ordinary video cameras. Our approach does not involve extracting and tracking feature points of target objects unlike some conventional methods. Our method identifies the voxel properties so as to keep the appearance from frame to frame consistent. We call the way of expression screened voxel expressions. The voxel properties are represented by state variables that represent shape and color, and they are calculated by backward and forward ray shooting. During the calculation each voxel has several candidate colors and the correct one is only determined at the end. In this paper, we will discuss the feature value assessment using some experiments. We will also explain the basic methodology of our approach.</p>","Computer Science, Software Engineering"
"WOS:000082725700016","3D modeling from captured images using bi-directional ray traversal method","1998","
<p>In general, modeling techniques are classified into the creative approach and the reproductive approach. We discuss the latter approach to reconstruct existing objects. Lately, computers and networks are significantly improved, so that application algorithms are also expected to be improved as well. For instance, capturing and modeling existing objects without special equipment is really desirable. We propose a method to reconstruct whole appearances of existing objects with voxel space from 2D images that are captured using ordinary video cameras. Our approach does not involve extracting and tracking feature points of target objects unlike some conventional methods. Our method identifies the voxel properties so as to keep the appearance from frame to frame consistent. We call the way of expression screened voxel expressions. The voxel properties are represented by state variables that represent shape and color, and they are calculated by backward and forward ray shooting. During the calculation each voxel has several candidate colors and the correct one is only determined at the end. In this paper, we will discuss the feature value assessment using some experiments. We will also explain the basic methodology of our approach.</p>","Computer Science"
"WOS:000075205900012","An empirical investigation of program spectra","1998","
<p>A variety of expensive software maintenance and testing tasks require a comparison of the behaviors of program versions. Program spectra have recently been proposed as a heuristic for use in performing such comparisons. To assess the potential usefulness of spectra in this context, we conducted an experiment that examined the relationship between program spectra and program behavior, and empirically compared several types of spectra. This paper reports the results of that experiment.</p>","Computer Science, Software Engineering"
"WOS:000075205900012","An empirical investigation of program spectra","1998","
<p>A variety of expensive software maintenance and testing tasks require a comparison of the behaviors of program versions. Program spectra have recently been proposed as a heuristic for use in performing such comparisons. To assess the potential usefulness of spectra in this context, we conducted an experiment that examined the relationship between program spectra and program behavior, and empirically compared several types of spectra. This paper reports the results of that experiment.</p>","Computer Science"
"WOS:000076622200002","Conceptual design of the branch-oriented simulation system DYMOS (Dynamic models for smog analysis)","1998","
<p>This paper gives an overview of the research at GMD FIRST in the field of air pollution simulation. It presents the conception and history of the DYMOS model system, and the interconnections between DYMOS and the more applied projects which are described in the subsequent papers of this issue. The developed systems have been applied for winter smog, summer smog, ozone forecast, transboundary air pollution, traffic-induced air pollution and environmental risk management; they will be enlarged to include climate change and sustainable development in the near future. The general aim of DYMOS is to provide users with branch-oriented simulation systems and the know-how necessary for applying these systems.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000076622200002","Conceptual design of the branch-oriented simulation system DYMOS (Dynamic models for smog analysis)","1998","
<p>This paper gives an overview of the research at GMD FIRST in the field of air pollution simulation. It presents the conception and history of the DYMOS model system, and the interconnections between DYMOS and the more applied projects which are described in the subsequent papers of this issue. The developed systems have been applied for winter smog, summer smog, ozone forecast, transboundary air pollution, traffic-induced air pollution and environmental risk management; they will be enlarged to include climate change and sustainable development in the near future. The general aim of DYMOS is to provide users with branch-oriented simulation systems and the know-how necessary for applying these systems.</p>","Computer Science, Software Engineering"
"WOS:000076622200002","Conceptual design of the branch-oriented simulation system DYMOS (Dynamic models for smog analysis)","1998","
<p>This paper gives an overview of the research at GMD FIRST in the field of air pollution simulation. It presents the conception and history of the DYMOS model system, and the interconnections between DYMOS and the more applied projects which are described in the subsequent papers of this issue. The developed systems have been applied for winter smog, summer smog, ozone forecast, transboundary air pollution, traffic-induced air pollution and environmental risk management; they will be enlarged to include climate change and sustainable development in the near future. The general aim of DYMOS is to provide users with branch-oriented simulation systems and the know-how necessary for applying these systems.</p>","Computer Science"
"WOS:000074283900002","Multimedia - An introduction","1998","
<p>Multimedia-the combination of text, animated graphics, video, and sound-presents information in a way that is more interesting and easier to grasp than text alone. It has been used for education at all levels, job training, and games and by the entertainment industry. It is becoming more readily available as the price of personal computers and their accessories declines. Multimedia as a human-computer interface was made possible some half-dozen years ago by the rise of affordable digital technology. Previously, multimedia effects were produced by computer-controlled analog devices, like videocassette recorders, projectors, and tape recorders. Digital technology's exponential decline in price and increase in capacity has enabled it to overtake analog technology. The Internet is the breeding ground for multimedia ideas and the delivery vehicle of multimedia objects to a huge audience. This paper reviews the uses of multimedia, the technologies that support it, and the larger architectural and design issues.</p>","Computer Science, Hardware & Architecture"
"WOS:000074283900002","Multimedia - An introduction","1998","
<p>Multimedia-the combination of text, animated graphics, video, and sound-presents information in a way that is more interesting and easier to grasp than text alone. It has been used for education at all levels, job training, and games and by the entertainment industry. It is becoming more readily available as the price of personal computers and their accessories declines. Multimedia as a human-computer interface was made possible some half-dozen years ago by the rise of affordable digital technology. Previously, multimedia effects were produced by computer-controlled analog devices, like videocassette recorders, projectors, and tape recorders. Digital technology's exponential decline in price and increase in capacity has enabled it to overtake analog technology. The Internet is the breeding ground for multimedia ideas and the delivery vehicle of multimedia objects to a huge audience. This paper reviews the uses of multimedia, the technologies that support it, and the larger architectural and design issues.</p>","Computer Science, Information Systems"
"WOS:000074283900002","Multimedia - An introduction","1998","
<p>Multimedia-the combination of text, animated graphics, video, and sound-presents information in a way that is more interesting and easier to grasp than text alone. It has been used for education at all levels, job training, and games and by the entertainment industry. It is becoming more readily available as the price of personal computers and their accessories declines. Multimedia as a human-computer interface was made possible some half-dozen years ago by the rise of affordable digital technology. Previously, multimedia effects were produced by computer-controlled analog devices, like videocassette recorders, projectors, and tape recorders. Digital technology's exponential decline in price and increase in capacity has enabled it to overtake analog technology. The Internet is the breeding ground for multimedia ideas and the delivery vehicle of multimedia objects to a huge audience. This paper reviews the uses of multimedia, the technologies that support it, and the larger architectural and design issues.</p>","Computer Science, Software Engineering"
"WOS:000074283900002","Multimedia - An introduction","1998","
<p>Multimedia-the combination of text, animated graphics, video, and sound-presents information in a way that is more interesting and easier to grasp than text alone. It has been used for education at all levels, job training, and games and by the entertainment industry. It is becoming more readily available as the price of personal computers and their accessories declines. Multimedia as a human-computer interface was made possible some half-dozen years ago by the rise of affordable digital technology. Previously, multimedia effects were produced by computer-controlled analog devices, like videocassette recorders, projectors, and tape recorders. Digital technology's exponential decline in price and increase in capacity has enabled it to overtake analog technology. The Internet is the breeding ground for multimedia ideas and the delivery vehicle of multimedia objects to a huge audience. This paper reviews the uses of multimedia, the technologies that support it, and the larger architectural and design issues.</p>","Computer Science, Theory & Methods"
"WOS:000074283900002","Multimedia - An introduction","1998","
<p>Multimedia-the combination of text, animated graphics, video, and sound-presents information in a way that is more interesting and easier to grasp than text alone. It has been used for education at all levels, job training, and games and by the entertainment industry. It is becoming more readily available as the price of personal computers and their accessories declines. Multimedia as a human-computer interface was made possible some half-dozen years ago by the rise of affordable digital technology. Previously, multimedia effects were produced by computer-controlled analog devices, like videocassette recorders, projectors, and tape recorders. Digital technology's exponential decline in price and increase in capacity has enabled it to overtake analog technology. The Internet is the breeding ground for multimedia ideas and the delivery vehicle of multimedia objects to a huge audience. This paper reviews the uses of multimedia, the technologies that support it, and the larger architectural and design issues.</p>","Computer Science"
"WOS:000073412100005","Assessment of clusteranalysis and self-organizing maps","1998","
<p>Market segmentation represents a central problem of preparing marketing activities. The methodical approach of this problem is supported by clustering methods. Available data are used to detect common grounds regarding their quality structures. Therefore statistics provides various methods for cluster analysis. Self-organizing maps are another possibility to form classes. They are a special approach of the artificial neural networks. The statistical methods and these methods, which are based on organic processes of our brain, offer different solutions although the starting conditions are the same. Often decisions about investigations are based on such solutions. Therefore the results of clustering are very important to reveal systematic information about the size of classes and their structure. Methodical notes are needed for the use of any clustering method. This paper offers a simplified way to select the best result for clustering.</p>","Computer Science, Artificial Intelligence"
"WOS:000073412100005","Assessment of clusteranalysis and self-organizing maps","1998","
<p>Market segmentation represents a central problem of preparing marketing activities. The methodical approach of this problem is supported by clustering methods. Available data are used to detect common grounds regarding their quality structures. Therefore statistics provides various methods for cluster analysis. Self-organizing maps are another possibility to form classes. They are a special approach of the artificial neural networks. The statistical methods and these methods, which are based on organic processes of our brain, offer different solutions although the starting conditions are the same. Often decisions about investigations are based on such solutions. Therefore the results of clustering are very important to reveal systematic information about the size of classes and their structure. Methodical notes are needed for the use of any clustering method. This paper offers a simplified way to select the best result for clustering.</p>","Computer Science"
"WOS:000075722000005","An operator splitting algorithm for the three-dimensional advection-diffusion equation","1998","
<p>Operator splitting algorithms are frequently used for solving the advection-diffusion equation, especially to deal with advection dominated transport problems. In this paper an operator splitting algorithm for the three-dimensional advection-diffusion equation is presented. The algorithm represents a second-order-accurate adaptation of the Holly and Preissmann scheme for three-dimensional problems. The governing equation is split into an advection equation and a diffusion equation, and they are solved by a backward method of characteristics and a finite element method, respectively. The Hermite interpolation function is used for interpolation of concentration in the advection step. The spatial gradients of concentration in the Hermite interpolation are obtained by solving equations for concentration gradients in the advection step. To make the composite algorithm efficient, only three equations for first-order concentration derivatives are solved in the diffusion step of computation. The higher-order spatial concentration gradients, necessary to advance the solution in a computational cycle, are obtained by numerical differentiations based on the available information. The simulation characteristics and accuracy of the proposed algorithm are demonstrated by several advection dominated transport problems. (C) 1998 John Wiley & Sons, Ltd.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000075722000005","An operator splitting algorithm for the three-dimensional advection-diffusion equation","1998","
<p>Operator splitting algorithms are frequently used for solving the advection-diffusion equation, especially to deal with advection dominated transport problems. In this paper an operator splitting algorithm for the three-dimensional advection-diffusion equation is presented. The algorithm represents a second-order-accurate adaptation of the Holly and Preissmann scheme for three-dimensional problems. The governing equation is split into an advection equation and a diffusion equation, and they are solved by a backward method of characteristics and a finite element method, respectively. The Hermite interpolation function is used for interpolation of concentration in the advection step. The spatial gradients of concentration in the Hermite interpolation are obtained by solving equations for concentration gradients in the advection step. To make the composite algorithm efficient, only three equations for first-order concentration derivatives are solved in the diffusion step of computation. The higher-order spatial concentration gradients, necessary to advance the solution in a computational cycle, are obtained by numerical differentiations based on the available information. The simulation characteristics and accuracy of the proposed algorithm are demonstrated by several advection dominated transport problems. (C) 1998 John Wiley & Sons, Ltd.</p>","Computer Science"
"WOS:000074183200005","Effect of a pneumatically driven haptic interface on the perceptional capabilities of human operators","1998","
<p>This paper describes experimental studies conducted using a pneumatically driven haptic interface (PHI) system. The PHI is a unilateral exoskeletal device that tracks the motion of the shoulder and elbow. The study was carried out to evaluate the impact of an exoskeletal haptic interface on human perceptional capabilities. A population of twenty subjects participated in a set of experiments that were tailored to assess force sensation, shape perception, and effect of force feedback in task performance. Using Weber fractions, we contrasted the outcome of our force sensation experiments against results reported by psychophysical researchers. The results indicated that the perception of weight (or force magnitude) through the haptic interface was significantly affected for relatively low reference force levels (4.44 N, Weber fraction = 0.5). The effect progressively diminished as the force level was increased, and almost matched the natural human capabilities for a reference force level of 18 N (Weber fraction = 0.06). The haptic shape identification experiments showed that the subjects were able to identify various shapes using the PHI system (1 = 0.3 m reference length, with Weber fraction = 0.38). This identification, however, was adversely affected by the lack of tactile sensation in the haptic device. The outcome of the force-feedback experiments demonstrated mixed results, an observation that was consistent with experimental studies of other researchers. While force feedback did not affect the time needed to complete the task, the subjects' performance was significantly improved when the experiments involved controlling the thickness of a curve drawn on a pressure-sensitive tablet.</p>","Computer Science, Cybernetics"
"WOS:000074183200005","Effect of a pneumatically driven haptic interface on the perceptional capabilities of human operators","1998","
<p>This paper describes experimental studies conducted using a pneumatically driven haptic interface (PHI) system. The PHI is a unilateral exoskeletal device that tracks the motion of the shoulder and elbow. The study was carried out to evaluate the impact of an exoskeletal haptic interface on human perceptional capabilities. A population of twenty subjects participated in a set of experiments that were tailored to assess force sensation, shape perception, and effect of force feedback in task performance. Using Weber fractions, we contrasted the outcome of our force sensation experiments against results reported by psychophysical researchers. The results indicated that the perception of weight (or force magnitude) through the haptic interface was significantly affected for relatively low reference force levels (4.44 N, Weber fraction = 0.5). The effect progressively diminished as the force level was increased, and almost matched the natural human capabilities for a reference force level of 18 N (Weber fraction = 0.06). The haptic shape identification experiments showed that the subjects were able to identify various shapes using the PHI system (1 = 0.3 m reference length, with Weber fraction = 0.38). This identification, however, was adversely affected by the lack of tactile sensation in the haptic device. The outcome of the force-feedback experiments demonstrated mixed results, an observation that was consistent with experimental studies of other researchers. While force feedback did not affect the time needed to complete the task, the subjects' performance was significantly improved when the experiments involved controlling the thickness of a curve drawn on a pressure-sensitive tablet.</p>","Computer Science, Software Engineering"
"WOS:000074183200005","Effect of a pneumatically driven haptic interface on the perceptional capabilities of human operators","1998","
<p>This paper describes experimental studies conducted using a pneumatically driven haptic interface (PHI) system. The PHI is a unilateral exoskeletal device that tracks the motion of the shoulder and elbow. The study was carried out to evaluate the impact of an exoskeletal haptic interface on human perceptional capabilities. A population of twenty subjects participated in a set of experiments that were tailored to assess force sensation, shape perception, and effect of force feedback in task performance. Using Weber fractions, we contrasted the outcome of our force sensation experiments against results reported by psychophysical researchers. The results indicated that the perception of weight (or force magnitude) through the haptic interface was significantly affected for relatively low reference force levels (4.44 N, Weber fraction = 0.5). The effect progressively diminished as the force level was increased, and almost matched the natural human capabilities for a reference force level of 18 N (Weber fraction = 0.06). The haptic shape identification experiments showed that the subjects were able to identify various shapes using the PHI system (1 = 0.3 m reference length, with Weber fraction = 0.38). This identification, however, was adversely affected by the lack of tactile sensation in the haptic device. The outcome of the force-feedback experiments demonstrated mixed results, an observation that was consistent with experimental studies of other researchers. While force feedback did not affect the time needed to complete the task, the subjects' performance was significantly improved when the experiments involved controlling the thickness of a curve drawn on a pressure-sensitive tablet.</p>","Computer Science"
"WOS:000072722800005","Log-time delay consideration on mesh with multiple buses","1998","
<p>Two-dimensional mesh-connected computer with multiple buses (2-MCCMB) and the two-dimensional mesh-connected computer with segmented buses (2-MCCSMB) are two promising parallel organizations. Many efficient algorithms have been developed on them under the constant-time delay model for each broadcast. In practice, we consider the log-time delay model in which it is assumed that each broadcast takes O(log S) time to reach all the processors connected to the bus, where S is the number of processors connected to the bus. Instead of putting O(log S) slow down factor in the lime complexity required, this paper presents better time bounds for semigroup and prefix computations on 2-MCCMB and 2-MCCSMB, respectively, by using the technique of balancing communications and computations.</p>","Computer Science, Hardware & Architecture"
"WOS:000072722800005","Log-time delay consideration on mesh with multiple buses","1998","
<p>Two-dimensional mesh-connected computer with multiple buses (2-MCCMB) and the two-dimensional mesh-connected computer with segmented buses (2-MCCSMB) are two promising parallel organizations. Many efficient algorithms have been developed on them under the constant-time delay model for each broadcast. In practice, we consider the log-time delay model in which it is assumed that each broadcast takes O(log S) time to reach all the processors connected to the bus, where S is the number of processors connected to the bus. Instead of putting O(log S) slow down factor in the lime complexity required, this paper presents better time bounds for semigroup and prefix computations on 2-MCCMB and 2-MCCSMB, respectively, by using the technique of balancing communications and computations.</p>","Computer Science, Software Engineering"
"WOS:000072722800005","Log-time delay consideration on mesh with multiple buses","1998","
<p>Two-dimensional mesh-connected computer with multiple buses (2-MCCMB) and the two-dimensional mesh-connected computer with segmented buses (2-MCCSMB) are two promising parallel organizations. Many efficient algorithms have been developed on them under the constant-time delay model for each broadcast. In practice, we consider the log-time delay model in which it is assumed that each broadcast takes O(log S) time to reach all the processors connected to the bus, where S is the number of processors connected to the bus. Instead of putting O(log S) slow down factor in the lime complexity required, this paper presents better time bounds for semigroup and prefix computations on 2-MCCMB and 2-MCCSMB, respectively, by using the technique of balancing communications and computations.</p>","Computer Science"
"WOS:000075487100004","Can Census Offices publish statistics for more than one small area geography? An analysis of the differencing problem in statistical disclosure","1998","
<p>The paper describes a problem faced by National Statistical Offices when publishing the results of decennial censuses for small geographical areas. If they publish statistical tables for two or more sets of areas, users can compare the tables and produce new statistics for the areas formed by differencing, which may have populations below confidentiality thresholds. To investigate the problem, the authors construct a software system and carry out a series of experiments using a large synthetic population base for Yorkshire and Humberside. The results indicate that publishing statistics for zones close in size to the primary areas is not safe unless the zones have been carefully designed. However, publishing statistics for sufficiently large areas such as 5 km grid squares or postal sectors alongside enumeration districts is safe.</p>","Computer Science, Information Systems"
"WOS:000075487100004","Can Census Offices publish statistics for more than one small area geography? An analysis of the differencing problem in statistical disclosure","1998","
<p>The paper describes a problem faced by National Statistical Offices when publishing the results of decennial censuses for small geographical areas. If they publish statistical tables for two or more sets of areas, users can compare the tables and produce new statistics for the areas formed by differencing, which may have populations below confidentiality thresholds. To investigate the problem, the authors construct a software system and carry out a series of experiments using a large synthetic population base for Yorkshire and Humberside. The results indicate that publishing statistics for zones close in size to the primary areas is not safe unless the zones have been carefully designed. However, publishing statistics for sufficiently large areas such as 5 km grid squares or postal sectors alongside enumeration districts is safe.</p>","Computer Science"
"WOS:000074404200003","The use of formative quizzes for deep learning","1998","
<p>Simple formative quizzes are widely used for assessment. These have traditionally tested knowledge and simple comprehension. This paper describes how to construct questions to test all cognitive levels of learning for a course in introductory programming. The framework for learning is based on Bloom's taxonomy. The paper gives practical advice through examples and by describing a computer system to help deliver quizzes. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000074404200003","The use of formative quizzes for deep learning","1998","
<p>Simple formative quizzes are widely used for assessment. These have traditionally tested knowledge and simple comprehension. This paper describes how to construct questions to test all cognitive levels of learning for a course in introductory programming. The framework for learning is based on Bloom's taxonomy. The paper gives practical advice through examples and by describing a computer system to help deliver quizzes. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science"
"WOS:000086659400012","An ontology-based approach to parsing Turkish sentences","1998","
<p>The main problem with natural language analysis is the ambiguity found in various levels of linguistic information. Syntactic analysis with word senses is frequently not enough to resolve all ambiguities found in a sentence. Although natural languages are highly connected to the real world knowledge, most of the parsing architectures do not make use of it effectively. In this paper, a new methodology is proposed for analyzing Turkish sentences which is heavily based on the constraints in the ontology. The methodology also makes use of morphological marks of Turkish which generally denote semantic properties. analysis aims to find the propositional structure of the input utterance without constructing a deep syntactic tree, instead it utilizes a weak interaction between syntax and semantics. The architecture constructs a specific meaning representation on top of the analyzed propositional structure.</p>","Computer Science, Artificial Intelligence"
"WOS:000086659400012","An ontology-based approach to parsing Turkish sentences","1998","
<p>The main problem with natural language analysis is the ambiguity found in various levels of linguistic information. Syntactic analysis with word senses is frequently not enough to resolve all ambiguities found in a sentence. Although natural languages are highly connected to the real world knowledge, most of the parsing architectures do not make use of it effectively. In this paper, a new methodology is proposed for analyzing Turkish sentences which is heavily based on the constraints in the ontology. The methodology also makes use of morphological marks of Turkish which generally denote semantic properties. analysis aims to find the propositional structure of the input utterance without constructing a deep syntactic tree, instead it utilizes a weak interaction between syntax and semantics. The architecture constructs a specific meaning representation on top of the analyzed propositional structure.</p>","Computer Science"
"WOS:000076286500007","A geographical information system for a GPS based personal guidance system","1998","
<p>This pager describes the process of building a GIS for use in real time by blind travellers. Initially the components of a Personal Guidance System (PGS) for blind pedestrians are outlined. The location finding and database components of the system are then elaborated. Next follows a discussion of the environmental features likely to be used by blind travellers, and a discussion of the different path following and environmental learning modes that can be activated in the system. Developments such as personalizing the system and accounting for veering are also presented. Finally, possible competing schemes and problems related to the GIS component are examined.</p>","Computer Science, Information Systems"
"WOS:000076286500007","A geographical information system for a GPS based personal guidance system","1998","
<p>This pager describes the process of building a GIS for use in real time by blind travellers. Initially the components of a Personal Guidance System (PGS) for blind pedestrians are outlined. The location finding and database components of the system are then elaborated. Next follows a discussion of the environmental features likely to be used by blind travellers, and a discussion of the different path following and environmental learning modes that can be activated in the system. Developments such as personalizing the system and accounting for veering are also presented. Finally, possible competing schemes and problems related to the GIS component are examined.</p>","Computer Science"
"WOS:000075787600009","Feature tracking with automatic selection of spatial scales","1998","
<p>When observing a dynamic world, the size of image structures may vary over time. This article emphasizes the need for including explicit mechanisms for automatic scale selection in feature tracking algorithms in order to: (i) adapt the local scale of processing to the local image structure, and (ii) adapt to the size variations that may occur over time. The problems of corner detection and blob detection are treated in detail, and a combined framework for feature tracking is presented. The integrated tracking algorithm overcomes some of the inherent limitations of exposing fixed-scale tracking methods to image sequences in which the size variations are large. It is also shown how the stability over time of scale descriptors can be used as a part of a multi-cue similarity measure for matching. Experiments on real-world sequences are presented showing the performance of the algorithm when applied to (individual) tracking of corners and blobs, (C) 1998 Academic Press.</p>","Computer Science, Artificial Intelligence"
"WOS:000075787600009","Feature tracking with automatic selection of spatial scales","1998","
<p>When observing a dynamic world, the size of image structures may vary over time. This article emphasizes the need for including explicit mechanisms for automatic scale selection in feature tracking algorithms in order to: (i) adapt the local scale of processing to the local image structure, and (ii) adapt to the size variations that may occur over time. The problems of corner detection and blob detection are treated in detail, and a combined framework for feature tracking is presented. The integrated tracking algorithm overcomes some of the inherent limitations of exposing fixed-scale tracking methods to image sequences in which the size variations are large. It is also shown how the stability over time of scale descriptors can be used as a part of a multi-cue similarity measure for matching. Experiments on real-world sequences are presented showing the performance of the algorithm when applied to (individual) tracking of corners and blobs, (C) 1998 Academic Press.</p>","Computer Science"
"WOS:000071853100007","PiN: Computation of pion-nucleon scattering and bound states with the color dielectric quark model","1998","
<p>Described is a 9421 line Fortran program PiN that solves momentum-space integral Lippmann-Schwinger equations for the scattering observables and the bound/resonant state properties of the coupled (pi N,pi Delta) system. The elementary interactions arise from the chiral color-dielectric quark and the cloudy bag models, which are computed by the code. The bound state wave functions of the quarks are computed by a separate code. (C) 1998 Elsevier Science B.V.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000071853100007","PiN: Computation of pion-nucleon scattering and bound states with the color dielectric quark model","1998","
<p>Described is a 9421 line Fortran program PiN that solves momentum-space integral Lippmann-Schwinger equations for the scattering observables and the bound/resonant state properties of the coupled (pi N,pi Delta) system. The elementary interactions arise from the chiral color-dielectric quark and the cloudy bag models, which are computed by the code. The bound state wave functions of the quarks are computed by a separate code. (C) 1998 Elsevier Science B.V.</p>","Computer Science"
"WOS:000083636700027","Reduced complexity correlation attacks on two clock-controlled generators","1998","
<p>The Shrinking Generator and the Alternating Step Generator are two of the most well known clock-controlled stream ciphers. We consider correlation attacks on these two generators, based on an identified relation to the decoding problem for the deletion channel and the insertion channel, respectively. Several ways of reducing the decoding complexity are proposed and investigated, resulting in ""divide-and-conquer"" attacks on the two generators having considerably lower complexity than previously known attacks.</p>","Computer Science, Software Engineering"
"WOS:000083636700027","Reduced complexity correlation attacks on two clock-controlled generators","1998","
<p>The Shrinking Generator and the Alternating Step Generator are two of the most well known clock-controlled stream ciphers. We consider correlation attacks on these two generators, based on an identified relation to the decoding problem for the deletion channel and the insertion channel, respectively. Several ways of reducing the decoding complexity are proposed and investigated, resulting in ""divide-and-conquer"" attacks on the two generators having considerably lower complexity than previously known attacks.</p>","Computer Science"
"WOS:000075597500008","Learning viewpoint-invariant face representations from visual experience in an attractor network","1998","
<p>In natural visual experience, different views of an object or face tend to appear in close temporal proximity as an animal manipulates the object or navigates around it, or as a face changes expression or pose. A set of simulations is presented which demonstrate how viewpoint-invariant representations of faces can be developed from visual experience by capturing the temporal relationships among the input patterns. The simulations explored the interaction of temporal smoothing of activity signals with Hebbian learning in both a feedforward layer and a second, recurrent layer of a network. The feedforward connections were trained by competitive Hebbian learning with temporal smoothing of the post-synaptic unit activities. The recurrent layer was a generalization of a Hopfield network with a low-pass temporal filter on all unit activities. The combination of basic Hebbian learning with temporal smoothing of unit activities produced an attractor network learning rule that associated temporally proximal input patterns into basins of attraction. These two mechanisms were demonstrated in a model that took grey-level images of faces as input. Following training on image sequences of faces as they changed pose, multiple views of a given face fell into the same basin of attraction,and the system acquired representations of faces that were approximately viewpoint-invariant.</p>","Computer Science, Artificial Intelligence"
"WOS:000075597500008","Learning viewpoint-invariant face representations from visual experience in an attractor network","1998","
<p>In natural visual experience, different views of an object or face tend to appear in close temporal proximity as an animal manipulates the object or navigates around it, or as a face changes expression or pose. A set of simulations is presented which demonstrate how viewpoint-invariant representations of faces can be developed from visual experience by capturing the temporal relationships among the input patterns. The simulations explored the interaction of temporal smoothing of activity signals with Hebbian learning in both a feedforward layer and a second, recurrent layer of a network. The feedforward connections were trained by competitive Hebbian learning with temporal smoothing of the post-synaptic unit activities. The recurrent layer was a generalization of a Hopfield network with a low-pass temporal filter on all unit activities. The combination of basic Hebbian learning with temporal smoothing of unit activities produced an attractor network learning rule that associated temporally proximal input patterns into basins of attraction. These two mechanisms were demonstrated in a model that took grey-level images of faces as input. Following training on image sequences of faces as they changed pose, multiple views of a given face fell into the same basin of attraction,and the system acquired representations of faces that were approximately viewpoint-invariant.</p>","Computer Science"
"WOS:000073519700004","Developing online support for clinical information system developers: The FAQ approach","1998","
<p>Objective: We investigate a knowledge-based help system for developers of an integrated clinical information system (CIS). The first objective in the study was to determine the system's ability to answer users' questions effectively. User performance and behavior were studied. The second objective was to evaluate the effect of using questions and answers to augment or replace traditional program documentation.</p>
<p>Design: A comparative study of user and system effectiveness using a collection of 47 veritable questions regarding the CIS, solicited from various CIS developers, is conducted. Most questions were concerning the clinical data model and acquiring the data.</p>
<p>Measurements: Answers using current documentation known by users were compared to answers found using the help system. Answers existing within traditional documentation were compared to answers existing within question-answer exchanges (Q-A's).</p>
<p>Results: The support system augmented 39% of users' answers to test questions. Though the Q-A's were less than 5% of the total documentation collected, these files contained answers to nearly 50% of the questions in the test group. The rest of the documentation contained about 75% of the answers.</p>
<p>Conclusions: A knowledge-based help system built by collecting questions and answers can be a viable alternative to large documentation files, providing the questions and answers can be collected effectively. (C) 1998 Academic Press.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000073519700004","Developing online support for clinical information system developers: The FAQ approach","1998","
<p>Objective: We investigate a knowledge-based help system for developers of an integrated clinical information system (CIS). The first objective in the study was to determine the system's ability to answer users' questions effectively. User performance and behavior were studied. The second objective was to evaluate the effect of using questions and answers to augment or replace traditional program documentation.</p>
<p>Design: A comparative study of user and system effectiveness using a collection of 47 veritable questions regarding the CIS, solicited from various CIS developers, is conducted. Most questions were concerning the clinical data model and acquiring the data.</p>
<p>Measurements: Answers using current documentation known by users were compared to answers found using the help system. Answers existing within traditional documentation were compared to answers existing within question-answer exchanges (Q-A's).</p>
<p>Results: The support system augmented 39% of users' answers to test questions. Though the Q-A's were less than 5% of the total documentation collected, these files contained answers to nearly 50% of the questions in the test group. The rest of the documentation contained about 75% of the answers.</p>
<p>Conclusions: A knowledge-based help system built by collecting questions and answers can be a viable alternative to large documentation files, providing the questions and answers can be collected effectively. (C) 1998 Academic Press.</p>","Computer Science"
"WOS:000074105300015","New method for the analysis of multiple positron emission tomography dynamic datasets: an example applied to the estimation of the cerebral metabolic rate of oxygen","1998","
<p>Positron emission tomography (PET) provides the ability to extract useful quantitative information not available through other radiological techniques. In certain studies, the physiological parameters of interest cannot be determined from the data obtained from a single PET experiment alone. In this case, multiple experiments are required. At present, the methods used to analyse measurements acquired from multiple experiments often involve considering them separately during the modelling procedures. These methods of analysis may cause errors to be propagated through successive modelling procedures and do not fully utilise the information content provided by the PET measurements. A new method is presented, based on linear least squares for the analysis of PET dynamic data acquired from multiple experiments. This method simultaneously considers the complete set of measurements obtained and provides reliable parameter estimates. The efficient use of the information content provided by multiple experiments is considered and the propagation of errors is discussed. To facilitate our discussion, we apply this new method to the estimation of the cerebral metabolic rate of oxygen and the parameters of the oxygen utilisation model as a practical example. The results demonstrate a significant improvement in the reliability and estimation accuracy of the estimates for this new method. Furthermore, this method reduced the likelihood of errors being propagated. Therefore, the proposed method is suitable for the analysis of multiple PET dynamic datasets.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000074105300015","New method for the analysis of multiple positron emission tomography dynamic datasets: an example applied to the estimation of the cerebral metabolic rate of oxygen","1998","
<p>Positron emission tomography (PET) provides the ability to extract useful quantitative information not available through other radiological techniques. In certain studies, the physiological parameters of interest cannot be determined from the data obtained from a single PET experiment alone. In this case, multiple experiments are required. At present, the methods used to analyse measurements acquired from multiple experiments often involve considering them separately during the modelling procedures. These methods of analysis may cause errors to be propagated through successive modelling procedures and do not fully utilise the information content provided by the PET measurements. A new method is presented, based on linear least squares for the analysis of PET dynamic data acquired from multiple experiments. This method simultaneously considers the complete set of measurements obtained and provides reliable parameter estimates. The efficient use of the information content provided by multiple experiments is considered and the propagation of errors is discussed. To facilitate our discussion, we apply this new method to the estimation of the cerebral metabolic rate of oxygen and the parameters of the oxygen utilisation model as a practical example. The results demonstrate a significant improvement in the reliability and estimation accuracy of the estimates for this new method. Furthermore, this method reduced the likelihood of errors being propagated. Therefore, the proposed method is suitable for the analysis of multiple PET dynamic datasets.</p>","Computer Science"
"WOS:000074928500010","Elastic flexural-torsional buckling of structures by computer","1998","
<p>Flexural-torsional buckling considerations usually control the design of unbraced steel beams and beam-columns. Although there are many sources of information on elastic buckling, these are not comprehensive, and often difficult to access. This paper describes and exemplifies a single comprehensive source of information, the computer program PRFELB for the elastic flexural-torsional buckling analysis of a wide range of beams, beam-columns, and plane frames. PRFELB is a user-friendly computer program, with menus and data and help screens that advise and assist in data input. Graphics can be used to check the geometry and to display the buckled shapes. The use of PRFELB will assist designers to implement the method of design by buckling analysis which is permitted in modern structural steel design standards, either explicitly or implicitly. (C) 1998 Elsevier Science Ltd and Civil-Comp Ltd. All rights reserved.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000074928500010","Elastic flexural-torsional buckling of structures by computer","1998","
<p>Flexural-torsional buckling considerations usually control the design of unbraced steel beams and beam-columns. Although there are many sources of information on elastic buckling, these are not comprehensive, and often difficult to access. This paper describes and exemplifies a single comprehensive source of information, the computer program PRFELB for the elastic flexural-torsional buckling analysis of a wide range of beams, beam-columns, and plane frames. PRFELB is a user-friendly computer program, with menus and data and help screens that advise and assist in data input. Graphics can be used to check the geometry and to display the buckled shapes. The use of PRFELB will assist designers to implement the method of design by buckling analysis which is permitted in modern structural steel design standards, either explicitly or implicitly. (C) 1998 Elsevier Science Ltd and Civil-Comp Ltd. All rights reserved.</p>","Computer Science"
"WOS:000082522800030","A tool for the development of meta-applications supporting several message-passing programming environments","1998","
<p>This paper presents the extensions made to WAMM (Wide Area, Metacomputer Manager) in order to manage a metacomputer composed of hosts on which several message-passing programming environments run. Initially, WAMM only permitted the execution of meta-applications consisting of PVM components. To allow the execution of meta-applications composed of PVM and MPI tasks, WAMM has been extended by means of PLUS. PLUS is one of the main components of the MOL Project. It provides an efficient, easy-to-use interface among various communication libraries.</p>
<p>The paper is organized as follows: after the introduction, Sections 2 and 3 describe WAMM and PLUS respectively Section 4 presents some implementation details related to the new WAMM functionalities, and Section 5 describes some performance measurements related to the new version of WAMM. Section 6 concludes the paper.</p>","Computer Science, Software Engineering"
"WOS:000082522800030","A tool for the development of meta-applications supporting several message-passing programming environments","1998","
<p>This paper presents the extensions made to WAMM (Wide Area, Metacomputer Manager) in order to manage a metacomputer composed of hosts on which several message-passing programming environments run. Initially, WAMM only permitted the execution of meta-applications consisting of PVM components. To allow the execution of meta-applications composed of PVM and MPI tasks, WAMM has been extended by means of PLUS. PLUS is one of the main components of the MOL Project. It provides an efficient, easy-to-use interface among various communication libraries.</p>
<p>The paper is organized as follows: after the introduction, Sections 2 and 3 describe WAMM and PLUS respectively Section 4 presents some implementation details related to the new WAMM functionalities, and Section 5 describes some performance measurements related to the new version of WAMM. Section 6 concludes the paper.</p>","Computer Science, Theory & Methods"
"WOS:000082522800030","A tool for the development of meta-applications supporting several message-passing programming environments","1998","
<p>This paper presents the extensions made to WAMM (Wide Area, Metacomputer Manager) in order to manage a metacomputer composed of hosts on which several message-passing programming environments run. Initially, WAMM only permitted the execution of meta-applications consisting of PVM components. To allow the execution of meta-applications composed of PVM and MPI tasks, WAMM has been extended by means of PLUS. PLUS is one of the main components of the MOL Project. It provides an efficient, easy-to-use interface among various communication libraries.</p>
<p>The paper is organized as follows: after the introduction, Sections 2 and 3 describe WAMM and PLUS respectively Section 4 presents some implementation details related to the new WAMM functionalities, and Section 5 describes some performance measurements related to the new version of WAMM. Section 6 concludes the paper.</p>","Computer Science"
"WOS:000072457300005","The recurrence of dynamic fuzzy systems","1998","
<p>This paper analyses a recurrent behavior of dynamic fuzzy systems defined by fuzzy relations on a Euclidean space. By introducing a recurrence for crisp sets, we prove probability-theoretical properties for the fuzzy systems. In the contractive case in Kurano et al. [Fuzzy Sets and Systems 51 (1992) 83-88], the existence of the maximum recurrent set is proved. As another case, we introduce a monotonicity for fuzzy relations, which is extended from the linear structure in Yoshida et al. [Fuzzy Sets and Systems 66 (1994) 83-95]. In the monotone case we prove the existence of the arcwise connected maximal recurrent sets. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science, Theory & Methods"
"WOS:000072457300005","The recurrence of dynamic fuzzy systems","1998","
<p>This paper analyses a recurrent behavior of dynamic fuzzy systems defined by fuzzy relations on a Euclidean space. By introducing a recurrence for crisp sets, we prove probability-theoretical properties for the fuzzy systems. In the contractive case in Kurano et al. [Fuzzy Sets and Systems 51 (1992) 83-88], the existence of the maximum recurrent set is proved. As another case, we introduce a monotonicity for fuzzy relations, which is extended from the linear structure in Yoshida et al. [Fuzzy Sets and Systems 66 (1994) 83-95]. In the monotone case we prove the existence of the arcwise connected maximal recurrent sets. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science"
"WOS:000077612700017","A multi-agent approach to process planning and fabrication in distributed manufacturing","1998","
<p>In the paper a multi-agent approach to the development of a distributed manufacturing architecture is presented. An essential building block introduced here is the virtual work system (VWS) which represents a manufacturing work system in the information space. The VWS is structured as an autonomous agent and is a constituent entity of an agent network. In the network dynamic clusters of cooperating agents are solving manufacturing tasks. A machining work system and its VWS is demonstrated in a case study. Its role in the agent communication network is discussed in a process planning and fabrication domain. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000077612700017","A multi-agent approach to process planning and fabrication in distributed manufacturing","1998","
<p>In the paper a multi-agent approach to the development of a distributed manufacturing architecture is presented. An essential building block introduced here is the virtual work system (VWS) which represents a manufacturing work system in the information space. The VWS is structured as an autonomous agent and is a constituent entity of an agent network. In the network dynamic clusters of cooperating agents are solving manufacturing tasks. A machining work system and its VWS is demonstrated in a case study. Its role in the agent communication network is discussed in a process planning and fabrication domain. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science"
"WOS:000076672700004","Web-based health care agents; the case of reminders and todos, too (R2Do2)","1998","
<p>This paper describes efforts to develop and field an agent-based, healthcare middleware framework that securely connects practice rule sets to patient records to anticipate health todo items and to remind and alert users about these items over the web. Reminders and todos, too (R2Do2 is an example of merging data- and document-centric architectures, and of integrating agents into patient-provider collaboration environments. A test of this capability verifies that R2Do2 is progressing toward its two goals: (1) an open standards framework for middleware in the healthcare field; and (2) an implementation of the 'principle of optimality' to derive the best possible health plans for each user. This paper concludes with lessons learned to date. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science, Artificial Intelligence"
"WOS:000076672700004","Web-based health care agents; the case of reminders and todos, too (R2Do2)","1998","
<p>This paper describes efforts to develop and field an agent-based, healthcare middleware framework that securely connects practice rule sets to patient records to anticipate health todo items and to remind and alert users about these items over the web. Reminders and todos, too (R2Do2 is an example of merging data- and document-centric architectures, and of integrating agents into patient-provider collaboration environments. A test of this capability verifies that R2Do2 is progressing toward its two goals: (1) an open standards framework for middleware in the healthcare field; and (2) an implementation of the 'principle of optimality' to derive the best possible health plans for each user. This paper concludes with lessons learned to date. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science"
"WOS:000076555400005","Control strategy for optimal compromise between trip time and energy consumption in a high-speed railway","1998","
<p>This paper presents an approach to identify a fuzzy control model for determining an economical running pattern for a high-speed railway through an optimal compromise between trip time and energy consumption. Since the linguistic model is intuitive and informative to railway operators, they can easily implement a control strategy for saving energy. The approach includes structure identification and parameter identification. It is proposed to utilize a fuzzy c-means clustering and a GA hybrid scheme to identify the structure and parameters of a fuzzy model, respectively. To evaluate the advantages and the effectiveness of the suggested approach, numerical examples are presented. Comparison shows that the proposed approach can produce a fuzzy model with higher accuracy and smaller number of rules than previously achieved in other works, To show the global optimization and local convergence of the GA hybrid-scheme, an optimization problem having a few local minima and maxima is considered.</p>","Computer Science, Cybernetics"
"WOS:000076555400005","Control strategy for optimal compromise between trip time and energy consumption in a high-speed railway","1998","
<p>This paper presents an approach to identify a fuzzy control model for determining an economical running pattern for a high-speed railway through an optimal compromise between trip time and energy consumption. Since the linguistic model is intuitive and informative to railway operators, they can easily implement a control strategy for saving energy. The approach includes structure identification and parameter identification. It is proposed to utilize a fuzzy c-means clustering and a GA hybrid scheme to identify the structure and parameters of a fuzzy model, respectively. To evaluate the advantages and the effectiveness of the suggested approach, numerical examples are presented. Comparison shows that the proposed approach can produce a fuzzy model with higher accuracy and smaller number of rules than previously achieved in other works, To show the global optimization and local convergence of the GA hybrid-scheme, an optimization problem having a few local minima and maxima is considered.</p>","Computer Science, Theory & Methods"
"WOS:000076555400005","Control strategy for optimal compromise between trip time and energy consumption in a high-speed railway","1998","
<p>This paper presents an approach to identify a fuzzy control model for determining an economical running pattern for a high-speed railway through an optimal compromise between trip time and energy consumption. Since the linguistic model is intuitive and informative to railway operators, they can easily implement a control strategy for saving energy. The approach includes structure identification and parameter identification. It is proposed to utilize a fuzzy c-means clustering and a GA hybrid scheme to identify the structure and parameters of a fuzzy model, respectively. To evaluate the advantages and the effectiveness of the suggested approach, numerical examples are presented. Comparison shows that the proposed approach can produce a fuzzy model with higher accuracy and smaller number of rules than previously achieved in other works, To show the global optimization and local convergence of the GA hybrid-scheme, an optimization problem having a few local minima and maxima is considered.</p>","Computer Science"
"WOS:000073493300005","Automatic multiblock decomposition using hypercube++ for grid generation","1998","
<p>A new method for an automatic multiblock decomposition of a field around any number of complex geometries is proposed. This method is based on hypercube+ +, data structure to represent the hierarchical relationship between various types of hypercubes while the geometry of the hypercube is represented by non-uniform rational B-splines (NURBS) volume which maps the physical space of a hypercube onto the parameter space. The generation of grid topology based on the hypercube+ + structure consists of two main steps: (1) the hypercube+ + generation step, which is applied to the region around a single shape element, for example a wing in an airplane, to generate an appropriate hypercube; and (2) the hypercube+ + merging step, which merges simple hypercubes or the ones merged already into a single but more complex hypercube+ + to represent the regions around the shape composed of several shape elements. This approach will be demonstrated with some examples to show that it allows a user to construct a multiblock decomposition in a matter of minutes for any three dimensional configurations in an automatic manner. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000073493300005","Automatic multiblock decomposition using hypercube++ for grid generation","1998","
<p>A new method for an automatic multiblock decomposition of a field around any number of complex geometries is proposed. This method is based on hypercube+ +, data structure to represent the hierarchical relationship between various types of hypercubes while the geometry of the hypercube is represented by non-uniform rational B-splines (NURBS) volume which maps the physical space of a hypercube onto the parameter space. The generation of grid topology based on the hypercube+ + structure consists of two main steps: (1) the hypercube+ + generation step, which is applied to the region around a single shape element, for example a wing in an airplane, to generate an appropriate hypercube; and (2) the hypercube+ + merging step, which merges simple hypercubes or the ones merged already into a single but more complex hypercube+ + to represent the regions around the shape composed of several shape elements. This approach will be demonstrated with some examples to show that it allows a user to construct a multiblock decomposition in a matter of minutes for any three dimensional configurations in an automatic manner. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science"
"WOS:000078155700005","Labour market ""consultancies""","1998","
<p>Unlike in the United States, Great Britain and to a smaller extent France consultancies in Germany don't recruit their Information Specialists mainly out of Library Schools. In a primary research with several consultancies it is clarified why the situation is as it is and which specific skills consultancies expect from people leaving Library Schools.</p>","Computer Science, Information Systems"
"WOS:000078155700005","Labour market ""consultancies""","1998","
<p>Unlike in the United States, Great Britain and to a smaller extent France consultancies in Germany don't recruit their Information Specialists mainly out of Library Schools. In a primary research with several consultancies it is clarified why the situation is as it is and which specific skills consultancies expect from people leaving Library Schools.</p>","Computer Science"
"WOS:000075865300005","Acquisition of shape information in working memory, as a function of viewing time and number of consecutive images: evidence for a succession of discrete storage classes","1998","
<p>The capacity of visual working memory was investigated using abstract images that were slightly distorted NXN (with generally N = 8) square lattices of black or white randomly selected elements. After viewing an image, or a sequence of images, the subjects viewed couples of images containing the test image and a distracter image derived from the first one by changing the black or white value of q randomly selected elements. The number q was adjusted in each experiment to the difficulty of the task and the abilities of the subject. The fraction of recognition errors, given q and N was used to evaluate the number M of bits memorized by the subject. For untrained subjects, this number M varied in a biphasic manner as a function of the time t of presentation of the test image: it was on average 13 bits for 1 s, 16 bits for 2 to 5 s, and 20 bits for 8 s. The slow pace of acquisition, from 1 to 8 s, seems due to encoding difficulties, and not to channel capacity limitations. Beyond 8 a, M(t), accurately determined for one subject, followed a square root law, in agreement with 19th century observations an the memorization of lists of digits. When two consecutive 8 x 8 images were viewed and tested in the same order, the number of memorized bits was downshifted by a nearly constant amount, independent of t, and equal on average to 6-7 bits. Across the subjects, the shift was independent of M. When two consecutive test images were related, the recognition errors decreased for both images, whether the testing was performed in the presentation or the reverse order. Studies involving three subjects, indicate that, when viewing m consecutive images, the average amount of information captured per image varies with m in a stepwise fashion. The first two step boundaries were around m = 3 and m = 9-12. The data are compatible with a model of organization of working memory in several successive layers containing increasing numbers of units, the more remote a unit, the lower the rate at which it may acquire encoded information. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science, Artificial Intelligence"
"WOS:000075865300005","Acquisition of shape information in working memory, as a function of viewing time and number of consecutive images: evidence for a succession of discrete storage classes","1998","
<p>The capacity of visual working memory was investigated using abstract images that were slightly distorted NXN (with generally N = 8) square lattices of black or white randomly selected elements. After viewing an image, or a sequence of images, the subjects viewed couples of images containing the test image and a distracter image derived from the first one by changing the black or white value of q randomly selected elements. The number q was adjusted in each experiment to the difficulty of the task and the abilities of the subject. The fraction of recognition errors, given q and N was used to evaluate the number M of bits memorized by the subject. For untrained subjects, this number M varied in a biphasic manner as a function of the time t of presentation of the test image: it was on average 13 bits for 1 s, 16 bits for 2 to 5 s, and 20 bits for 8 s. The slow pace of acquisition, from 1 to 8 s, seems due to encoding difficulties, and not to channel capacity limitations. Beyond 8 a, M(t), accurately determined for one subject, followed a square root law, in agreement with 19th century observations an the memorization of lists of digits. When two consecutive 8 x 8 images were viewed and tested in the same order, the number of memorized bits was downshifted by a nearly constant amount, independent of t, and equal on average to 6-7 bits. Across the subjects, the shift was independent of M. When two consecutive test images were related, the recognition errors decreased for both images, whether the testing was performed in the presentation or the reverse order. Studies involving three subjects, indicate that, when viewing m consecutive images, the average amount of information captured per image varies with m in a stepwise fashion. The first two step boundaries were around m = 3 and m = 9-12. The data are compatible with a model of organization of working memory in several successive layers containing increasing numbers of units, the more remote a unit, the lower the rate at which it may acquire encoded information. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science"
"WOS:000077186300028","Different discrete wavelet transforms applied to denoising analytical data","1998","
<p>Discrete wavelet transform (DWT) denoising contains three steps: forward transformation of the signal to the wavelet domain, reduction of the wavelet coefficients; and inverse transformation to the native domain. Three aspects that should be considered for DWT denoising include selecting the wavelet type, selecting the threshold, and applying the threshold to the wavelet coefficients. Although there exists an infinite variety of wavelet transformations, 22 orthonormal wavelet transforms that are typically used, which include Haar, 9 daublets, 5 coiflets, and 7 symmlets, were evaluated. Four threshold selection methods have been studied: universal, minimax, Stein's unbiased estimate of risk (SURE), and minimum description length (MDL) criteria. The application of the threshold to the wavelet coefficients includes global (hard, soft, garrote, and firm), level-dependent, data-dependent, translation invariant (TI), and wavelet package transform (WPT) thresholding methods. The different DWT-based denoising methods were evaluated by using synthetic data containing white Gaussian noise. The results of comparison have shown that most DWTs are very powerful methods for denoising and that the MDL and the TI methods are practical. The MDL criterion is the only method that can select a threshold-for wavelet coefficients and select an optimal transform type. The TI method is insensitive to the wavelet filter so that for a variety of wavelet filters equivalent results were obtained. Savitzky-Golay and Fourier transform denoising results were used as reference methods. IR and HPLC data were used to compare denoising methods.</p>","Computer Science, Information Systems"
"WOS:000077186300028","Different discrete wavelet transforms applied to denoising analytical data","1998","
<p>Discrete wavelet transform (DWT) denoising contains three steps: forward transformation of the signal to the wavelet domain, reduction of the wavelet coefficients; and inverse transformation to the native domain. Three aspects that should be considered for DWT denoising include selecting the wavelet type, selecting the threshold, and applying the threshold to the wavelet coefficients. Although there exists an infinite variety of wavelet transformations, 22 orthonormal wavelet transforms that are typically used, which include Haar, 9 daublets, 5 coiflets, and 7 symmlets, were evaluated. Four threshold selection methods have been studied: universal, minimax, Stein's unbiased estimate of risk (SURE), and minimum description length (MDL) criteria. The application of the threshold to the wavelet coefficients includes global (hard, soft, garrote, and firm), level-dependent, data-dependent, translation invariant (TI), and wavelet package transform (WPT) thresholding methods. The different DWT-based denoising methods were evaluated by using synthetic data containing white Gaussian noise. The results of comparison have shown that most DWTs are very powerful methods for denoising and that the MDL and the TI methods are practical. The MDL criterion is the only method that can select a threshold-for wavelet coefficients and select an optimal transform type. The TI method is insensitive to the wavelet filter so that for a variety of wavelet filters equivalent results were obtained. Savitzky-Golay and Fourier transform denoising results were used as reference methods. IR and HPLC data were used to compare denoising methods.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000077186300028","Different discrete wavelet transforms applied to denoising analytical data","1998","
<p>Discrete wavelet transform (DWT) denoising contains three steps: forward transformation of the signal to the wavelet domain, reduction of the wavelet coefficients; and inverse transformation to the native domain. Three aspects that should be considered for DWT denoising include selecting the wavelet type, selecting the threshold, and applying the threshold to the wavelet coefficients. Although there exists an infinite variety of wavelet transformations, 22 orthonormal wavelet transforms that are typically used, which include Haar, 9 daublets, 5 coiflets, and 7 symmlets, were evaluated. Four threshold selection methods have been studied: universal, minimax, Stein's unbiased estimate of risk (SURE), and minimum description length (MDL) criteria. The application of the threshold to the wavelet coefficients includes global (hard, soft, garrote, and firm), level-dependent, data-dependent, translation invariant (TI), and wavelet package transform (WPT) thresholding methods. The different DWT-based denoising methods were evaluated by using synthetic data containing white Gaussian noise. The results of comparison have shown that most DWTs are very powerful methods for denoising and that the MDL and the TI methods are practical. The MDL criterion is the only method that can select a threshold-for wavelet coefficients and select an optimal transform type. The TI method is insensitive to the wavelet filter so that for a variety of wavelet filters equivalent results were obtained. Savitzky-Golay and Fourier transform denoising results were used as reference methods. IR and HPLC data were used to compare denoising methods.</p>","Computer Science"
"WOS:000073200100026","The capacity of orthogonal and bi-orthogonal codes on the Gaussian channel","1998","
<p>This correspondence analyzes the performance of concatenated coding systems and modulation schemes operating over the additive white Gaussian noise (AWGN) channel by examining the loss of capacity resulting from each of the processing steps. The techniques described in this correspondence allow the separate evaluation of codes and decoders and thus the identification of where loss of capacity occurs. Knowledge of this capacity loss is very useful for the overall design of a communications system, ag., for evaluating the benefits of inner decoders that produce information beyond the maximum-likelihood (ML) estimate. The first two sections of this correspondence provide a general technique for calculating the composite capacity of an orthogonal or a bi-orthogonal code and the AWGN channel in isolation. The later sections examine the composite capacities of an orthogonal or a bi-orthogonal code, the AWGN channel, and various inner decoders including the decoder estimating the bit-by-bit probability of a one, as is used in turbo-codes. The calculations in these. examples show that the ML decoder introduces a large loss in capacity. Much of this capacity loss can be regained by using only slightly more complex inner decoders, e.g., a detector for M-ary frequency-shift keying (MFSK) that puts out the two most likely frequencies and the probability the ML estimate is correct produces significantly less degradation than one that puts out only the most likely frequency.</p>","Computer Science, Information Systems"
"WOS:000073200100026","The capacity of orthogonal and bi-orthogonal codes on the Gaussian channel","1998","
<p>This correspondence analyzes the performance of concatenated coding systems and modulation schemes operating over the additive white Gaussian noise (AWGN) channel by examining the loss of capacity resulting from each of the processing steps. The techniques described in this correspondence allow the separate evaluation of codes and decoders and thus the identification of where loss of capacity occurs. Knowledge of this capacity loss is very useful for the overall design of a communications system, ag., for evaluating the benefits of inner decoders that produce information beyond the maximum-likelihood (ML) estimate. The first two sections of this correspondence provide a general technique for calculating the composite capacity of an orthogonal or a bi-orthogonal code and the AWGN channel in isolation. The later sections examine the composite capacities of an orthogonal or a bi-orthogonal code, the AWGN channel, and various inner decoders including the decoder estimating the bit-by-bit probability of a one, as is used in turbo-codes. The calculations in these. examples show that the ML decoder introduces a large loss in capacity. Much of this capacity loss can be regained by using only slightly more complex inner decoders, e.g., a detector for M-ary frequency-shift keying (MFSK) that puts out the two most likely frequencies and the probability the ML estimate is correct produces significantly less degradation than one that puts out only the most likely frequency.</p>","Computer Science"
"WOS:000076133800069","EQL an EXPRESS Query Language","1998","
<p>EQL, an acronym for EXPRESS Query Language, is an SQL-like query language that is used to perform ad hoc queries on data in PART 21 files. PART 21 is the clear text encoding of data in the object-oriented EXPRESS modeling format and is the format for the STEP standards like AP203. Traditional uses for STEP files have been for transferring data between similar tools and populating a data model in one tool with the data from another tool. For example, moving a part design from one CAD system to another CAD system. If however, a software system has a different view of the information, a STEP file from one system contains significant amounts of data not applicable to other system. The receiving system needs the ability to query the STEP file for the objects important to its processing. Additionally, to integrate software systems using EXPRESS and PART 21 as a data transfer mechanism; an ad hoc query language is needed to account for the data in multiple schemas that a tool might expect to encounter. EQL is designed to accept data files in schemas that are not predefined to the tool and has the ability to perform all traditional data manipulation (DML) operations: select, update, insert and delete. EQL does not support data definition (DDL) like creating new object classes. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000076133800069","EQL an EXPRESS Query Language","1998","
<p>EQL, an acronym for EXPRESS Query Language, is an SQL-like query language that is used to perform ad hoc queries on data in PART 21 files. PART 21 is the clear text encoding of data in the object-oriented EXPRESS modeling format and is the format for the STEP standards like AP203. Traditional uses for STEP files have been for transferring data between similar tools and populating a data model in one tool with the data from another tool. For example, moving a part design from one CAD system to another CAD system. If however, a software system has a different view of the information, a STEP file from one system contains significant amounts of data not applicable to other system. The receiving system needs the ability to query the STEP file for the objects important to its processing. Additionally, to integrate software systems using EXPRESS and PART 21 as a data transfer mechanism; an ad hoc query language is needed to account for the data in multiple schemas that a tool might expect to encounter. EQL is designed to accept data files in schemas that are not predefined to the tool and has the ability to perform all traditional data manipulation (DML) operations: select, update, insert and delete. EQL does not support data definition (DDL) like creating new object classes. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science"
"WOS:000073646700003","Projection space image reconstruction using strip functions to calculate pixels more ""Natural"" for modeling the geometric response of the SPECT collimator","1998","
<p>The spatially varying geometric response of the collimator-detector system in single photon emission computed tomography (SPECT) causes loss in resolution, shape distortions, reconstructed density nonuniformity, and quantitative inaccuracies. A projection space image reconstruction algorithm is used to correct these reconstruction artifacts. The projectors F use strip functions to calculate pixels more ""natural"" far modeling the two-dimensional (2-D) geometric response of the SPECT collimator transaxially to the axis of rotation. These projectors are defined by summing the intersection of an array of multiple strips rotated at equal angles to approximate the ideal system geometric response of the collimator. Two projection models were evaluated for modeling the system geometric response function. For one projector each strip is of equal weight, for the other projector a Gaussian weighting is used, Parallel beam and fan beam projections of a physical three-dimensional (3-D) Hoffman brain phantom and a Jaszczak cold rod phantom were used to evaluate the geometric response correction Reconstructions were obtained by using the singular value decomposition (SVD) method and the iterative conjugate gradient algorithm to solve for q in the imaging equation FGq = p, where p is the projection measurement. The projector F included the new models for the geometric response, whereas, the backprojector G did not always model the geometric response in order to increase the computational speed. The final reconstruction was obtained by sampling the backprojection Gq at a discrete array of points. Reconstructions produced by the two proposed projectors showed improved resolution when compared against a unit-strip ""natural"" pixel model, the conventional image pixelized model with ray tracing to calculate the geometric response, and the filtered backprojection algorithm, When the reconstruction is displayed on fine grid points, the continuity and resolution of the image is preserved without the ring artifacts seen in the unit-strip ""natural"" pixel model. With present computing power, the geometric response correction using the proposed projection space reconstruction approach is not get feasible for routine clinical use.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000073646700003","Projection space image reconstruction using strip functions to calculate pixels more ""Natural"" for modeling the geometric response of the SPECT collimator","1998","
<p>The spatially varying geometric response of the collimator-detector system in single photon emission computed tomography (SPECT) causes loss in resolution, shape distortions, reconstructed density nonuniformity, and quantitative inaccuracies. A projection space image reconstruction algorithm is used to correct these reconstruction artifacts. The projectors F use strip functions to calculate pixels more ""natural"" far modeling the two-dimensional (2-D) geometric response of the SPECT collimator transaxially to the axis of rotation. These projectors are defined by summing the intersection of an array of multiple strips rotated at equal angles to approximate the ideal system geometric response of the collimator. Two projection models were evaluated for modeling the system geometric response function. For one projector each strip is of equal weight, for the other projector a Gaussian weighting is used, Parallel beam and fan beam projections of a physical three-dimensional (3-D) Hoffman brain phantom and a Jaszczak cold rod phantom were used to evaluate the geometric response correction Reconstructions were obtained by using the singular value decomposition (SVD) method and the iterative conjugate gradient algorithm to solve for q in the imaging equation FGq = p, where p is the projection measurement. The projector F included the new models for the geometric response, whereas, the backprojector G did not always model the geometric response in order to increase the computational speed. The final reconstruction was obtained by sampling the backprojection Gq at a discrete array of points. Reconstructions produced by the two proposed projectors showed improved resolution when compared against a unit-strip ""natural"" pixel model, the conventional image pixelized model with ray tracing to calculate the geometric response, and the filtered backprojection algorithm, When the reconstruction is displayed on fine grid points, the continuity and resolution of the image is preserved without the ring artifacts seen in the unit-strip ""natural"" pixel model. With present computing power, the geometric response correction using the proposed projection space reconstruction approach is not get feasible for routine clinical use.</p>","Computer Science"
"WOS:000080299600004","On a refined cyclic reservation-based MAC scheme for ultra high-speed hierarchical ring MANs","1998","
<p>To support a great number of dispersed users in a wider area with high-speed communication services, we investigate a highly efficient medium access control (MAC) scheme for the hierarchical ring networks. In these networks, traffic congestion may happen due to the mismatch of transmission speed between backbone and outer rings. To cope with the issue, we propose a refined MAC protocol based on the cyclic reservation-based access control method for different network resources, viz. network bandwidth and buffers. By this approach, our protocol can achieve fair access to the network resources and avoid traffic congestion at the same time. Also, the networks will have the properties of extremely high throughput, low delay, fair access, simple implementation, and so forth. To evaluate the network, several simulative experiments are performed and some optimistic results are revealed.</p>","Computer Science, Hardware & Architecture"
"WOS:000080299600004","On a refined cyclic reservation-based MAC scheme for ultra high-speed hierarchical ring MANs","1998","
<p>To support a great number of dispersed users in a wider area with high-speed communication services, we investigate a highly efficient medium access control (MAC) scheme for the hierarchical ring networks. In these networks, traffic congestion may happen due to the mismatch of transmission speed between backbone and outer rings. To cope with the issue, we propose a refined MAC protocol based on the cyclic reservation-based access control method for different network resources, viz. network bandwidth and buffers. By this approach, our protocol can achieve fair access to the network resources and avoid traffic congestion at the same time. Also, the networks will have the properties of extremely high throughput, low delay, fair access, simple implementation, and so forth. To evaluate the network, several simulative experiments are performed and some optimistic results are revealed.</p>","Computer Science"
"WOS:000077502400006","Adaptive Forms: an interaction technique for entering structured data","1998","
<p>Many software applications solicit input from the user via a ""forms"" paradigm that emulates their paper equivalent. It exploits the users' familiarity with these and is well suited for the input of simple attribute-value data (name, phone number, etc.). The paper-forms paradigm starts breaking down when there is user input that may or may not be applicable depending on previous user input. In paper-based forms, this manifests itself by sections marked ""fill out only if you entered yes in question 8a above"", and simple electronic forms suffer from the same problem-much space is taken up for input fields that are not applicable. One possible approach to making only relevant sections appear is to hand-write program fragments to hide and show them. As an alternative, we have developed a form specification language based on a context-free grammar that encodes data dependencies of the input, together with an accompanying run-time interpreter that uses novel layout techniques for collapsing already-entered input fields, for ""blending"" input fields possibly yet to come, and for showing only the applicable sections of the form. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science, Artificial Intelligence"
"WOS:000077502400006","Adaptive Forms: an interaction technique for entering structured data","1998","
<p>Many software applications solicit input from the user via a ""forms"" paradigm that emulates their paper equivalent. It exploits the users' familiarity with these and is well suited for the input of simple attribute-value data (name, phone number, etc.). The paper-forms paradigm starts breaking down when there is user input that may or may not be applicable depending on previous user input. In paper-based forms, this manifests itself by sections marked ""fill out only if you entered yes in question 8a above"", and simple electronic forms suffer from the same problem-much space is taken up for input fields that are not applicable. One possible approach to making only relevant sections appear is to hand-write program fragments to hide and show them. As an alternative, we have developed a form specification language based on a context-free grammar that encodes data dependencies of the input, together with an accompanying run-time interpreter that uses novel layout techniques for collapsing already-entered input fields, for ""blending"" input fields possibly yet to come, and for showing only the applicable sections of the form. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science"
"WOS:000076241800005","Performance evaluation of forwarding strategies for location management in mobile networks","1998","
<p>This paper presents a methodology for evaluating the performance of forwarding strategies for location management in a personal communication services (PCS) mobile network. A forwarding strategy in the PCS network can be implemented by two mechanisms: a forwarding operation which follows a chain of databases to locate a mobile user and a resetting operation which updates the databases in the chain so that the current location of a mobile user can be known directly without having to follow a chain of databases. In this paper, we consider the PCS network as a server whose function is to provide services to the mobile user for 'updating the location of the user as the user moves across a database boundary' and 'locating the mobile user'. We use a Markov chain to describe the behavior of the mobile user and analyze the best time when forwarding and resetting should be performed in order to optimize the service rate of the PCS network. We demonstrate the applicability of our approach with hexagonal and mesh coverage models for the PCS network and provide a physical interpretation of the result.</p>","Computer Science, Hardware & Architecture"
"WOS:000076241800005","Performance evaluation of forwarding strategies for location management in mobile networks","1998","
<p>This paper presents a methodology for evaluating the performance of forwarding strategies for location management in a personal communication services (PCS) mobile network. A forwarding strategy in the PCS network can be implemented by two mechanisms: a forwarding operation which follows a chain of databases to locate a mobile user and a resetting operation which updates the databases in the chain so that the current location of a mobile user can be known directly without having to follow a chain of databases. In this paper, we consider the PCS network as a server whose function is to provide services to the mobile user for 'updating the location of the user as the user moves across a database boundary' and 'locating the mobile user'. We use a Markov chain to describe the behavior of the mobile user and analyze the best time when forwarding and resetting should be performed in order to optimize the service rate of the PCS network. We demonstrate the applicability of our approach with hexagonal and mesh coverage models for the PCS network and provide a physical interpretation of the result.</p>","Computer Science, Information Systems"
"WOS:000076241800005","Performance evaluation of forwarding strategies for location management in mobile networks","1998","
<p>This paper presents a methodology for evaluating the performance of forwarding strategies for location management in a personal communication services (PCS) mobile network. A forwarding strategy in the PCS network can be implemented by two mechanisms: a forwarding operation which follows a chain of databases to locate a mobile user and a resetting operation which updates the databases in the chain so that the current location of a mobile user can be known directly without having to follow a chain of databases. In this paper, we consider the PCS network as a server whose function is to provide services to the mobile user for 'updating the location of the user as the user moves across a database boundary' and 'locating the mobile user'. We use a Markov chain to describe the behavior of the mobile user and analyze the best time when forwarding and resetting should be performed in order to optimize the service rate of the PCS network. We demonstrate the applicability of our approach with hexagonal and mesh coverage models for the PCS network and provide a physical interpretation of the result.</p>","Computer Science, Software Engineering"
"WOS:000076241800005","Performance evaluation of forwarding strategies for location management in mobile networks","1998","
<p>This paper presents a methodology for evaluating the performance of forwarding strategies for location management in a personal communication services (PCS) mobile network. A forwarding strategy in the PCS network can be implemented by two mechanisms: a forwarding operation which follows a chain of databases to locate a mobile user and a resetting operation which updates the databases in the chain so that the current location of a mobile user can be known directly without having to follow a chain of databases. In this paper, we consider the PCS network as a server whose function is to provide services to the mobile user for 'updating the location of the user as the user moves across a database boundary' and 'locating the mobile user'. We use a Markov chain to describe the behavior of the mobile user and analyze the best time when forwarding and resetting should be performed in order to optimize the service rate of the PCS network. We demonstrate the applicability of our approach with hexagonal and mesh coverage models for the PCS network and provide a physical interpretation of the result.</p>","Computer Science, Theory & Methods"
"WOS:000076241800005","Performance evaluation of forwarding strategies for location management in mobile networks","1998","
<p>This paper presents a methodology for evaluating the performance of forwarding strategies for location management in a personal communication services (PCS) mobile network. A forwarding strategy in the PCS network can be implemented by two mechanisms: a forwarding operation which follows a chain of databases to locate a mobile user and a resetting operation which updates the databases in the chain so that the current location of a mobile user can be known directly without having to follow a chain of databases. In this paper, we consider the PCS network as a server whose function is to provide services to the mobile user for 'updating the location of the user as the user moves across a database boundary' and 'locating the mobile user'. We use a Markov chain to describe the behavior of the mobile user and analyze the best time when forwarding and resetting should be performed in order to optimize the service rate of the PCS network. We demonstrate the applicability of our approach with hexagonal and mesh coverage models for the PCS network and provide a physical interpretation of the result.</p>","Computer Science"
"WOS:000074417400006","Learning with unreliable boundary queries","1998","
<p>We introduce a model for learning from examples and membership queries in situations where the boundary between positive and negative examples is somewhat iii-defined. In our model, queries near the boundary of a target concept may receive incorrect or ""don't care"" responses, and the distribution of examples has zero probability mass on the boundary region. The motivation behind our model is that in many cases the boundary between positive and negative examples is complicated or ""fuzzy."" However, one may still hope to learn successfully, because the typical examples that one sees do not come from that region, We present several positive results in this new model. We show how to learn the intersection of two arbitrary halfspaces when membership queries near the boundary may be answered incorrectly. Our algorithm is an extension of an algorithm of Baum (1990, 1991) that learns the intersection of two halfspaces whose bounding planes pass through the origin in the PAC-with-membership-queries model. We also describe algorithms for reaming several subclasses of monotone DNF formulas. (C) 1998 Academic Press.</p>","Computer Science, Hardware & Architecture"
"WOS:000074417400006","Learning with unreliable boundary queries","1998","
<p>We introduce a model for learning from examples and membership queries in situations where the boundary between positive and negative examples is somewhat iii-defined. In our model, queries near the boundary of a target concept may receive incorrect or ""don't care"" responses, and the distribution of examples has zero probability mass on the boundary region. The motivation behind our model is that in many cases the boundary between positive and negative examples is complicated or ""fuzzy."" However, one may still hope to learn successfully, because the typical examples that one sees do not come from that region, We present several positive results in this new model. We show how to learn the intersection of two arbitrary halfspaces when membership queries near the boundary may be answered incorrectly. Our algorithm is an extension of an algorithm of Baum (1990, 1991) that learns the intersection of two halfspaces whose bounding planes pass through the origin in the PAC-with-membership-queries model. We also describe algorithms for reaming several subclasses of monotone DNF formulas. (C) 1998 Academic Press.</p>","Computer Science, Theory & Methods"
"WOS:000074417400006","Learning with unreliable boundary queries","1998","
<p>We introduce a model for learning from examples and membership queries in situations where the boundary between positive and negative examples is somewhat iii-defined. In our model, queries near the boundary of a target concept may receive incorrect or ""don't care"" responses, and the distribution of examples has zero probability mass on the boundary region. The motivation behind our model is that in many cases the boundary between positive and negative examples is complicated or ""fuzzy."" However, one may still hope to learn successfully, because the typical examples that one sees do not come from that region, We present several positive results in this new model. We show how to learn the intersection of two arbitrary halfspaces when membership queries near the boundary may be answered incorrectly. Our algorithm is an extension of an algorithm of Baum (1990, 1991) that learns the intersection of two halfspaces whose bounding planes pass through the origin in the PAC-with-membership-queries model. We also describe algorithms for reaming several subclasses of monotone DNF formulas. (C) 1998 Academic Press.</p>","Computer Science"
"WOS:000074010900002","Panoramic stereo imaging system with automatic disparity warping and seaming","1998","
<p>Two commonly used approaches for building a virtual reality (VR) world are the model-based approach and the image-based approach. Recently, the image-based approach has received much attention for its advantages of being easier to build a VR model and of being able to provide photo-realistic views. However, traditional image-based VR systems cannot produce the stereo views that can give the users the feeling of 3D depth. In this paper, we present a panoramic stereo imaging (PSI) system which can produce stereo panoramas for image-based VR systems. This PSI system is referred to as the PSI-II system, which is an improved system of our previous experimental PSI-I system. The PSI-I system uses a well-calibrated tripod system to acquire a series of stereo image pairs, while the PSI-II system does not require the use of a well-calibrated tripod system and can automatically generate a stereo-pair of panoramic images by using a novel disparity warping technique and a hierarchical seaming algorithm. Our PSI-II system can automatically correct the epipolar-line inconsistency of the stereo images pairs and the image disparity caused by the dislocation of the camera's lens center in the image acquisition process. Our experiments have shown that the proposed method can easily provide realistic 360 degrees panoramic views for image-based VR systems. (C) 1998 Academic Press.</p>","Computer Science, Software Engineering"
"WOS:000074010900002","Panoramic stereo imaging system with automatic disparity warping and seaming","1998","
<p>Two commonly used approaches for building a virtual reality (VR) world are the model-based approach and the image-based approach. Recently, the image-based approach has received much attention for its advantages of being easier to build a VR model and of being able to provide photo-realistic views. However, traditional image-based VR systems cannot produce the stereo views that can give the users the feeling of 3D depth. In this paper, we present a panoramic stereo imaging (PSI) system which can produce stereo panoramas for image-based VR systems. This PSI system is referred to as the PSI-II system, which is an improved system of our previous experimental PSI-I system. The PSI-I system uses a well-calibrated tripod system to acquire a series of stereo image pairs, while the PSI-II system does not require the use of a well-calibrated tripod system and can automatically generate a stereo-pair of panoramic images by using a novel disparity warping technique and a hierarchical seaming algorithm. Our PSI-II system can automatically correct the epipolar-line inconsistency of the stereo images pairs and the image disparity caused by the dislocation of the camera's lens center in the image acquisition process. Our experiments have shown that the proposed method can easily provide realistic 360 degrees panoramic views for image-based VR systems. (C) 1998 Academic Press.</p>","Computer Science"
"WOS:000077413300004","Vehicle crash modelling using recurrent neural networks","1998","
<p>The initial velocity and structural characteristics of any vehicle are the main factors affecting the vehicle response in case of frontal impact. Finite Element (FE) simulations are essential tools for crashworthiness analysis, however, the FE models are getting bigger, which increases the simulation time and cost. In the current research, an advanced Artificial Neural Network (ANN) was used to store the nonlinear dynamic characteristics of the vehicle structure. Therefore, several impact scenarios can be analyzed quickly with much less computational cost by using the trained networks. The equation of motion of the dynamic system was used to define the inputs and outputs of the ANN. The system dynamics was included in the network performance and the recurrent back-propagation learning rule was adapted in training the network.</p>
<p>The results of the numerical examples indicated that the recurrent ANN can accurately capture the frontal crash characteristics of the impacting structures, and predict the crash performance of the same structures for any other crash scenario within the training limits. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000077413300004","Vehicle crash modelling using recurrent neural networks","1998","
<p>The initial velocity and structural characteristics of any vehicle are the main factors affecting the vehicle response in case of frontal impact. Finite Element (FE) simulations are essential tools for crashworthiness analysis, however, the FE models are getting bigger, which increases the simulation time and cost. In the current research, an advanced Artificial Neural Network (ANN) was used to store the nonlinear dynamic characteristics of the vehicle structure. Therefore, several impact scenarios can be analyzed quickly with much less computational cost by using the trained networks. The equation of motion of the dynamic system was used to define the inputs and outputs of the ANN. The system dynamics was included in the network performance and the recurrent back-propagation learning rule was adapted in training the network.</p>
<p>The results of the numerical examples indicated that the recurrent ANN can accurately capture the frontal crash characteristics of the impacting structures, and predict the crash performance of the same structures for any other crash scenario within the training limits. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science, Software Engineering"
"WOS:000077413300004","Vehicle crash modelling using recurrent neural networks","1998","
<p>The initial velocity and structural characteristics of any vehicle are the main factors affecting the vehicle response in case of frontal impact. Finite Element (FE) simulations are essential tools for crashworthiness analysis, however, the FE models are getting bigger, which increases the simulation time and cost. In the current research, an advanced Artificial Neural Network (ANN) was used to store the nonlinear dynamic characteristics of the vehicle structure. Therefore, several impact scenarios can be analyzed quickly with much less computational cost by using the trained networks. The equation of motion of the dynamic system was used to define the inputs and outputs of the ANN. The system dynamics was included in the network performance and the recurrent back-propagation learning rule was adapted in training the network.</p>
<p>The results of the numerical examples indicated that the recurrent ANN can accurately capture the frontal crash characteristics of the impacting structures, and predict the crash performance of the same structures for any other crash scenario within the training limits. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science"
"WOS:000075449700003","Transaction management mechanisms for active and real-time databases: A comprehensive protocol and a performance study","1998","
<p>Active and real-time databases (ARTDB) have a variety of applications in electronic brokerages in financial markets, stock trading, network management and manufacturing process control. Transaction processing (TP) in ARTDB is extremely complicated since transactions may trigger other real-time transactions to an arbitrary depth with various types of dependencies (coupling modes). Therefore, transaction processing must be cognizant of not only the time deadlines but also the types of semantic dependencies with other transactions. The conflict resolution between two transactions cannot be considered in isolation since affecting one transaction may affect every other semantically dependent transaction. Similarly, transaction scheduling needs to be compatible with the concurrency control to avoid unnecessary restarts. In this paper we argue that transaction pre-analysis using the pre-declaration paradigm is an efficient mechanism to integrate the various issues of transaction processing such as concurrency control, scheduling, and semantic dependencies. The pre-analysis is possible since in many applications transactions repeat from a set of transaction classes, and the conflicts can be easily determined at a logical level by partitioning relations into mutually exclusive subset (e.g., by stock-id in financial applications). We develop a pre-analysis based transaction processing mechanism called OCCWB. OCCWB is an extended optimistic concurrency control protocol with blocking that combines the benefits of both optimistic and lock based protocols. Such an approach also has an implicit overload management mechanism required in many applications. OCCWB consists of four phases, namely, transaction pre-analysis, serialization ordering, priority adjustment and priority wait. Our protocol is validated using simulation and is shown to outperform existing protocols under various workload and parameter settings. (C) 1998 Elsevier Science Inc. All rights reserved.</p>","Computer Science, Software Engineering"
"WOS:000075449700003","Transaction management mechanisms for active and real-time databases: A comprehensive protocol and a performance study","1998","
<p>Active and real-time databases (ARTDB) have a variety of applications in electronic brokerages in financial markets, stock trading, network management and manufacturing process control. Transaction processing (TP) in ARTDB is extremely complicated since transactions may trigger other real-time transactions to an arbitrary depth with various types of dependencies (coupling modes). Therefore, transaction processing must be cognizant of not only the time deadlines but also the types of semantic dependencies with other transactions. The conflict resolution between two transactions cannot be considered in isolation since affecting one transaction may affect every other semantically dependent transaction. Similarly, transaction scheduling needs to be compatible with the concurrency control to avoid unnecessary restarts. In this paper we argue that transaction pre-analysis using the pre-declaration paradigm is an efficient mechanism to integrate the various issues of transaction processing such as concurrency control, scheduling, and semantic dependencies. The pre-analysis is possible since in many applications transactions repeat from a set of transaction classes, and the conflicts can be easily determined at a logical level by partitioning relations into mutually exclusive subset (e.g., by stock-id in financial applications). We develop a pre-analysis based transaction processing mechanism called OCCWB. OCCWB is an extended optimistic concurrency control protocol with blocking that combines the benefits of both optimistic and lock based protocols. Such an approach also has an implicit overload management mechanism required in many applications. OCCWB consists of four phases, namely, transaction pre-analysis, serialization ordering, priority adjustment and priority wait. Our protocol is validated using simulation and is shown to outperform existing protocols under various workload and parameter settings. (C) 1998 Elsevier Science Inc. All rights reserved.</p>","Computer Science, Theory & Methods"
"WOS:000075449700003","Transaction management mechanisms for active and real-time databases: A comprehensive protocol and a performance study","1998","
<p>Active and real-time databases (ARTDB) have a variety of applications in electronic brokerages in financial markets, stock trading, network management and manufacturing process control. Transaction processing (TP) in ARTDB is extremely complicated since transactions may trigger other real-time transactions to an arbitrary depth with various types of dependencies (coupling modes). Therefore, transaction processing must be cognizant of not only the time deadlines but also the types of semantic dependencies with other transactions. The conflict resolution between two transactions cannot be considered in isolation since affecting one transaction may affect every other semantically dependent transaction. Similarly, transaction scheduling needs to be compatible with the concurrency control to avoid unnecessary restarts. In this paper we argue that transaction pre-analysis using the pre-declaration paradigm is an efficient mechanism to integrate the various issues of transaction processing such as concurrency control, scheduling, and semantic dependencies. The pre-analysis is possible since in many applications transactions repeat from a set of transaction classes, and the conflicts can be easily determined at a logical level by partitioning relations into mutually exclusive subset (e.g., by stock-id in financial applications). We develop a pre-analysis based transaction processing mechanism called OCCWB. OCCWB is an extended optimistic concurrency control protocol with blocking that combines the benefits of both optimistic and lock based protocols. Such an approach also has an implicit overload management mechanism required in many applications. OCCWB consists of four phases, namely, transaction pre-analysis, serialization ordering, priority adjustment and priority wait. Our protocol is validated using simulation and is shown to outperform existing protocols under various workload and parameter settings. (C) 1998 Elsevier Science Inc. All rights reserved.</p>","Computer Science"
"WOS:000082362300003","Corrected likelihood ratio tests for von Mises regression models","1998","
<p>We present, in matrix notation, a general Bartlett correction formula to improve likelihood ratio tests in von Mises regression models with concentration covariates. The formula is simple enough to be used analytically to obtain several closed-form Bartlett corrections in a variety of important tests. It also has advantages for numerical purposes. Finally, we apply our main result to a few special. models.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000082362300003","Corrected likelihood ratio tests for von Mises regression models","1998","
<p>We present, in matrix notation, a general Bartlett correction formula to improve likelihood ratio tests in von Mises regression models with concentration covariates. The formula is simple enough to be used analytically to obtain several closed-form Bartlett corrections in a variety of important tests. It also has advantages for numerical purposes. Finally, we apply our main result to a few special. models.</p>","Computer Science"
"WOS:000075476800006","An efficient parallel algorithm for maximum matching for some classes of graphs","1998","
<p>The P-4-tidy graphs were introduced by I. Rusu to generalize some already known classes of graphs with few induced P-4 (cographs, P-4-sparse graphs, P-4-lite graphs). Here, we propose an extension of R. Lin and S. Olariu's work (1994. J. Parallel Distributed Computing 22, 26-36.) on cographs, using the modular decomposition. As an application, we show how to obtain a maximum matching parallel algorithm for the family of P-4-tidy graphs (represented by a parse tree) in O(log n) time with O(n/log n) processors in the EREW-PRAM model with n-vertex graphs. (C) 1998 Academic Press, Inc.</p>","Computer Science, Theory & Methods"
"WOS:000075476800006","An efficient parallel algorithm for maximum matching for some classes of graphs","1998","
<p>The P-4-tidy graphs were introduced by I. Rusu to generalize some already known classes of graphs with few induced P-4 (cographs, P-4-sparse graphs, P-4-lite graphs). Here, we propose an extension of R. Lin and S. Olariu's work (1994. J. Parallel Distributed Computing 22, 26-36.) on cographs, using the modular decomposition. As an application, we show how to obtain a maximum matching parallel algorithm for the family of P-4-tidy graphs (represented by a parse tree) in O(log n) time with O(n/log n) processors in the EREW-PRAM model with n-vertex graphs. (C) 1998 Academic Press, Inc.</p>","Computer Science"
"WOS:000077564200016","Automated formal analysis of networks: FDR models of arbitrary topologies and flow-control mechanisms","1998","
<p>We present new techniques for formally modeling arbitrary network topologies and control-flow schemes, applicable to high-speed networks. A novel induction technique suitable for process algebraic, finite-state machine techniques is described which can be used to verify end-to-end properties of certain arbitrarily configured networks. We also present a formal model of an algorithm for regulating burstiness of network traffic, which incorporates discrete timing constraints. Our models are presented in CSP with automatic verification by FDR.</p>","Computer Science, Software Engineering"
"WOS:000077564200016","Automated formal analysis of networks: FDR models of arbitrary topologies and flow-control mechanisms","1998","
<p>We present new techniques for formally modeling arbitrary network topologies and control-flow schemes, applicable to high-speed networks. A novel induction technique suitable for process algebraic, finite-state machine techniques is described which can be used to verify end-to-end properties of certain arbitrarily configured networks. We also present a formal model of an algorithm for regulating burstiness of network traffic, which incorporates discrete timing constraints. Our models are presented in CSP with automatic verification by FDR.</p>","Computer Science, Theory & Methods"
"WOS:000077564200016","Automated formal analysis of networks: FDR models of arbitrary topologies and flow-control mechanisms","1998","
<p>We present new techniques for formally modeling arbitrary network topologies and control-flow schemes, applicable to high-speed networks. A novel induction technique suitable for process algebraic, finite-state machine techniques is described which can be used to verify end-to-end properties of certain arbitrarily configured networks. We also present a formal model of an algorithm for regulating burstiness of network traffic, which incorporates discrete timing constraints. Our models are presented in CSP with automatic verification by FDR.</p>","Computer Science"
"WOS:000082113200020","Formal representation of temporal items of the diagnostic and statistic manual of mental disorders - A description logic approach based on the CEN time standards for health care specific problems","1998","
<p>As a paradigm for abstract, nested time-references occurring in medical domain knowledge we analyze temporal diagnostic items of the Diagnostic and Statistic Manual of Mental Disorders (DSM-IV). Based on an elicitation of categories of DSM-IV time-references a formal approach to the representation of nested temporal references is proposed. Generic time-related concepts are introduced, which have to be specialized by concepts representing temporal patterns of the diagnostic items. Satisfiability of the knowledge base ensures the conformance to the Time Standards for health care specific problems fa CEN-prestandard).). Terminological inferences support the nosologic analysis of the DSM-IV classification.</p>","Computer Science, Artificial Intelligence"
"WOS:000082113200020","Formal representation of temporal items of the diagnostic and statistic manual of mental disorders - A description logic approach based on the CEN time standards for health care specific problems","1998","
<p>As a paradigm for abstract, nested time-references occurring in medical domain knowledge we analyze temporal diagnostic items of the Diagnostic and Statistic Manual of Mental Disorders (DSM-IV). Based on an elicitation of categories of DSM-IV time-references a formal approach to the representation of nested temporal references is proposed. Generic time-related concepts are introduced, which have to be specialized by concepts representing temporal patterns of the diagnostic items. Satisfiability of the knowledge base ensures the conformance to the Time Standards for health care specific problems fa CEN-prestandard).). Terminological inferences support the nosologic analysis of the DSM-IV classification.</p>","Computer Science"
"WOS:000071068300008","Recognizing patterns in information retrieval: a memory-based classifier for inferring relevancy","1998","
<p>An application of memory-based reasoning (MBR) for determining the relevancy of retrieved information, customized for the needs and preferences of an individual user, is presented. The use of MBR method in conjunction with a classification scheme such as k-nearest neighbors (k-NN) can set the foundation for an intelligent agent for search of patterns in databases. An intelligent agent is an adaptive database search system that can help reduce user information overload problem. To this end, a system called VINAYAK,(dagger) has been developed to investigate various issues concerned with information retrieval as an automated function. Current experiments have focused on chemical engineering literature search and safety-related retrieval through nuclear databases. The results point towards the advantage of the use of intelligent agent methodology in conjunction with a database search engine. (C) 1997 Elsevier Science Limited.</p>","Computer Science, Artificial Intelligence"
"WOS:000071068300008","Recognizing patterns in information retrieval: a memory-based classifier for inferring relevancy","1998","
<p>An application of memory-based reasoning (MBR) for determining the relevancy of retrieved information, customized for the needs and preferences of an individual user, is presented. The use of MBR method in conjunction with a classification scheme such as k-nearest neighbors (k-NN) can set the foundation for an intelligent agent for search of patterns in databases. An intelligent agent is an adaptive database search system that can help reduce user information overload problem. To this end, a system called VINAYAK,(dagger) has been developed to investigate various issues concerned with information retrieval as an automated function. Current experiments have focused on chemical engineering literature search and safety-related retrieval through nuclear databases. The results point towards the advantage of the use of intelligent agent methodology in conjunction with a database search engine. (C) 1997 Elsevier Science Limited.</p>","Computer Science"
"WOS:000077599700005","Selective write-update: A method to relax execution constraints in a critical section","1998","
<p>In a shared-memory multiprocessor. shared data are usually accessed in a critical section that is protected by a lock variable. Therefore, the order of accesses by multiple processors to the shared data corresponds to the order of acquiring the ownership of the lock variable. This paper presents a selective write-update protocol, where data modified in a critical section are stored in a write cache and, at a synchronization point, they are transferred only to the processor that will execute the critical section following the current processor. By using QOLB synchronization primitives, the next processor can be determined at the execution time. We prove that the selective write-update protocol ensures data coherency of parallel programs that comply with release consistency, and evaluate the performance of the protocol by analytical modeling and program-driven simulation. The simulation results show that our protocol can reduce the number of coherence misses in a critical section while avoiding the multicast of write-update requests on an interconnection network. In addition, we observe that synchronization latency can be decreased by reducing both the execution time of a critical section and the number of write-update requests. From the simulation results, it is shown that our protocol provides better performance than a write-invalidate protocol and a write-update protocol as the number of processors increases.</p>","Computer Science, Information Systems"
"WOS:000077599700005","Selective write-update: A method to relax execution constraints in a critical section","1998","
<p>In a shared-memory multiprocessor. shared data are usually accessed in a critical section that is protected by a lock variable. Therefore, the order of accesses by multiple processors to the shared data corresponds to the order of acquiring the ownership of the lock variable. This paper presents a selective write-update protocol, where data modified in a critical section are stored in a write cache and, at a synchronization point, they are transferred only to the processor that will execute the critical section following the current processor. By using QOLB synchronization primitives, the next processor can be determined at the execution time. We prove that the selective write-update protocol ensures data coherency of parallel programs that comply with release consistency, and evaluate the performance of the protocol by analytical modeling and program-driven simulation. The simulation results show that our protocol can reduce the number of coherence misses in a critical section while avoiding the multicast of write-update requests on an interconnection network. In addition, we observe that synchronization latency can be decreased by reducing both the execution time of a critical section and the number of write-update requests. From the simulation results, it is shown that our protocol provides better performance than a write-invalidate protocol and a write-update protocol as the number of processors increases.</p>","Computer Science, Software Engineering"
"WOS:000077599700005","Selective write-update: A method to relax execution constraints in a critical section","1998","
<p>In a shared-memory multiprocessor. shared data are usually accessed in a critical section that is protected by a lock variable. Therefore, the order of accesses by multiple processors to the shared data corresponds to the order of acquiring the ownership of the lock variable. This paper presents a selective write-update protocol, where data modified in a critical section are stored in a write cache and, at a synchronization point, they are transferred only to the processor that will execute the critical section following the current processor. By using QOLB synchronization primitives, the next processor can be determined at the execution time. We prove that the selective write-update protocol ensures data coherency of parallel programs that comply with release consistency, and evaluate the performance of the protocol by analytical modeling and program-driven simulation. The simulation results show that our protocol can reduce the number of coherence misses in a critical section while avoiding the multicast of write-update requests on an interconnection network. In addition, we observe that synchronization latency can be decreased by reducing both the execution time of a critical section and the number of write-update requests. From the simulation results, it is shown that our protocol provides better performance than a write-invalidate protocol and a write-update protocol as the number of processors increases.</p>","Computer Science"
"WOS:000071754900008","Resolution-based approach to compatibility analysis of interacting automata","1998","
<p>The problem of compatibility analysis of two interacting automata arises in the development of algorithms for reactive systems if partial nondeterministic automata serve as models for both the reactive system and its environment. Intuitively, two interacting automata are compatible if a signal from one automaton always induces a defined transition in the other. We present the method for solving this problem in the automated design of reactive system algorithms specified by formulas of a first-order monadic logic. Compatibility analysis is automatically performed both for initial specifications of the algorithm and its environment and for any specification of the same kind obtained as a result of changes made by the designer. So, there is a need in the development of methods for solving this problem at the level of logical specification. The methods based on the resolution principle proved to be very helpful for this purpose.</p>","Computer Science, Theory & Methods"
"WOS:000071754900008","Resolution-based approach to compatibility analysis of interacting automata","1998","
<p>The problem of compatibility analysis of two interacting automata arises in the development of algorithms for reactive systems if partial nondeterministic automata serve as models for both the reactive system and its environment. Intuitively, two interacting automata are compatible if a signal from one automaton always induces a defined transition in the other. We present the method for solving this problem in the automated design of reactive system algorithms specified by formulas of a first-order monadic logic. Compatibility analysis is automatically performed both for initial specifications of the algorithm and its environment and for any specification of the same kind obtained as a result of changes made by the designer. So, there is a need in the development of methods for solving this problem at the level of logical specification. The methods based on the resolution principle proved to be very helpful for this purpose.</p>","Computer Science"
"WOS:000075461200009","A virtual environment for the exploration of diffusion and flow phenomena in complex geometries","1998","
<p>With the development of high-performance computing techniques and the increase in availability of computer power, large and complex time dependent data sets are generated in simulations used in industrial and scientific applications. These data sets are not only large, but they also represent results of simulations of increasingly complex phenomena which often vary dynamically. In many cases, visual exploration of these complex data sets is one of the few options to analyze these data and to obtain further insight in the simulated phenomena.</p>
<p>As a test case in the development of an immersive virtual exploration environment we have used data sets resulting from 3D simulations of diffusion and flow processes, and their impact on biological growth. We show that an immersive virtual environment such as the CAVE aids in the interactive exploration of the large scale and time dependent data sets that result from these simulations. In addition, the CAVE has found to be a suitable environment to compare shapes emerging in simulated biological growth processes with data sets obtained from CT scans of the actual objects. (C) 1998 Elsevier Science B.V.</p>","Computer Science, Theory & Methods"
"WOS:000075461200009","A virtual environment for the exploration of diffusion and flow phenomena in complex geometries","1998","
<p>With the development of high-performance computing techniques and the increase in availability of computer power, large and complex time dependent data sets are generated in simulations used in industrial and scientific applications. These data sets are not only large, but they also represent results of simulations of increasingly complex phenomena which often vary dynamically. In many cases, visual exploration of these complex data sets is one of the few options to analyze these data and to obtain further insight in the simulated phenomena.</p>
<p>As a test case in the development of an immersive virtual exploration environment we have used data sets resulting from 3D simulations of diffusion and flow processes, and their impact on biological growth. We show that an immersive virtual environment such as the CAVE aids in the interactive exploration of the large scale and time dependent data sets that result from these simulations. In addition, the CAVE has found to be a suitable environment to compare shapes emerging in simulated biological growth processes with data sets obtained from CT scans of the actual objects. (C) 1998 Elsevier Science B.V.</p>","Computer Science"
"WOS:000074916200019","Separator-based sparsification - II: Edge and vertex connectivity","1998","
<p>We consider the problem of maintaining a dynamic planar graph subject to edge insertions and edge deletions that preserve planarity but that can change the embedding. We describe algorithms and data structures for maintaining information about 2- and 3-vertex-connectivity, and 3- and 4-edge-connectivity in a planar graph in O(n(1/2)) amortized time per insertion, deletion, or connectivity query. All of the data structures handle insertions that keep the graph planar without regard to any particular embedding of the graph. Our algorithms are based on a new type of sparsification combined with several properties of separators in planar graphs.</p>","Computer Science, Theory & Methods"
"WOS:000074916200019","Separator-based sparsification - II: Edge and vertex connectivity","1998","
<p>We consider the problem of maintaining a dynamic planar graph subject to edge insertions and edge deletions that preserve planarity but that can change the embedding. We describe algorithms and data structures for maintaining information about 2- and 3-vertex-connectivity, and 3- and 4-edge-connectivity in a planar graph in O(n(1/2)) amortized time per insertion, deletion, or connectivity query. All of the data structures handle insertions that keep the graph planar without regard to any particular embedding of the graph. Our algorithms are based on a new type of sparsification combined with several properties of separators in planar graphs.</p>","Computer Science"
"WOS:000076890000007","A probabilistic framework for memory-based reasoning","1998","
<p>In this paper, we propose a probabilistic framework for memory-based reasoning (MBR). The framework allows us to clarify the technical merits and limitations of several recently published MBR methods and to design new variants. The proposed computational framework consists of three components: a specification language to define an adaptive notion of relevant context for a query; mechanisms for retrieving this context; and local learning procedures that are used to induce the desired action from this context. We primarily focus on actions in the form of a classification. Based on the framework we derive several analytical and empirical results that shed light on MBR algorithms. We introduce the notion of an MBR transform, and discuss its utility for learning algorithms. We also provide several perspectives on memory-based reasoning from a multi-disciplinary point of view. (C) 1998 Published by Elsevier Science B.V.</p>","Computer Science, Artificial Intelligence"
"WOS:000076890000007","A probabilistic framework for memory-based reasoning","1998","
<p>In this paper, we propose a probabilistic framework for memory-based reasoning (MBR). The framework allows us to clarify the technical merits and limitations of several recently published MBR methods and to design new variants. The proposed computational framework consists of three components: a specification language to define an adaptive notion of relevant context for a query; mechanisms for retrieving this context; and local learning procedures that are used to induce the desired action from this context. We primarily focus on actions in the form of a classification. Based on the framework we derive several analytical and empirical results that shed light on MBR algorithms. We introduce the notion of an MBR transform, and discuss its utility for learning algorithms. We also provide several perspectives on memory-based reasoning from a multi-disciplinary point of view. (C) 1998 Published by Elsevier Science B.V.</p>","Computer Science"
"WOS:000078927600035","Highly nonlinear balanced Boolean functions with a good correlation-immunity","1998","
<p>We study a corpus of particular Boolean functions: the idempotents. They enable us to construct functions which achieve the best possible tradeoffs between the cryptographic fundamental properties: balancedness, correlation-immunity, a high degree and a high nonlinearity (that is a high distance from the affine functions). They all represent extremely secure cryptographic primitives to be implemented in stream ciphers.</p>","Computer Science, Theory & Methods"
"WOS:000078927600035","Highly nonlinear balanced Boolean functions with a good correlation-immunity","1998","
<p>We study a corpus of particular Boolean functions: the idempotents. They enable us to construct functions which achieve the best possible tradeoffs between the cryptographic fundamental properties: balancedness, correlation-immunity, a high degree and a high nonlinearity (that is a high distance from the affine functions). They all represent extremely secure cryptographic primitives to be implemented in stream ciphers.</p>","Computer Science"
"WOS:000085482900024","Arrays in Blitz plus","1998","
<p>The Blitz++ library provides numeric arrays for C++ with efficiency that rivals Fortran, without any language extensions. Blitz++ has features unavailable in Fortran 90/95, such as arbitrary transpose operations, array renaming, tensor notation, partial reductions, multicomponent arrays and stencil operators. The library handles parsing and analysis of array expressions on its own using the expression templates technique, and performs optimizations (such as loop transformations) which have until now been the responsibility of compilers.</p>","Computer Science, Theory & Methods"
"WOS:000085482900024","Arrays in Blitz plus","1998","
<p>The Blitz++ library provides numeric arrays for C++ with efficiency that rivals Fortran, without any language extensions. Blitz++ has features unavailable in Fortran 90/95, such as arbitrary transpose operations, array renaming, tensor notation, partial reductions, multicomponent arrays and stencil operators. The library handles parsing and analysis of array expressions on its own using the expression templates technique, and performs optimizations (such as loop transformations) which have until now been the responsibility of compilers.</p>","Computer Science"
"WOS:000083173400056","Tarskian set constraints are in NEXPTIME","1998","
<p>In this paper we show that satisfiability of Tarskian set constraints (without recursion) can be decided in exponential time. This doses the gap left open by D.A. McAllester, R. Givan, C. Witty and D. Kozen in [14].</p>","Computer Science, Theory & Methods"
"WOS:000083173400056","Tarskian set constraints are in NEXPTIME","1998","
<p>In this paper we show that satisfiability of Tarskian set constraints (without recursion) can be decided in exponential time. This doses the gap left open by D.A. McAllester, R. Givan, C. Witty and D. Kozen in [14].</p>","Computer Science"
"WOS:000072655500004","Decision support for managing organizational design dynamics","1998","
<p>Changes in the business environment and the advent of new Information Technologies (IT) create new challenges and opportunities for redesigning the organization. Organizational design encompasses a large number of factors including IT, incentives, business processes, and the scope of decision authority. Further, a design configuration based on a given business and technological environment may have to be modified with changes in the environment. We suggest using an approach called business value complementarity [8] as a systematic basis to ensure that design decisions contribute maximally to bottom-line performance measures such as profitability. It is based on the notion of creating a business value model showing relationships between key performance measures and design decision variables involving IT, incentives, etc. Complementarity between the variables in the business value model provides a basis for choosing the levels of design variables. We derive some results which specify conditions under which the overall performance measure(s) are complementary in the design variables. We suggest that a Decision Support System (DSS) would be appropriate for helping senior management in the complex process of developing and maintaining a business value model and in choosing a suitable organizational design, We also enumerate high level functional requirements of such a DSS. (C) 1998 Elsevier Science B.V.</p>","Computer Science, Artificial Intelligence"
"WOS:000072655500004","Decision support for managing organizational design dynamics","1998","
<p>Changes in the business environment and the advent of new Information Technologies (IT) create new challenges and opportunities for redesigning the organization. Organizational design encompasses a large number of factors including IT, incentives, business processes, and the scope of decision authority. Further, a design configuration based on a given business and technological environment may have to be modified with changes in the environment. We suggest using an approach called business value complementarity [8] as a systematic basis to ensure that design decisions contribute maximally to bottom-line performance measures such as profitability. It is based on the notion of creating a business value model showing relationships between key performance measures and design decision variables involving IT, incentives, etc. Complementarity between the variables in the business value model provides a basis for choosing the levels of design variables. We derive some results which specify conditions under which the overall performance measure(s) are complementary in the design variables. We suggest that a Decision Support System (DSS) would be appropriate for helping senior management in the complex process of developing and maintaining a business value model and in choosing a suitable organizational design, We also enumerate high level functional requirements of such a DSS. (C) 1998 Elsevier Science B.V.</p>","Computer Science, Information Systems"
"WOS:000072655500004","Decision support for managing organizational design dynamics","1998","
<p>Changes in the business environment and the advent of new Information Technologies (IT) create new challenges and opportunities for redesigning the organization. Organizational design encompasses a large number of factors including IT, incentives, business processes, and the scope of decision authority. Further, a design configuration based on a given business and technological environment may have to be modified with changes in the environment. We suggest using an approach called business value complementarity [8] as a systematic basis to ensure that design decisions contribute maximally to bottom-line performance measures such as profitability. It is based on the notion of creating a business value model showing relationships between key performance measures and design decision variables involving IT, incentives, etc. Complementarity between the variables in the business value model provides a basis for choosing the levels of design variables. We derive some results which specify conditions under which the overall performance measure(s) are complementary in the design variables. We suggest that a Decision Support System (DSS) would be appropriate for helping senior management in the complex process of developing and maintaining a business value model and in choosing a suitable organizational design, We also enumerate high level functional requirements of such a DSS. (C) 1998 Elsevier Science B.V.</p>","Computer Science"
"WOS:000074598100003","Lessons from characterizing the input/output behavior of parallel scientific applications","1998","
<p>As both processor and interprocessor communication hardware is evolving rapidly with only moderate improvements to file system performance in parallel systems, it is becoming increasingly difficult to provide sufficient input/output (I/O) performance to parallel applications. I/O hardware and file system parallelism are the key to bridging this performance gap. Prerequisite to the development of efficient parallel file systems is the detailed characterization of the I/O demands of parallel applications.</p>
<p>In the paper, we present a comparative study of parallel I/O access patterns, commonly found in I/O intensive scientific applications. The Pablo performance analysis tool and its I/O extensions is a valuable resource in capturing and analyzing the I/O access attributes and their interactions with extant parallel I/O systems. This analysis is instrumental in guiding the development of new application programming interfaces (APIs) for parallel file systems and effective file system policies that respond to complex application I/O requirements. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science, Hardware & Architecture"
"WOS:000074598100003","Lessons from characterizing the input/output behavior of parallel scientific applications","1998","
<p>As both processor and interprocessor communication hardware is evolving rapidly with only moderate improvements to file system performance in parallel systems, it is becoming increasingly difficult to provide sufficient input/output (I/O) performance to parallel applications. I/O hardware and file system parallelism are the key to bridging this performance gap. Prerequisite to the development of efficient parallel file systems is the detailed characterization of the I/O demands of parallel applications.</p>
<p>In the paper, we present a comparative study of parallel I/O access patterns, commonly found in I/O intensive scientific applications. The Pablo performance analysis tool and its I/O extensions is a valuable resource in capturing and analyzing the I/O access attributes and their interactions with extant parallel I/O systems. This analysis is instrumental in guiding the development of new application programming interfaces (APIs) for parallel file systems and effective file system policies that respond to complex application I/O requirements. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science, Theory & Methods"
"WOS:000074598100003","Lessons from characterizing the input/output behavior of parallel scientific applications","1998","
<p>As both processor and interprocessor communication hardware is evolving rapidly with only moderate improvements to file system performance in parallel systems, it is becoming increasingly difficult to provide sufficient input/output (I/O) performance to parallel applications. I/O hardware and file system parallelism are the key to bridging this performance gap. Prerequisite to the development of efficient parallel file systems is the detailed characterization of the I/O demands of parallel applications.</p>
<p>In the paper, we present a comparative study of parallel I/O access patterns, commonly found in I/O intensive scientific applications. The Pablo performance analysis tool and its I/O extensions is a valuable resource in capturing and analyzing the I/O access attributes and their interactions with extant parallel I/O systems. This analysis is instrumental in guiding the development of new application programming interfaces (APIs) for parallel file systems and effective file system policies that respond to complex application I/O requirements. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science"
"WOS:000076198500010","Intelligibility improvements obtained by an enhancement method applied to speech corrupted by noise and reverberation","1998","
<p>A series of psychoacoustic experiments are described, which attempt to assess the capability of a Multi-Microphone Sub-band Adaptive Signal (MMSBA) processing scheme, for improving the intelligibility of speech corrupted with noise and reverberation. The processing scheme applies the Least Mean Squares (LMS) adaptive algorithm in frequency delimited sub-bands to process speech signals from simulated and real room acoustic environments with various realistic signal to noise ratios (SNR). The processing scheme aims to take advantage of binaural input channels to perform noise cancellation. The two wide-band signals are split into linear or cochlear distributed sub-bands, then processed according to their sub-band signal characteristics. The results of a series of intelligibility tests are presented in which speech and noise data, generated in simulated and real room conditions, was presented to human volunteer subjects at various SNRs, sub-band distributions and sub-band spacings. The results from both simulated and real room acoustical environments show that the MMSBA processing scheme significantly improves both SNR and intelligibility. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000076198500010","Intelligibility improvements obtained by an enhancement method applied to speech corrupted by noise and reverberation","1998","
<p>A series of psychoacoustic experiments are described, which attempt to assess the capability of a Multi-Microphone Sub-band Adaptive Signal (MMSBA) processing scheme, for improving the intelligibility of speech corrupted with noise and reverberation. The processing scheme applies the Least Mean Squares (LMS) adaptive algorithm in frequency delimited sub-bands to process speech signals from simulated and real room acoustic environments with various realistic signal to noise ratios (SNR). The processing scheme aims to take advantage of binaural input channels to perform noise cancellation. The two wide-band signals are split into linear or cochlear distributed sub-bands, then processed according to their sub-band signal characteristics. The results of a series of intelligibility tests are presented in which speech and noise data, generated in simulated and real room conditions, was presented to human volunteer subjects at various SNRs, sub-band distributions and sub-band spacings. The results from both simulated and real room acoustical environments show that the MMSBA processing scheme significantly improves both SNR and intelligibility. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science"
"WOS:000075315300016","Real-time synthesis of sparsely interconnected neural associative memories","1998","
<p>The problem of implementing associative memories using sparsely interconnected generalized Brain-State-in-a-Box (gBSB) network is addressed in this paper. In particular, a ''designer'' neural network that synthesizes the associative memories is proposed. An upper bound on the time required for the designer network to reach a solution is determined. A neighborhood criterion with toroidal geometry for the cellular gBSB network is analyzed, in which the number of adjacent cells is independent of the generic cell location. A design method of neural associative memories with prespecified interconnecting weights is presented. The effectiveness of the proposed synthesis method is demonstrated with numerical examples. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science, Artificial Intelligence"
"WOS:000075315300016","Real-time synthesis of sparsely interconnected neural associative memories","1998","
<p>The problem of implementing associative memories using sparsely interconnected generalized Brain-State-in-a-Box (gBSB) network is addressed in this paper. In particular, a ''designer'' neural network that synthesizes the associative memories is proposed. An upper bound on the time required for the designer network to reach a solution is determined. A neighborhood criterion with toroidal geometry for the cellular gBSB network is analyzed, in which the number of adjacent cells is independent of the generic cell location. A design method of neural associative memories with prespecified interconnecting weights is presented. The effectiveness of the proposed synthesis method is demonstrated with numerical examples. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science"
"WOS:000074376500009","A neural model of contour integration in the primary visual cortex","1998","
<p>Experimental observations suggest that contour integration may take place in V1. However, there has yet to be a model of contour integration that uses only known V1 elements, operations, and connection patterns. This article introduces such a model, using orientation selective cells, local cortical circuits, and horizontal intracortical connections. The model is composed of recurrently connected excitatory neurons and inhibitory interneurons, receiving visual input via oriented receptive fields resembling those found in primary visual cortex. Intracortical interactions modify initial activity patterns from input, selectively amplifying the activities of edges that form smooth contours in the image. The neural activities produced by such interactions are oscillatory and edge segments within a contour oscillate in synchrony. It is shown analytically and empirically that the extent of contour enhancement and neural synchrony increases with the smoothness, length, and closure of contours, as observed in experiments on some of these phenomena. In addition, the model incorporates a feedback mechanism that allows higher visual centers selectively to enhance or suppress sensitivities to given contours, effectively segmenting one from another. The model makes the testable prediction that the horizontal cortical connections are more likely to target excitatory (or inhibitory) cells when the two linked cells have their preferred orientation aligned with (or orthogonal to) their relative receptive field center displacements.</p>","Computer Science, Artificial Intelligence"
"WOS:000074376500009","A neural model of contour integration in the primary visual cortex","1998","
<p>Experimental observations suggest that contour integration may take place in V1. However, there has yet to be a model of contour integration that uses only known V1 elements, operations, and connection patterns. This article introduces such a model, using orientation selective cells, local cortical circuits, and horizontal intracortical connections. The model is composed of recurrently connected excitatory neurons and inhibitory interneurons, receiving visual input via oriented receptive fields resembling those found in primary visual cortex. Intracortical interactions modify initial activity patterns from input, selectively amplifying the activities of edges that form smooth contours in the image. The neural activities produced by such interactions are oscillatory and edge segments within a contour oscillate in synchrony. It is shown analytically and empirically that the extent of contour enhancement and neural synchrony increases with the smoothness, length, and closure of contours, as observed in experiments on some of these phenomena. In addition, the model incorporates a feedback mechanism that allows higher visual centers selectively to enhance or suppress sensitivities to given contours, effectively segmenting one from another. The model makes the testable prediction that the horizontal cortical connections are more likely to target excitatory (or inhibitory) cells when the two linked cells have their preferred orientation aligned with (or orthogonal to) their relative receptive field center displacements.</p>","Computer Science"
"WOS:000077816400018","An algorithm for determining whether a space-time is homothetic","1998","
<p>We present an algorithm that determines whether or not a space-time admits a homothetic Killing vector. Various applications of the algorithm are demonstrated. (C) 1998 Published by Elsevier Science B.V.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000077816400018","An algorithm for determining whether a space-time is homothetic","1998","
<p>We present an algorithm that determines whether or not a space-time admits a homothetic Killing vector. Various applications of the algorithm are demonstrated. (C) 1998 Published by Elsevier Science B.V.</p>","Computer Science"
"WOS:000078291400003","Listening to music in the real world? A critical discussion of Marc Leman's (1995) Music and Schema Theory: Cognitive Foundations of Systematic Musicology","1998","
<p>Leman's recent book is examined from both musical and scientific perspectives. The book describes an important new development in the context-sensitive modeling of musical pitch perception, and a promising basis for the modeling of other musical parameters such as timbre, and even emotion. The main advantage of the approach is its ecological validity: as far as current limitations of knowledge and technology allow, the model realistically simulates, or attempts to simulate, various peripheral and central parts of human auditory physiology; and the input to the model is not the artificial constructs of music theory or even notated music, but real, sounding music. Leman describes how schema of musical pitch perception can develop by listening to music, and how they may consequently be used to monitor the tonality of real pieces. A major unresolved problem is that, according to the various simulations presented in the book, the more complex models do not necessarily perform better than the simpler models when predictions are compared against experimental data. Thus, the book does not always present a strong case for the realism and usefulness of the more complex models.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000078291400003","Listening to music in the real world? A critical discussion of Marc Leman's (1995) Music and Schema Theory: Cognitive Foundations of Systematic Musicology","1998","
<p>Leman's recent book is examined from both musical and scientific perspectives. The book describes an important new development in the context-sensitive modeling of musical pitch perception, and a promising basis for the modeling of other musical parameters such as timbre, and even emotion. The main advantage of the approach is its ecological validity: as far as current limitations of knowledge and technology allow, the model realistically simulates, or attempts to simulate, various peripheral and central parts of human auditory physiology; and the input to the model is not the artificial constructs of music theory or even notated music, but real, sounding music. Leman describes how schema of musical pitch perception can develop by listening to music, and how they may consequently be used to monitor the tonality of real pieces. A major unresolved problem is that, according to the various simulations presented in the book, the more complex models do not necessarily perform better than the simpler models when predictions are compared against experimental data. Thus, the book does not always present a strong case for the realism and usefulness of the more complex models.</p>","Computer Science"
"WOS:000076016500003","NUTS: A distributed object-oriented platform with high level communication functions","1998","
<p>An extensible object-oriented platform NUTS for distributed computing is described which is based on an object-oriented programming environment NUT which supports automatic synthesis of programs. It is built on top of the Parallel Virtual Machine (PVM), and hides all low-level features of the latter. The language of NUTS is a concurrent object-oriented programming language with coarse-grained parallelism and distributed shared memory communication model implemented on a distributed memory architecture. It differs from other languages of concurrent programming in the following: concurrent processes are represented by packages which are semantically richer entities than objects, inter-process communication is performed in terms of classes, objects, scripts and packages, using the EDA communication model; processes can be arranged into structured collections: grids which enable one to program data-parallel computations on a high level; sequential segments of programs can be synthesized automatically from specifications represented as classes using the program synthesis features of NUT. Examples of usage of generic parallel computing control structures PARDIF and PARARR are given.</p>","Computer Science, Artificial Intelligence"
"WOS:000076016500003","NUTS: A distributed object-oriented platform with high level communication functions","1998","
<p>An extensible object-oriented platform NUTS for distributed computing is described which is based on an object-oriented programming environment NUT which supports automatic synthesis of programs. It is built on top of the Parallel Virtual Machine (PVM), and hides all low-level features of the latter. The language of NUTS is a concurrent object-oriented programming language with coarse-grained parallelism and distributed shared memory communication model implemented on a distributed memory architecture. It differs from other languages of concurrent programming in the following: concurrent processes are represented by packages which are semantically richer entities than objects, inter-process communication is performed in terms of classes, objects, scripts and packages, using the EDA communication model; processes can be arranged into structured collections: grids which enable one to program data-parallel computations on a high level; sequential segments of programs can be synthesized automatically from specifications represented as classes using the program synthesis features of NUT. Examples of usage of generic parallel computing control structures PARDIF and PARARR are given.</p>","Computer Science"
"WOS:000073455100174","Fuzzy modeling and flowsheeting of imprecise units","1998","
<p>Precise mathematical models are not always available for some process units which are too complicated to be understood clearly in mechanism and variable relationship. These process units are difficult to be analyzed and simulated in conventional approaches. A rule based approach is proposed in this paper to model the fuzzy units. Conversion between fuzzy and crisp numbers at the input and output of fuzzy units is discussed in details and a slicing approach for conversion is proposed A case study shows that this approach provides a way to incorporate fuzzy models into conventional process simulators. (C) 1998 Published by Elsevier Science Ltd. All rights reserved.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000073455100174","Fuzzy modeling and flowsheeting of imprecise units","1998","
<p>Precise mathematical models are not always available for some process units which are too complicated to be understood clearly in mechanism and variable relationship. These process units are difficult to be analyzed and simulated in conventional approaches. A rule based approach is proposed in this paper to model the fuzzy units. Conversion between fuzzy and crisp numbers at the input and output of fuzzy units is discussed in details and a slicing approach for conversion is proposed A case study shows that this approach provides a way to incorporate fuzzy models into conventional process simulators. (C) 1998 Published by Elsevier Science Ltd. All rights reserved.</p>","Computer Science"
"WOS:000077612700041","Neural networks of data inhibiting long memory pattern","1998","
<p>We experiment with three neural network models for forecasting to better understand the performance of neural networks for the case when the data exhibits a long memory pattern. To obtain the optimum networks, the effect of network characteristics such as the training parameters, the number of hidden layers, and the testing and training percentages are simulated The third model, which consists of a combination of individual time series forecasts, provides superior results. (C) 1998 Published by Elsevier Science Ltd. All rights reserved.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000077612700041","Neural networks of data inhibiting long memory pattern","1998","
<p>We experiment with three neural network models for forecasting to better understand the performance of neural networks for the case when the data exhibits a long memory pattern. To obtain the optimum networks, the effect of network characteristics such as the training parameters, the number of hidden layers, and the testing and training percentages are simulated The third model, which consists of a combination of individual time series forecasts, provides superior results. (C) 1998 Published by Elsevier Science Ltd. All rights reserved.</p>","Computer Science"
"WOS:000082774600027","Towards performance evaluation with general distributions in process algebras","1998","
<p>We present a process algebra for the performance modeling and evaluation of concurrent systems whose activity durations are expressed through general probability distributions. We first determine the class of generalized semi-Markov processes (GSMPs) as being the class of stochastic processes on which we must rely for performance evaluation to be possible. Then we argue that in this context the right semantics for algebraic terms is a variant of the ST semantics which accounts for both functional and performance aspects. The GSMP based process algebra we propose is introduced together with its formal semantics, an example of performance evaluation, and a notion of probabilistic bisimulation based equivalence accounting for action durations which is shown to be a congruence.</p>","Computer Science, Theory & Methods"
"WOS:000082774600027","Towards performance evaluation with general distributions in process algebras","1998","
<p>We present a process algebra for the performance modeling and evaluation of concurrent systems whose activity durations are expressed through general probability distributions. We first determine the class of generalized semi-Markov processes (GSMPs) as being the class of stochastic processes on which we must rely for performance evaluation to be possible. Then we argue that in this context the right semantics for algebraic terms is a variant of the ST semantics which accounts for both functional and performance aspects. The GSMP based process algebra we propose is introduced together with its formal semantics, an example of performance evaluation, and a notion of probabilistic bisimulation based equivalence accounting for action durations which is shown to be a congruence.</p>","Computer Science"
"WOS:000076944300007","A method for analysing neural computation using receptive fields in state space","1998","
<p>The behaviour of spiking neurons that are involved in a control task can be quantified by mapping receptive fields in the state space of the control problem. These receptive fields link spikes, the operands of neural computation, to state variables, the operands of conventional control theory. They allow neural computation underlying control tasks to be quantitatively analysed, and meaningfully discussed in ordinary language, by providing a rigorous way to interpret single spikes as assertions about dynamical state. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science, Artificial Intelligence"
"WOS:000076944300007","A method for analysing neural computation using receptive fields in state space","1998","
<p>The behaviour of spiking neurons that are involved in a control task can be quantified by mapping receptive fields in the state space of the control problem. These receptive fields link spikes, the operands of neural computation, to state variables, the operands of conventional control theory. They allow neural computation underlying control tasks to be quantitatively analysed, and meaningfully discussed in ordinary language, by providing a rigorous way to interpret single spikes as assertions about dynamical state. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science"
"WOS:000072114100008","Secret sets and applications","1998","
<p>This paper introduces the notion of a secret set - a basic construct for communication with groups of mutually suspicious entities. A set is secret if any entity can test its membership in the set but can determine neither the other set members nor the cardinality of the set. A number of possible secret set constructions are presented, analyzed and contrasted according to criteria such as: security (strength) as well as bandwidth and processing overheads. Example applications of secret sets are discussed. (C) 1998 Elsevier Science B.V.</p>","Computer Science, Information Systems"
"WOS:000072114100008","Secret sets and applications","1998","
<p>This paper introduces the notion of a secret set - a basic construct for communication with groups of mutually suspicious entities. A set is secret if any entity can test its membership in the set but can determine neither the other set members nor the cardinality of the set. A number of possible secret set constructions are presented, analyzed and contrasted according to criteria such as: security (strength) as well as bandwidth and processing overheads. Example applications of secret sets are discussed. (C) 1998 Elsevier Science B.V.</p>","Computer Science"
"WOS:000078045800003","A PDES/STEP-based information model for computer-aided process planning","1998","
<p>Computer Aided Process Planning (CAPP) constitutes one of the most essential elements in Computer Integrated Manufacturing (CIM). Although many CAPP systems have been reported in literature during the last two decades, few of them are compatible enough to integrate easily with other systems in the CIM environment. One major reason is the lack of an effective method to represent the information required by CAPP, and to unify such information with the information of other systems in the CIM environment. Indeed, this problem has received relatively inadequate attention in the recent research of CAPP systems. In this paper, an information model for CAPP is developed by using the object-oriented modeling and the Product Data Exchange Step/STandard of Exchange Product data (PDES/STEP) techniques. The model consists of the part information model, the process plan information model, and the production resource information model. The EXPRESS language or the EXPRESS-G diagram is used to represent these models. Indeed, the proposed information model will greatly improve the CAPP system's capability of effective integration with other systems in the CIM environment, and, ultimately, to facilitate the implementation of the whole CIM strategy in manufacturing enterprises. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000078045800003","A PDES/STEP-based information model for computer-aided process planning","1998","
<p>Computer Aided Process Planning (CAPP) constitutes one of the most essential elements in Computer Integrated Manufacturing (CIM). Although many CAPP systems have been reported in literature during the last two decades, few of them are compatible enough to integrate easily with other systems in the CIM environment. One major reason is the lack of an effective method to represent the information required by CAPP, and to unify such information with the information of other systems in the CIM environment. Indeed, this problem has received relatively inadequate attention in the recent research of CAPP systems. In this paper, an information model for CAPP is developed by using the object-oriented modeling and the Product Data Exchange Step/STandard of Exchange Product data (PDES/STEP) techniques. The model consists of the part information model, the process plan information model, and the production resource information model. The EXPRESS language or the EXPRESS-G diagram is used to represent these models. Indeed, the proposed information model will greatly improve the CAPP system's capability of effective integration with other systems in the CIM environment, and, ultimately, to facilitate the implementation of the whole CIM strategy in manufacturing enterprises. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science"
"WOS:000076904900009","Metacomputing in gigabit environments: Networks, tools, and applications","1998","
<p>This article gives an overview over recent and current metacomputing activities of the Computing Centers at the Forschungszentrum Julich, the GMD Forschungszentrum Informationstechnik and the University of Stuttgart. It starts with a discussion of the underlying network connections which are dedicated testbeds. A library that provides an MPI-API for metacomputing applications is presented, as well as a library that supports load-balancing and latency hiding, Finally, results from several applications ranging from tightly coupled homogeneous to loosely coupled heterogeneous metacomputing are presented. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science, Theory & Methods"
"WOS:000076904900009","Metacomputing in gigabit environments: Networks, tools, and applications","1998","
<p>This article gives an overview over recent and current metacomputing activities of the Computing Centers at the Forschungszentrum Julich, the GMD Forschungszentrum Informationstechnik and the University of Stuttgart. It starts with a discussion of the underlying network connections which are dedicated testbeds. A library that provides an MPI-API for metacomputing applications is presented, as well as a library that supports load-balancing and latency hiding, Finally, results from several applications ranging from tightly coupled homogeneous to loosely coupled heterogeneous metacomputing are presented. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science"
"WOS:000074747600002","Standard based framework for the development of manufacturing control systems","1998","
<p>A Manufacturing Control Systems CASE tool, called MCSTOOLS, has been developed with the aim of reducing the effort needed to produce this kind of software. The control systems developed by means of this tool follow the Control Systems for Integrated Manufacturing (COSIMA conceptual model (ESPRIT 477 project). In order to minimize the development time of Manufacturing Control Systems (MCS), this CASE tool automatically generates as much software as possible. In addition, the software produced by the MCSTOOLS is highly reusable, as it conforms to a widely accepted software engineering method-Object Oriented Analysis/Design/Programming (OOA/D/P-and several manufacturing and communications standards. Among these standards the most relevant are; Product Data Representation and Exchange (also known as STEP, ISO 10303), Manufacturing Message Specification (MMS, ISO 9506) and the Common Object Request Broker Architecture (CORBA). In order to achieve the objective of producing MCSs able to be adapted to a wide range of manufacturing environments, several techniques have been incorporated into the software generated by the CASE tool. The most important of these techniques are: object oriented distributed computing, object oriented multithreading computing and object persistence. In addition, the software produced includes a Coloured Petri Net based state control module and a generic user interface written in Java. The MCS CASE tool developed can be used in different ways depending on the number of new system elements to be controlled and the complexity of this system. So in some situations it is possible to generate the control software automatically without manual assistance by using the graphical user interface of MCSTOOLS. In other cases it is also necessary to modify or to develop software using high level directives and macro language constructs included in the CASE tool. Whereas for the remainder of cases, as for other systems, it is necessary to modify or develop C++ software. When needed, new source code is developed out of a skeleton generated by the CASE tool.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000074747600002","Standard based framework for the development of manufacturing control systems","1998","
<p>A Manufacturing Control Systems CASE tool, called MCSTOOLS, has been developed with the aim of reducing the effort needed to produce this kind of software. The control systems developed by means of this tool follow the Control Systems for Integrated Manufacturing (COSIMA conceptual model (ESPRIT 477 project). In order to minimize the development time of Manufacturing Control Systems (MCS), this CASE tool automatically generates as much software as possible. In addition, the software produced by the MCSTOOLS is highly reusable, as it conforms to a widely accepted software engineering method-Object Oriented Analysis/Design/Programming (OOA/D/P-and several manufacturing and communications standards. Among these standards the most relevant are; Product Data Representation and Exchange (also known as STEP, ISO 10303), Manufacturing Message Specification (MMS, ISO 9506) and the Common Object Request Broker Architecture (CORBA). In order to achieve the objective of producing MCSs able to be adapted to a wide range of manufacturing environments, several techniques have been incorporated into the software generated by the CASE tool. The most important of these techniques are: object oriented distributed computing, object oriented multithreading computing and object persistence. In addition, the software produced includes a Coloured Petri Net based state control module and a generic user interface written in Java. The MCS CASE tool developed can be used in different ways depending on the number of new system elements to be controlled and the complexity of this system. So in some situations it is possible to generate the control software automatically without manual assistance by using the graphical user interface of MCSTOOLS. In other cases it is also necessary to modify or to develop software using high level directives and macro language constructs included in the CASE tool. Whereas for the remainder of cases, as for other systems, it is necessary to modify or develop C++ software. When needed, new source code is developed out of a skeleton generated by the CASE tool.</p>","Computer Science"
"WOS:000076354600003","Strategies for simultaneous multiple autonomous underwater vehicle operation and control","1998","
<p>Undersampling of the coastal oceans remains a persistent problem for standard oceanographic measurement practice wherein an instrument package is tethered to a research vessel. The overhead costs associated,with operating a large research vessel impose a strict minimum on the cost of data collected Owing to the overheads, significant improvements in sampling technology on the tethered platform can only produce modest gains in the cost effectiveness. In contrast, untethered vehicles if operated simultaneously! have the potential to increase cost effectiveness significantly by distributing the overhead costs over several sampling platforms. Furthermore, synoptic and pseudosynoptic data can be collected with multiple autonomous underwater vehicles (AUVs), thereby providing the type of information critical to dynamic process modeling unattainable with non-synoptic data. While the goal of simultaneous multiple-vehicle operation has been espoused over the last few years, A UV technology and practice have until recently beer? too immature to realize that potential. Recently, Florida Atlantic University (FAU) has developed a new series of modular AUT's with the express purpose of supporting multiple sensors and multiple-vehicle operation. This series of vehicle is called the Ocean Explorer of which three have been produced so far. This paper will explore some of the associated navigation, tracking, control and deployment problems associated with multiple-vehicle operation in coastal applications. In addition, the characteristics of the component level intelligent distributed control system, integrated data logger and vehicle control system will be discussed. III particular this paper will discuss how FAU has applied the concepts of elastic constraint propagation and the symmetric fuzzy decision-making model to AUV control systems. Some results of early experiments in synoptic data collection with a conductivity, temperature and depth (CTD) sensor using multiple AUVs for the determination of horizontal structure will be described.</p>","Computer Science, Theory & Methods"
"WOS:000076354600003","Strategies for simultaneous multiple autonomous underwater vehicle operation and control","1998","
<p>Undersampling of the coastal oceans remains a persistent problem for standard oceanographic measurement practice wherein an instrument package is tethered to a research vessel. The overhead costs associated,with operating a large research vessel impose a strict minimum on the cost of data collected Owing to the overheads, significant improvements in sampling technology on the tethered platform can only produce modest gains in the cost effectiveness. In contrast, untethered vehicles if operated simultaneously! have the potential to increase cost effectiveness significantly by distributing the overhead costs over several sampling platforms. Furthermore, synoptic and pseudosynoptic data can be collected with multiple autonomous underwater vehicles (AUVs), thereby providing the type of information critical to dynamic process modeling unattainable with non-synoptic data. While the goal of simultaneous multiple-vehicle operation has been espoused over the last few years, A UV technology and practice have until recently beer? too immature to realize that potential. Recently, Florida Atlantic University (FAU) has developed a new series of modular AUT's with the express purpose of supporting multiple sensors and multiple-vehicle operation. This series of vehicle is called the Ocean Explorer of which three have been produced so far. This paper will explore some of the associated navigation, tracking, control and deployment problems associated with multiple-vehicle operation in coastal applications. In addition, the characteristics of the component level intelligent distributed control system, integrated data logger and vehicle control system will be discussed. III particular this paper will discuss how FAU has applied the concepts of elastic constraint propagation and the symmetric fuzzy decision-making model to AUV control systems. Some results of early experiments in synoptic data collection with a conductivity, temperature and depth (CTD) sensor using multiple AUVs for the determination of horizontal structure will be described.</p>","Computer Science"
"WOS:000077563800004","Analysis of a guard condition in type theory (extended abstract)","1998","
<p>We present a realizability interpretation of co-inductive types based on partial equivalence relations (per's). We extract from the per's interpretation sound rules to type recursive definitions. These recursive definitions are needed to introduce 'infinite' and 'total' objects of co-inductive type such as an infinite stream, a digital transducer, or a nonterminating process. We show that the proposed type system subsumes those studied by Coquand and Gimenez while still enjoying the basic syntactic properties of subject reduction and strong normalization with respect to a confluent rewriting system first put forward by Gimenez.</p>","Computer Science, Software Engineering"
"WOS:000077563800004","Analysis of a guard condition in type theory (extended abstract)","1998","
<p>We present a realizability interpretation of co-inductive types based on partial equivalence relations (per's). We extract from the per's interpretation sound rules to type recursive definitions. These recursive definitions are needed to introduce 'infinite' and 'total' objects of co-inductive type such as an infinite stream, a digital transducer, or a nonterminating process. We show that the proposed type system subsumes those studied by Coquand and Gimenez while still enjoying the basic syntactic properties of subject reduction and strong normalization with respect to a confluent rewriting system first put forward by Gimenez.</p>","Computer Science, Theory & Methods"
"WOS:000077563800004","Analysis of a guard condition in type theory (extended abstract)","1998","
<p>We present a realizability interpretation of co-inductive types based on partial equivalence relations (per's). We extract from the per's interpretation sound rules to type recursive definitions. These recursive definitions are needed to introduce 'infinite' and 'total' objects of co-inductive type such as an infinite stream, a digital transducer, or a nonterminating process. We show that the proposed type system subsumes those studied by Coquand and Gimenez while still enjoying the basic syntactic properties of subject reduction and strong normalization with respect to a confluent rewriting system first put forward by Gimenez.</p>","Computer Science"
"WOS:000087841800035","Modeling of nonlinear friction in complex mechanisms using spectral analysis","1998","
<p>In this paper a new method of modeling nonlinear friction as a function of both position and velocity is introduced. The proposed empirical model uses spectral analysis to identify the main sources of position dependent friction in complex mechanisms. In addition, the model describes the contribution to the overall friction of every moving part of the mechanism. In the paper, the proposed spectral-based modeling technique is used to describe static friction and sliding friction (which includes negative damping friction) of the first joint of PUElrIA 560 robot. This is the first friction model truly capable of describing nonlinear friction in complex mechanical systems.</p>","Computer Science, Information Systems"
"WOS:000087841800035","Modeling of nonlinear friction in complex mechanisms using spectral analysis","1998","
<p>In this paper a new method of modeling nonlinear friction as a function of both position and velocity is introduced. The proposed empirical model uses spectral analysis to identify the main sources of position dependent friction in complex mechanisms. In addition, the model describes the contribution to the overall friction of every moving part of the mechanism. In the paper, the proposed spectral-based modeling technique is used to describe static friction and sliding friction (which includes negative damping friction) of the first joint of PUElrIA 560 robot. This is the first friction model truly capable of describing nonlinear friction in complex mechanical systems.</p>","Computer Science"
"WOS:000082482800077","Contribution to better handling of irregular problems in HPF2","1998","
<p>In this paper, we present our contribution for handling irregular applications with HPF2. We propose a programming style of irregular applications close to the regular case, so that both compile-time and run-time techniques can be more easily performed. We use the well-known tree data structure to represent irregular data structures with hierarchical access, such as sparse matrices. This algorithmic representation avoids the indirections coming from the standard irregular programming style. We use derived data types of Fortran 90 to define trees and some approved extensions of HPF2 for their mapping. We also propose a run-time support for irregular applications with loop-carried dependencies that cannot be determined at compile-time. Then, we present the TriDenT library, which supports distributed trees and provides runtime optimizations based on the inspector/executor paradigm. Finally, we validate our contribution with experimental results on IBM SP2 for a sparse Cholesky factorization algorithm.</p>","Computer Science, Hardware & Architecture"
"WOS:000082482800077","Contribution to better handling of irregular problems in HPF2","1998","
<p>In this paper, we present our contribution for handling irregular applications with HPF2. We propose a programming style of irregular applications close to the regular case, so that both compile-time and run-time techniques can be more easily performed. We use the well-known tree data structure to represent irregular data structures with hierarchical access, such as sparse matrices. This algorithmic representation avoids the indirections coming from the standard irregular programming style. We use derived data types of Fortran 90 to define trees and some approved extensions of HPF2 for their mapping. We also propose a run-time support for irregular applications with loop-carried dependencies that cannot be determined at compile-time. Then, we present the TriDenT library, which supports distributed trees and provides runtime optimizations based on the inspector/executor paradigm. Finally, we validate our contribution with experimental results on IBM SP2 for a sparse Cholesky factorization algorithm.</p>","Computer Science, Information Systems"
"WOS:000082482800077","Contribution to better handling of irregular problems in HPF2","1998","
<p>In this paper, we present our contribution for handling irregular applications with HPF2. We propose a programming style of irregular applications close to the regular case, so that both compile-time and run-time techniques can be more easily performed. We use the well-known tree data structure to represent irregular data structures with hierarchical access, such as sparse matrices. This algorithmic representation avoids the indirections coming from the standard irregular programming style. We use derived data types of Fortran 90 to define trees and some approved extensions of HPF2 for their mapping. We also propose a run-time support for irregular applications with loop-carried dependencies that cannot be determined at compile-time. Then, we present the TriDenT library, which supports distributed trees and provides runtime optimizations based on the inspector/executor paradigm. Finally, we validate our contribution with experimental results on IBM SP2 for a sparse Cholesky factorization algorithm.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000082482800077","Contribution to better handling of irregular problems in HPF2","1998","
<p>In this paper, we present our contribution for handling irregular applications with HPF2. We propose a programming style of irregular applications close to the regular case, so that both compile-time and run-time techniques can be more easily performed. We use the well-known tree data structure to represent irregular data structures with hierarchical access, such as sparse matrices. This algorithmic representation avoids the indirections coming from the standard irregular programming style. We use derived data types of Fortran 90 to define trees and some approved extensions of HPF2 for their mapping. We also propose a run-time support for irregular applications with loop-carried dependencies that cannot be determined at compile-time. Then, we present the TriDenT library, which supports distributed trees and provides runtime optimizations based on the inspector/executor paradigm. Finally, we validate our contribution with experimental results on IBM SP2 for a sparse Cholesky factorization algorithm.</p>","Computer Science, Software Engineering"
"WOS:000082482800077","Contribution to better handling of irregular problems in HPF2","1998","
<p>In this paper, we present our contribution for handling irregular applications with HPF2. We propose a programming style of irregular applications close to the regular case, so that both compile-time and run-time techniques can be more easily performed. We use the well-known tree data structure to represent irregular data structures with hierarchical access, such as sparse matrices. This algorithmic representation avoids the indirections coming from the standard irregular programming style. We use derived data types of Fortran 90 to define trees and some approved extensions of HPF2 for their mapping. We also propose a run-time support for irregular applications with loop-carried dependencies that cannot be determined at compile-time. Then, we present the TriDenT library, which supports distributed trees and provides runtime optimizations based on the inspector/executor paradigm. Finally, we validate our contribution with experimental results on IBM SP2 for a sparse Cholesky factorization algorithm.</p>","Computer Science, Theory & Methods"
"WOS:000082482800077","Contribution to better handling of irregular problems in HPF2","1998","
<p>In this paper, we present our contribution for handling irregular applications with HPF2. We propose a programming style of irregular applications close to the regular case, so that both compile-time and run-time techniques can be more easily performed. We use the well-known tree data structure to represent irregular data structures with hierarchical access, such as sparse matrices. This algorithmic representation avoids the indirections coming from the standard irregular programming style. We use derived data types of Fortran 90 to define trees and some approved extensions of HPF2 for their mapping. We also propose a run-time support for irregular applications with loop-carried dependencies that cannot be determined at compile-time. Then, we present the TriDenT library, which supports distributed trees and provides runtime optimizations based on the inspector/executor paradigm. Finally, we validate our contribution with experimental results on IBM SP2 for a sparse Cholesky factorization algorithm.</p>","Computer Science"
"WOS:000073400800006","A study on interleaving versus segmentation","1998","
<p>Many algorithms process an array by dividing it into smaller subarrays. The most commonly used dividing strategy is segmentation. Though somewhat unintuitive and thus less commonly used, interleaving can be advantageous. We will show that interleaving is far superior to segmentation in the context of a parallel quicksort algorithm. (C) 1998 Published by Elsevier Science B.V.</p>","Computer Science, Information Systems"
"WOS:000073400800006","A study on interleaving versus segmentation","1998","
<p>Many algorithms process an array by dividing it into smaller subarrays. The most commonly used dividing strategy is segmentation. Though somewhat unintuitive and thus less commonly used, interleaving can be advantageous. We will show that interleaving is far superior to segmentation in the context of a parallel quicksort algorithm. (C) 1998 Published by Elsevier Science B.V.</p>","Computer Science"
"WOS:000074341700004","Extension principles for fuzzy set theory","1998","
<p>In several cases we show that it is possible to extend a notion in classical mathematics by identifying each fuzzy subset with the continuous chain of its closed cuts and by applying this notion to these cuts. In particular this idea is applied to extend functions from subsets into subsets (for instance, closure operators) and functions from sets into real numbers (for instance, measures). (C) 1998 Elsevier Science Inc. All rights reserved.</p>","Computer Science, Information Systems"
"WOS:000074341700004","Extension principles for fuzzy set theory","1998","
<p>In several cases we show that it is possible to extend a notion in classical mathematics by identifying each fuzzy subset with the continuous chain of its closed cuts and by applying this notion to these cuts. In particular this idea is applied to extend functions from subsets into subsets (for instance, closure operators) and functions from sets into real numbers (for instance, measures). (C) 1998 Elsevier Science Inc. All rights reserved.</p>","Computer Science"
"WOS:000077860900001","Complex domain distillation calculations","1998","
<p>The phrase ""complex column"" is quite widely used in the literature on distillation and usually is intended to conjure up images of distillation columns with multiple feeds, sidedraws, pumparounds, and sidestrippers. We prefer to classify these operations as ""complicated columns."" In this paper, the phrase ""complex distillation"" refers to distillation calculations carried out in the complex domain.</p>
<p>Lucia and Xu (""Global Minima in Root Finding"", In Recent Advances in Global Optimization, C.A. Floudas and P.M. Pardalos, Eds., Princeton Univ. Press, 543, 1992) studied the complex domain behavior of various fixed-point methods for solving the Soave-Redlich-Kwong equation of state and show that the bifurcation of a pair of roots into the complex plane coincide with a phase transition. They also show that the dogleg strategy of Powell (1970) can terminate at singular points and that these singular points are saddle points of the complex absolute value function. Based on this observation, they proposed an extension of the dogleg strategy for reliably finding real- or complex-valued roots.</p>
<p>Lucia and Taylor (""Complex Iterative Solutions to Process Model Equations?"", European Symp. Comp. Aided Process Eng., S387-S394, 1992) had shown that there are some advantages to carrying out equilibrium flash calculations in the complex domain. Although in that exercise, only real-valued solutions were obtained, Lucia et al. (""Process Simulation in the Complex Domain"", A.I.Ch.E. J, 39 (3), 461-470, 1993) obtained complex solutions to dew point temperature calculations when activity coefficient models are part of the phase equilibrium equations. Complex solutions had been found by Lucia and Wang (""Complex Domain Process Dynamics"", Ind. Eng. Chem. Res., 202-208, 1995) for the TV flash of a system whose thermodynamics were modeled using a cubic equation of state. Sridhar and Lucia (""Process Analysis in the Complex Domain"", A.I.Ch.E. J., 585-590, 1995) had shown that TP flashes involving cubic equations of state must have real-valued solutions and suggested an eigenvalue-eigenvalue decomposition for moving from a singular point to a solution.</p>
<p>The objectives of this study were to see if there are any advantages to carrying out distillation calculations in the complex domain. We also wanted to know if it was possible to find complex solutions to the equations that model multicomponent separation process calculations. Such solutions, if they exist, may have a bearing on the feasibility of a desired separation.</p>
<p>The starting point for our work was an existing computer program, written in Fortran 77, for performing multicomponent, multistage separation process calculations using Newton's method. The code was ""complexified"", mainly by converting all double precision declarations and function calls to their complex equivalents. The code was used to solve a variety of problems that often are considered difficult with results summarized below.</p>
<p>","Computer Science, Interdisciplinary Applications"
"WOS:000077860900001","Complex domain distillation calculations","1998","
<p>The phrase ""complex column"" is quite widely used in the literature on distillation and usually is intended to conjure up images of distillation columns with multiple feeds, sidedraws, pumparounds, and sidestrippers. We prefer to classify these operations as ""complicated columns."" In this paper, the phrase ""complex distillation"" refers to distillation calculations carried out in the complex domain.</p>
<p>Lucia and Xu (""Global Minima in Root Finding"", In Recent Advances in Global Optimization, C.A. Floudas and P.M. Pardalos, Eds., Princeton Univ. Press, 543, 1992) studied the complex domain behavior of various fixed-point methods for solving the Soave-Redlich-Kwong equation of state and show that the bifurcation of a pair of roots into the complex plane coincide with a phase transition. They also show that the dogleg strategy of Powell (1970) can terminate at singular points and that these singular points are saddle points of the complex absolute value function. Based on this observation, they proposed an extension of the dogleg strategy for reliably finding real- or complex-valued roots.</p>
<p>Lucia and Taylor (""Complex Iterative Solutions to Process Model Equations?"", European Symp. Comp. Aided Process Eng., S387-S394, 1992) had shown that there are some advantages to carrying out equilibrium flash calculations in the complex domain. Although in that exercise, only real-valued solutions were obtained, Lucia et al. (""Process Simulation in the Complex Domain"", A.I.Ch.E. J, 39 (3), 461-470, 1993) obtained complex solutions to dew point temperature calculations when activity coefficient models are part of the phase equilibrium equations. Complex solutions had been found by Lucia and Wang (""Complex Domain Process Dynamics"", Ind. Eng. Chem. Res., 202-208, 1995) for the TV flash of a system whose thermodynamics were modeled using a cubic equation of state. Sridhar and Lucia (""Process Analysis in the Complex Domain"", A.I.Ch.E. J., 585-590, 1995) had shown that TP flashes involving cubic equations of state must have real-valued solutions and suggested an eigenvalue-eigenvalue decomposition for moving from a singular point to a solution.</p>
<p>The objectives of this study were to see if there are any advantages to carrying out distillation calculations in the complex domain. We also wanted to know if it was possible to find complex solutions to the equations that model multicomponent separation process calculations. Such solutions, if they exist, may have a bearing on the feasibility of a desired separation.</p>
<p>The starting point for our work was an existing computer program, written in Fortran 77, for performing multicomponent, multistage separation process calculations using Newton's method. The code was ""complexified"", mainly by converting all double precision declarations and function calls to their complex equivalents. The code was used to solve a variety of problems that often are considered difficult with results summarized below.</p>
<p>","Computer Science"
"WOS:000082521300003","Aspects of digital evolution: Geometry and learning","1998","
<p>In this paper we present a new chromosome representation for evolving digital circuits. The representation is based very closely on the chip architecture of the Xilinx 6216 FPGA. We examine the effectiveness of evolving circuit functionality by using randomly chosen examples taken from the truth table. We consider the merits of a cell architecture in which functional cells alternate with routing cells and compare this with an architecture in which any cell can implement a function or be merely used for routing signals. It is noteworthy that the presence of elitism significantly improves the Genetic Algorithm performance.</p>","Computer Science, Artificial Intelligence"
"WOS:000082521300003","Aspects of digital evolution: Geometry and learning","1998","
<p>In this paper we present a new chromosome representation for evolving digital circuits. The representation is based very closely on the chip architecture of the Xilinx 6216 FPGA. We examine the effectiveness of evolving circuit functionality by using randomly chosen examples taken from the truth table. We consider the merits of a cell architecture in which functional cells alternate with routing cells and compare this with an architecture in which any cell can implement a function or be merely used for routing signals. It is noteworthy that the presence of elitism significantly improves the Genetic Algorithm performance.</p>","Computer Science, Cybernetics"
"WOS:000082521300003","Aspects of digital evolution: Geometry and learning","1998","
<p>In this paper we present a new chromosome representation for evolving digital circuits. The representation is based very closely on the chip architecture of the Xilinx 6216 FPGA. We examine the effectiveness of evolving circuit functionality by using randomly chosen examples taken from the truth table. We consider the merits of a cell architecture in which functional cells alternate with routing cells and compare this with an architecture in which any cell can implement a function or be merely used for routing signals. It is noteworthy that the presence of elitism significantly improves the Genetic Algorithm performance.</p>","Computer Science, Hardware & Architecture"
"WOS:000082521300003","Aspects of digital evolution: Geometry and learning","1998","
<p>In this paper we present a new chromosome representation for evolving digital circuits. The representation is based very closely on the chip architecture of the Xilinx 6216 FPGA. We examine the effectiveness of evolving circuit functionality by using randomly chosen examples taken from the truth table. We consider the merits of a cell architecture in which functional cells alternate with routing cells and compare this with an architecture in which any cell can implement a function or be merely used for routing signals. It is noteworthy that the presence of elitism significantly improves the Genetic Algorithm performance.</p>","Computer Science, Theory & Methods"
"WOS:000082521300003","Aspects of digital evolution: Geometry and learning","1998","
<p>In this paper we present a new chromosome representation for evolving digital circuits. The representation is based very closely on the chip architecture of the Xilinx 6216 FPGA. We examine the effectiveness of evolving circuit functionality by using randomly chosen examples taken from the truth table. We consider the merits of a cell architecture in which functional cells alternate with routing cells and compare this with an architecture in which any cell can implement a function or be merely used for routing signals. It is noteworthy that the presence of elitism significantly improves the Genetic Algorithm performance.</p>","Computer Science"
"WOS:000171768600068","Health information identification and de-identification toolkit","1998","
<p>Health identifiers are required for health information systems ranging in scope from the national to the smallest clinical study. Identification systems will differ in the tradeoffs of privacy and control that they represent. The Health Information Identification and De-Identification Toolkit (HIIDIT) is a generator of health identification systems that allows a system architect to specify the set of tradeoffs that are desired for any particular health information system.</p>","Computer Science, Information Systems"
"WOS:000171768600068","Health information identification and de-identification toolkit","1998","
<p>Health identifiers are required for health information systems ranging in scope from the national to the smallest clinical study. Identification systems will differ in the tradeoffs of privacy and control that they represent. The Health Information Identification and De-Identification Toolkit (HIIDIT) is a generator of health identification systems that allows a system architect to specify the set of tradeoffs that are desired for any particular health information system.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000171768600068","Health information identification and de-identification toolkit","1998","
<p>Health identifiers are required for health information systems ranging in scope from the national to the smallest clinical study. Identification systems will differ in the tradeoffs of privacy and control that they represent. The Health Information Identification and De-Identification Toolkit (HIIDIT) is a generator of health identification systems that allows a system architect to specify the set of tradeoffs that are desired for any particular health information system.</p>","Computer Science"
"WOS:000077612600100","A development tool environment for configuration, build, and launch of complex applications","1998","
<p>The increasing size and complexity of high-performance applications have motivated a new round of innovation related to configuration, build, and launch of applications for large computing platforms, especially heterogeneous multicomputers. This paper describes the software technology of the Talaris(TM) Environment, created by, Mercury Computer Systems, Inc. to enable a new generation of tools that construct and initiate applications for large distributed and parallel computer systems.</p>
<p>The Talaris Environment provides an extensible framework for cooperating tools that share application configuration information. Tools developed by Mercury for the Environment focus on high-performance embedded DSP applications that run on Mercury's RACE(R) series multicomputer systems. Additional tools under development by Mercury and other organizations support other target systems and programming interfaces that include UNIX workstation networks, the IBM SP/2, real-time DSP platforms, the Message Passing Interface(MPI), and POSIX.</p>
<p>Development of the Talaris Environment has been funded in part by the Defense Advanced Research Projects Agency (DARPA) under the ""Bridging the Gap"" and ""Three Steps"" programs. The Talaris Environment is currently available in connection with these DARPA programs.</p>","Computer Science, Theory & Methods"
"WOS:000077612600100","A development tool environment for configuration, build, and launch of complex applications","1998","
<p>The increasing size and complexity of high-performance applications have motivated a new round of innovation related to configuration, build, and launch of applications for large computing platforms, especially heterogeneous multicomputers. This paper describes the software technology of the Talaris(TM) Environment, created by, Mercury Computer Systems, Inc. to enable a new generation of tools that construct and initiate applications for large distributed and parallel computer systems.</p>
<p>The Talaris Environment provides an extensible framework for cooperating tools that share application configuration information. Tools developed by Mercury for the Environment focus on high-performance embedded DSP applications that run on Mercury's RACE(R) series multicomputer systems. Additional tools under development by Mercury and other organizations support other target systems and programming interfaces that include UNIX workstation networks, the IBM SP/2, real-time DSP platforms, the Message Passing Interface(MPI), and POSIX.</p>
<p>Development of the Talaris Environment has been funded in part by the Defense Advanced Research Projects Agency (DARPA) under the ""Bridging the Gap"" and ""Three Steps"" programs. The Talaris Environment is currently available in connection with these DARPA programs.</p>","Computer Science"
"WOS:000071517500006","The presence of Canadian hospitals on the World Wide Web: An empirical analysis","1998","
<p>As the World Wide Web expands, hospitals are considering how they may benefit from establishing Web sites of their own. We examined the Web sites of 20 Canadian hospitals to identify and compare their features. We developed two instruments for this assessment: a quantitative Features Checklist containing 67 items and a qualitative Categorical Rating Scale using 15 dimensions. Two of us (O.G. and D.A.M.) assessed each site.</p>
<p>At most sites the most fully implemented feature was the provision of basic contact information, although development was inconsistent. Few sites took full advantage of multimedia capabilities. There was a strong correlation (r = 0.82, P <0.001) between the number of features observed at a site and its score on the Categorical Rating Scale.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000071517500006","The presence of Canadian hospitals on the World Wide Web: An empirical analysis","1998","
<p>As the World Wide Web expands, hospitals are considering how they may benefit from establishing Web sites of their own. We examined the Web sites of 20 Canadian hospitals to identify and compare their features. We developed two instruments for this assessment: a quantitative Features Checklist containing 67 items and a qualitative Categorical Rating Scale using 15 dimensions. Two of us (O.G. and D.A.M.) assessed each site.</p>
<p>At most sites the most fully implemented feature was the provision of basic contact information, although development was inconsistent. Few sites took full advantage of multimedia capabilities. There was a strong correlation (r = 0.82, P <0.001) between the number of features observed at a site and its score on the Categorical Rating Scale.</p>","Computer Science"
"WOS:000082725700007","Artificial Neural Networks for motion emulation in virtual environments","1998","
<p>Simulation of natural human movement has proven to be a challenging problem, difficult to be solved by more or less traditional bio-inspired strategies. In opposition to several existing solutions, mainly based upon deterministic algorithms, a data-driven approach is presented herewith, which is able to grasp not only the natural essence of human movements, but also their intrinsic variability, the latter being a necessary feature for many ergonomic applications. For these purposes a recurrent Artificial Neural Network with some novel features (recurrent RPROP, state neurons, weighted cost function) has been adopted and combined with an original pre-processing step on experimental data, resulting in a new hybrid approach for data aggregation. Encouraging results on human hand reaching movements are also presented.</p>","Computer Science, Artificial Intelligence"
"WOS:000082725700007","Artificial Neural Networks for motion emulation in virtual environments","1998","
<p>Simulation of natural human movement has proven to be a challenging problem, difficult to be solved by more or less traditional bio-inspired strategies. In opposition to several existing solutions, mainly based upon deterministic algorithms, a data-driven approach is presented herewith, which is able to grasp not only the natural essence of human movements, but also their intrinsic variability, the latter being a necessary feature for many ergonomic applications. For these purposes a recurrent Artificial Neural Network with some novel features (recurrent RPROP, state neurons, weighted cost function) has been adopted and combined with an original pre-processing step on experimental data, resulting in a new hybrid approach for data aggregation. Encouraging results on human hand reaching movements are also presented.</p>","Computer Science, Software Engineering"
"WOS:000082725700007","Artificial Neural Networks for motion emulation in virtual environments","1998","
<p>Simulation of natural human movement has proven to be a challenging problem, difficult to be solved by more or less traditional bio-inspired strategies. In opposition to several existing solutions, mainly based upon deterministic algorithms, a data-driven approach is presented herewith, which is able to grasp not only the natural essence of human movements, but also their intrinsic variability, the latter being a necessary feature for many ergonomic applications. For these purposes a recurrent Artificial Neural Network with some novel features (recurrent RPROP, state neurons, weighted cost function) has been adopted and combined with an original pre-processing step on experimental data, resulting in a new hybrid approach for data aggregation. Encouraging results on human hand reaching movements are also presented.</p>","Computer Science"
"WOS:000075088400009","Statistical aspects on fitting the Arrhenius equation","1998","
<p>Motivated by a recent mathematical paper, we discuss statistical parameter estimation in the Arrhenius equation, that relates kinetic reaction rates to temperature. In opposition to the paper in question, we argue theoretically for the appropriate ness of using ordinary least squares on log-transformed data and supply some empirical support in this direction. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science, Artificial Intelligence"
"WOS:000075088400009","Statistical aspects on fitting the Arrhenius equation","1998","
<p>Motivated by a recent mathematical paper, we discuss statistical parameter estimation in the Arrhenius equation, that relates kinetic reaction rates to temperature. In opposition to the paper in question, we argue theoretically for the appropriate ness of using ordinary least squares on log-transformed data and supply some empirical support in this direction. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science"
"WOS:000074058800002","Assigning phrase breaks from part-of-speech sequences","1998","
<p>This paper presents an algorithm for automatically assigning phrase breaks to unrestricted text for use in a text-to-speech synthesizer. Text is first converted into a sequence of part-of-speech tags. Next a Markov model is used to give the most likely sequence of phrase breaks for the input part-of-speech tags. In the Markov model, states represent types of phrase break and the transitions between states represent the likelihoods of sequences of phrase types occurring. The paper reports a variety of experiments investigating part-of-speech tag-sets, Markov model structure and smoothing. The best setup correctly identifies 79% of breaks in the test corpus. (C) 1998 Academic Press Limited.</p>","Computer Science, Artificial Intelligence"
"WOS:000074058800002","Assigning phrase breaks from part-of-speech sequences","1998","
<p>This paper presents an algorithm for automatically assigning phrase breaks to unrestricted text for use in a text-to-speech synthesizer. Text is first converted into a sequence of part-of-speech tags. Next a Markov model is used to give the most likely sequence of phrase breaks for the input part-of-speech tags. In the Markov model, states represent types of phrase break and the transitions between states represent the likelihoods of sequences of phrase types occurring. The paper reports a variety of experiments investigating part-of-speech tag-sets, Markov model structure and smoothing. The best setup correctly identifies 79% of breaks in the test corpus. (C) 1998 Academic Press Limited.</p>","Computer Science"
"WOS:000074038800004","A Monte Carlo algorithm for probabilistic propagation in belief networks based on importance sampling and stratified simulation techniques","1998","
<p>A class of Monte Carlo algorithms for probability propagation in belief networks is given. The simulation is based on a two steps procedure. The first one is a node deletion technique to calculate the 'a posteriori' distribution on a variable, with the particularity that when exact computations are too costly, they are carried out in an approximate way. In the second step, the computations done in the first one are used to obtain random configurations for the variables of interest. These configurations are weighted following importance sampling methodology. Different particular algorithms are obtained depending on the approximation procedure used in the first step and the way of obtaining the random configurations. In this last case, a stratified sampling technique :is used, which has been adapted for application to very large networks without round-off error problems. (C) 1998 Elsevier Science Inc.</p>","Computer Science, Artificial Intelligence"
"WOS:000074038800004","A Monte Carlo algorithm for probabilistic propagation in belief networks based on importance sampling and stratified simulation techniques","1998","
<p>A class of Monte Carlo algorithms for probability propagation in belief networks is given. The simulation is based on a two steps procedure. The first one is a node deletion technique to calculate the 'a posteriori' distribution on a variable, with the particularity that when exact computations are too costly, they are carried out in an approximate way. In the second step, the computations done in the first one are used to obtain random configurations for the variables of interest. These configurations are weighted following importance sampling methodology. Different particular algorithms are obtained depending on the approximation procedure used in the first step and the way of obtaining the random configurations. In this last case, a stratified sampling technique :is used, which has been adapted for application to very large networks without round-off error problems. (C) 1998 Elsevier Science Inc.</p>","Computer Science"
"WOS:000075862200002","BLAST: broadband lightweight ATM secure transport for high-performance distributed computing","1998","
<p>This paper investigates the use of ATM for cluster-based computing. The need for a native ATM API is discussed as well as the performance of message passing libraries (MPL) that are written to use such an API to exploit the advantages of a high-speed network for cluster-based computing. The MPLs offer a standard interface, such as PVM or MPI, and interoperate with existing TCP/IP- and UDP/IP-based versions in addition to the ATM API environment. The interoperability extensions made to two MPLs, MPI and Prowess, which allow a hybrid environment of both ATM and TCP-based legacy network technology will be described. Shared object space (SOS), an extension to the MPLs, is described that helps support the geographically distributed computing (GDC) environment through latency hiding. It allows a user to develop applications in a shared memory type of environment. The native ATM API which supports cluster-based computing is described in this paper. This API provides a reliable transport interface to the MPL which has been optimized for an ATM environment. The transport protocol is a low-state design that optimizes the performance based on the available bandwidth, buffer constraints, propagation delay characteristics and security requirements of a particular connection. (C) 1998 Elsevier Science B.V.</p>","Computer Science, Information Systems"
"WOS:000075862200002","BLAST: broadband lightweight ATM secure transport for high-performance distributed computing","1998","
<p>This paper investigates the use of ATM for cluster-based computing. The need for a native ATM API is discussed as well as the performance of message passing libraries (MPL) that are written to use such an API to exploit the advantages of a high-speed network for cluster-based computing. The MPLs offer a standard interface, such as PVM or MPI, and interoperate with existing TCP/IP- and UDP/IP-based versions in addition to the ATM API environment. The interoperability extensions made to two MPLs, MPI and Prowess, which allow a hybrid environment of both ATM and TCP-based legacy network technology will be described. Shared object space (SOS), an extension to the MPLs, is described that helps support the geographically distributed computing (GDC) environment through latency hiding. It allows a user to develop applications in a shared memory type of environment. The native ATM API which supports cluster-based computing is described in this paper. This API provides a reliable transport interface to the MPL which has been optimized for an ATM environment. The transport protocol is a low-state design that optimizes the performance based on the available bandwidth, buffer constraints, propagation delay characteristics and security requirements of a particular connection. (C) 1998 Elsevier Science B.V.</p>","Computer Science"
"WOS:000079985900004","Comparisons between mesh and hierarchical ring networks for shared-memory multiprocessors","1998","
<p>Communication performance comparisons. in terms of message latency. between mesh and hierarchical ring interconnection networks for shared-memory multiprocessors are developed. Traffic in the networks is assumed to consist of the short, fixed-length messages needed for remote-memory read/write operations and cache coherency control. Square-mesh networks with bidirectional links and no end-around connections are compared to two- and three-level hierarchical rings. Wormhole routing is used in the meshes, with messages consisting of I hits, with each Ait containing b bits. In the ring networks, each ring segment can contain a complete message of f x b bits. Meshes and three-level rings provide comparable delays for system sizes of up to 200 or 300 processor clusters. The delay comparisons are valid only for the lightly loaded traffic case in which contention and blocking are low. This is the situation in practical shared-memory multiprocessor systems with a low cache miss rate and a high physical locality of reference in the global address space.</p>","Computer Science, Hardware & Architecture"
"WOS:000079985900004","Comparisons between mesh and hierarchical ring networks for shared-memory multiprocessors","1998","
<p>Communication performance comparisons. in terms of message latency. between mesh and hierarchical ring interconnection networks for shared-memory multiprocessors are developed. Traffic in the networks is assumed to consist of the short, fixed-length messages needed for remote-memory read/write operations and cache coherency control. Square-mesh networks with bidirectional links and no end-around connections are compared to two- and three-level hierarchical rings. Wormhole routing is used in the meshes, with messages consisting of I hits, with each Ait containing b bits. In the ring networks, each ring segment can contain a complete message of f x b bits. Meshes and three-level rings provide comparable delays for system sizes of up to 200 or 300 processor clusters. The delay comparisons are valid only for the lightly loaded traffic case in which contention and blocking are low. This is the situation in practical shared-memory multiprocessor systems with a low cache miss rate and a high physical locality of reference in the global address space.</p>","Computer Science"
"WOS:000074349800006","Universal continuous routing strategies","1998","
<p>We analyze universal routing protocols, that is, protocols that can be used for any communication pattern in any network, under a stochastic model of continuous message generation, In particular, we present two universal protocols, a store-and-forward and a wormhole routing protocol, and characterize their performance by the following three parameters: the maximum message generation rate for which the protocol is stable, the expected delay of a message from generation to service, and the time the protocol needs to recover from worst-case scenarios. Both protocols yield significant performance improvements over all previously known continuous routing protocols. In addition, we present adaptations of our protocols to continuous routing in node-symmetric networks, butterflies, and meshes.</p>","Computer Science, Theory & Methods"
"WOS:000074349800006","Universal continuous routing strategies","1998","
<p>We analyze universal routing protocols, that is, protocols that can be used for any communication pattern in any network, under a stochastic model of continuous message generation, In particular, we present two universal protocols, a store-and-forward and a wormhole routing protocol, and characterize their performance by the following three parameters: the maximum message generation rate for which the protocol is stable, the expected delay of a message from generation to service, and the time the protocol needs to recover from worst-case scenarios. Both protocols yield significant performance improvements over all previously known continuous routing protocols. In addition, we present adaptations of our protocols to continuous routing in node-symmetric networks, butterflies, and meshes.</p>","Computer Science"
"WOS:000078023500007","Molecular connectivity indices of iterated line graphs. A new source of descriptors for QSPR and QSAR studies","1998","
<p>Connectivity indices chi of the line graph L and higher line graphs L-n, n = 2, 3, ..., of the molecular graph G are examined as possible structure-descriptors in QSPR and QSAR studies. In the case of alkanes, regression models based on chi(G), chi(L), chi(L-2), chi(L-3) and chi(L-4) are found to be significantly better than those based solely on the index chi(G), with chi(G) continuing to play an important role. This has been demonstrated for boiling point, molar volume, molar refraction and surface tension. The same model was then applied for predicting the microsomal p-hydroxylation of aniline by alcohols. The QSAR model obtained by using connectivity indices of higher line graphs represents a significant improvement respect to the model using solely chi(G).</p>","Computer Science, Interdisciplinary Applications"
"WOS:000078023500007","Molecular connectivity indices of iterated line graphs. A new source of descriptors for QSPR and QSAR studies","1998","
<p>Connectivity indices chi of the line graph L and higher line graphs L-n, n = 2, 3, ..., of the molecular graph G are examined as possible structure-descriptors in QSPR and QSAR studies. In the case of alkanes, regression models based on chi(G), chi(L), chi(L-2), chi(L-3) and chi(L-4) are found to be significantly better than those based solely on the index chi(G), with chi(G) continuing to play an important role. This has been demonstrated for boiling point, molar volume, molar refraction and surface tension. The same model was then applied for predicting the microsomal p-hydroxylation of aniline by alcohols. The QSAR model obtained by using connectivity indices of higher line graphs represents a significant improvement respect to the model using solely chi(G).</p>","Computer Science"
"WOS:000077059000009","Building a multicasting tree in a high-speed network","1998","
<p>To build a multicasting tree in a multicomputer network, the authors propose three strategies based on voting, constructing a minimum spanning tree, and repeatedly constructing multiple minimum spanning trees. Typically, performance metrics to evaluate a multicast solution include time and traffic. Simultaneously optimizing both metrics is computationally intractable because the problem is NP-complete. The first scheme always guarantees the use of the shortest path from the source node to each destination, which makes it time-optimal. The other hva schemes do not guarantee this but try to reduce the traffic as much as possible. To demonstrate these strategies' effectiveness, the authors apply them to hypercubes, star graphs, and star graphs with some faults. They report experimental results to evaluate the performance of these solutions.</p>","Computer Science, Theory & Methods"
"WOS:000077059000009","Building a multicasting tree in a high-speed network","1998","
<p>To build a multicasting tree in a multicomputer network, the authors propose three strategies based on voting, constructing a minimum spanning tree, and repeatedly constructing multiple minimum spanning trees. Typically, performance metrics to evaluate a multicast solution include time and traffic. Simultaneously optimizing both metrics is computationally intractable because the problem is NP-complete. The first scheme always guarantees the use of the shortest path from the source node to each destination, which makes it time-optimal. The other hva schemes do not guarantee this but try to reduce the traffic as much as possible. To demonstrate these strategies' effectiveness, the authors apply them to hypercubes, star graphs, and star graphs with some faults. They report experimental results to evaluate the performance of these solutions.</p>","Computer Science"
"WOS:000073273100024","Stability analysis of dynamic neural control","1998","
<p>In this paper, the authors summarize their research related to dynamic neural control. In particular, results on nonlinear system identification, nonlinear trajectory tracking, and input-to-state stability (ISS) of dynamic neural networks are presented. The main analysis tool utilized is the Lyapunov approach. References for the detailed demonstrations are given. We illustrate the applicability of the results by means of examples. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science, Artificial Intelligence"
"WOS:000073273100024","Stability analysis of dynamic neural control","1998","
<p>In this paper, the authors summarize their research related to dynamic neural control. In particular, results on nonlinear system identification, nonlinear trajectory tracking, and input-to-state stability (ISS) of dynamic neural networks are presented. The main analysis tool utilized is the Lyapunov approach. References for the detailed demonstrations are given. We illustrate the applicability of the results by means of examples. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science"
"WOS:000073397300001","Modeling and simulating morphological evolution in an artificial life environment","1998","
<p>This paper presents a computer-based environment designed to study biological evolution considering morphological aspects. It was inspired on cellular automata and evolutionary algorithm principles. Simple rules are used to determine the genotype and phenotype of individuals and their relationships with behavioral aspects in a square matrix environment, where individuals can evolve. Two methods to simulate mutational errors and to introduce variability of mutations are discussed. A series of four simulations show that the model promotes phenotype evolution depending on the distribution of food over the environment; morphology evolved as to favor movement of the individuals towards the portion of the environment in which the food has been distributed or to capture falling food, (C) 1998 academic Press.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000073397300001","Modeling and simulating morphological evolution in an artificial life environment","1998","
<p>This paper presents a computer-based environment designed to study biological evolution considering morphological aspects. It was inspired on cellular automata and evolutionary algorithm principles. Simple rules are used to determine the genotype and phenotype of individuals and their relationships with behavioral aspects in a square matrix environment, where individuals can evolve. Two methods to simulate mutational errors and to introduce variability of mutations are discussed. A series of four simulations show that the model promotes phenotype evolution depending on the distribution of food over the environment; morphology evolved as to favor movement of the individuals towards the portion of the environment in which the food has been distributed or to capture falling food, (C) 1998 academic Press.</p>","Computer Science"
"WOS:000085345200005","Elliptic curve public-key cryptosystems - An introduction","1998","
<p>We give a brief introduction to elliptic curve public-key cryptosystems. We explain how the discrete logarithm in an elliptic curve group can be used to construct cryptosystems. We also focus on practical aspects such as implementation, standardization and intellectual property.</p>","Computer Science, Theory & Methods"
"WOS:000085345200005","Elliptic curve public-key cryptosystems - An introduction","1998","
<p>We give a brief introduction to elliptic curve public-key cryptosystems. We explain how the discrete logarithm in an elliptic curve group can be used to construct cryptosystems. We also focus on practical aspects such as implementation, standardization and intellectual property.</p>","Computer Science"
"WOS:000077981600006","Route following based on adaptive visual landmark matching","1998","
<p>Route following based on visual landmark matching may require many models to cover all different situations. This paper describes a system that is able to adapt template's modelling parameters to environmental conditions (lighting, shadows, etc.) by a genetic learning technique. In addition, the mobile robot self-localisation is obtained by a stereo approach that uses the centres of matching in the two images to solve in a simple way the correspondence problem in the 3D position estimation. The experimental results show that the tracking robustness is improved, while using a small set of templates. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science, Artificial Intelligence"
"WOS:000077981600006","Route following based on adaptive visual landmark matching","1998","
<p>Route following based on visual landmark matching may require many models to cover all different situations. This paper describes a system that is able to adapt template's modelling parameters to environmental conditions (lighting, shadows, etc.) by a genetic learning technique. In addition, the mobile robot self-localisation is obtained by a stereo approach that uses the centres of matching in the two images to solve in a simple way the correspondence problem in the 3D position estimation. The experimental results show that the tracking robustness is improved, while using a small set of templates. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science"
"WOS:000078928400013","A filter-mechanism for method-driven trace capture","1998","
<p>Traceability is a prerequisite for developing high quality (software) systems. Recording and maintaining all available information is too labor intensive and thus by Tar too expensive. A project-specific definition of the trace information to be recorded and the method fragments (so called trace fragments) to he executed for recording the information provides a solution for this problem. But the amount of traces to he recorded does not only vary from project to project. II also varies between project phases and even within a project phase. As a consequence project-specific trace fragments need to be adapted according to the actual project phase.</p>
<p>In this paper we propose a model-based filter mechanism to significantly reduce the required effort to adapt trace fragments. By defining appropriate filters the project manager is able to (dynamically) adapt the project-specific trace fragments to the actual needs. We present an example to highlight the benefits of the approach and discuss possible extensions.</p>","Computer Science, Theory & Methods"
"WOS:000078928400013","A filter-mechanism for method-driven trace capture","1998","
<p>Traceability is a prerequisite for developing high quality (software) systems. Recording and maintaining all available information is too labor intensive and thus by Tar too expensive. A project-specific definition of the trace information to be recorded and the method fragments (so called trace fragments) to he executed for recording the information provides a solution for this problem. But the amount of traces to he recorded does not only vary from project to project. II also varies between project phases and even within a project phase. As a consequence project-specific trace fragments need to be adapted according to the actual project phase.</p>
<p>In this paper we propose a model-based filter mechanism to significantly reduce the required effort to adapt trace fragments. By defining appropriate filters the project manager is able to (dynamically) adapt the project-specific trace fragments to the actual needs. We present an example to highlight the benefits of the approach and discuss possible extensions.</p>","Computer Science"
"WOS:000074695400001","Sequence alignment in molecular biology","1998","
<p>Molecular biology is becoming a computationally intense realm of contemporary science and faces some of the current grand scientific challenges. In its context, tools that identify, store, compare and analyze effectively large and growing numbers of bio-sequences are found of increasingly crucial importance. Biosequences are routinely compared or aligned, in a variety of ways, to infer common ancestry, to detect functional equivalence, or simply while searching for similar entries in a database. A considerable body of knowledge has accumulated on sequence alignment during the past few decades. Without pretending to be exhaustive, this paper attempts a survey of some criteria of wide use in sequence alignment and comparison problems, and of the corresponding solutions. The paper is based on presentations and literature given at the Workshop on Sequence Alignment held at Princeton, N,J,, in November 1994, as part of the DIMACS Special Year on Mathematical Support for Molecular Biology.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000074695400001","Sequence alignment in molecular biology","1998","
<p>Molecular biology is becoming a computationally intense realm of contemporary science and faces some of the current grand scientific challenges. In its context, tools that identify, store, compare and analyze effectively large and growing numbers of bio-sequences are found of increasingly crucial importance. Biosequences are routinely compared or aligned, in a variety of ways, to infer common ancestry, to detect functional equivalence, or simply while searching for similar entries in a database. A considerable body of knowledge has accumulated on sequence alignment during the past few decades. Without pretending to be exhaustive, this paper attempts a survey of some criteria of wide use in sequence alignment and comparison problems, and of the corresponding solutions. The paper is based on presentations and literature given at the Workshop on Sequence Alignment held at Princeton, N,J,, in November 1994, as part of the DIMACS Special Year on Mathematical Support for Molecular Biology.</p>","Computer Science"
"WOS:000077467400005","A learning environment for the conservation of area and its measurement: a computer microworld","1998","
<p>In this paper, we present the design and implementation of a microworld as a possible learning environment for the concept of conservation of area and its measurement. This microworld is the result of the synthesis of three models. The first is the model of the subject matter. The second is a model of children's sensory-motor actions while they face specific measurement tasks. The third is a model of learning viewed as an active, subjective and constructive process. All these models are enriched by the features of the electronic media which have an impact on the children's cognitive development. The modelling is the result of the study of past research in the above areas but also reflects the designers' beliefs about the nature of mathematics, its teaching and learning, and about the ways that computers can be used in the teaching and learning of mathematics. The requirements for the design of the software emerged from the educational requirements defined as a result of the modelling. The aim of this microworld is to provide the students with a set of tools to create their own objects (shapes) and transform or compare them using measurement or conservation concepts. The rationale of the design of this computer environment and an analysis of its main features are presented. Finally, the design of the pilot study of evaluation of the microworld is presented and the results of this study are discussed. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000077467400005","A learning environment for the conservation of area and its measurement: a computer microworld","1998","
<p>In this paper, we present the design and implementation of a microworld as a possible learning environment for the concept of conservation of area and its measurement. This microworld is the result of the synthesis of three models. The first is the model of the subject matter. The second is a model of children's sensory-motor actions while they face specific measurement tasks. The third is a model of learning viewed as an active, subjective and constructive process. All these models are enriched by the features of the electronic media which have an impact on the children's cognitive development. The modelling is the result of the study of past research in the above areas but also reflects the designers' beliefs about the nature of mathematics, its teaching and learning, and about the ways that computers can be used in the teaching and learning of mathematics. The requirements for the design of the software emerged from the educational requirements defined as a result of the modelling. The aim of this microworld is to provide the students with a set of tools to create their own objects (shapes) and transform or compare them using measurement or conservation concepts. The rationale of the design of this computer environment and an analysis of its main features are presented. Finally, the design of the pilot study of evaluation of the microworld is presented and the results of this study are discussed. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science"
"WOS:000078366900005","Simulating hardware, software and electromechanical parts using communicating simulators","1998","
<p>The design of embedded processor circuits for the control of electromechanical systems, in a unified environment, offers many advantages including reduced system design and debug cycles and shorter time to market. Simulation plays a very important role in such a design methodology. In this paper, we will present, a brief description of the design methodology of our research team which focuses on the co-simulation of various heterogeneous parts, More specifically, we will present a couple of flexible interfaces that have been implemented for bridging VHDL with Physical Systems Simulators as well as VHDL simulators with the application programs that control the digital circuit simulated in VHDL. The basic concepts and features used for these implementations are characterised by increased portability, while other significant advantages of the proposed schemes are their speed, their flexibility in supporting various communication protocols and the simplified and fully automated simulation procedure which is transparent to the user.</p>","Computer Science, Hardware & Architecture"
"WOS:000078366900005","Simulating hardware, software and electromechanical parts using communicating simulators","1998","
<p>The design of embedded processor circuits for the control of electromechanical systems, in a unified environment, offers many advantages including reduced system design and debug cycles and shorter time to market. Simulation plays a very important role in such a design methodology. In this paper, we will present, a brief description of the design methodology of our research team which focuses on the co-simulation of various heterogeneous parts, More specifically, we will present a couple of flexible interfaces that have been implemented for bridging VHDL with Physical Systems Simulators as well as VHDL simulators with the application programs that control the digital circuit simulated in VHDL. The basic concepts and features used for these implementations are characterised by increased portability, while other significant advantages of the proposed schemes are their speed, their flexibility in supporting various communication protocols and the simplified and fully automated simulation procedure which is transparent to the user.</p>","Computer Science, Software Engineering"
"WOS:000078366900005","Simulating hardware, software and electromechanical parts using communicating simulators","1998","
<p>The design of embedded processor circuits for the control of electromechanical systems, in a unified environment, offers many advantages including reduced system design and debug cycles and shorter time to market. Simulation plays a very important role in such a design methodology. In this paper, we will present, a brief description of the design methodology of our research team which focuses on the co-simulation of various heterogeneous parts, More specifically, we will present a couple of flexible interfaces that have been implemented for bridging VHDL with Physical Systems Simulators as well as VHDL simulators with the application programs that control the digital circuit simulated in VHDL. The basic concepts and features used for these implementations are characterised by increased portability, while other significant advantages of the proposed schemes are their speed, their flexibility in supporting various communication protocols and the simplified and fully automated simulation procedure which is transparent to the user.</p>","Computer Science"
"WOS:000075317000024","The effect of quantization on the performance of sampling designs","1998","
<p>The most common form of quantization is rounding-off, which occurs in all digital systems. A general quantizer approximates an observed value by the nearest among a finite number of representative values. In estimating weighted integrals of time series with no quadratic mean derivatives, by means of samples at discrete times, it is known that the rate of convergence of the mean-square error is reduced from n(-2) to n(-1.5) when the samples are quantized, For smoother time series, with k = 1, 2, ... quadratic mean derivatives, it is now shown that the rate of convergence is reduced from n(-2k-2) to n(-2) when the samples are quantized, which is a very significant reduction. The interplay between sampling and quantization is also studied, leading to (asymptotically) optimal allocation between the number of samples and the number of levels of quantization.</p>","Computer Science, Information Systems"
"WOS:000075317000024","The effect of quantization on the performance of sampling designs","1998","
<p>The most common form of quantization is rounding-off, which occurs in all digital systems. A general quantizer approximates an observed value by the nearest among a finite number of representative values. In estimating weighted integrals of time series with no quadratic mean derivatives, by means of samples at discrete times, it is known that the rate of convergence of the mean-square error is reduced from n(-2) to n(-1.5) when the samples are quantized, For smoother time series, with k = 1, 2, ... quadratic mean derivatives, it is now shown that the rate of convergence is reduced from n(-2k-2) to n(-2) when the samples are quantized, which is a very significant reduction. The interplay between sampling and quantization is also studied, leading to (asymptotically) optimal allocation between the number of samples and the number of levels of quantization.</p>","Computer Science"
"WOS:000077564000016","A complete declarative debugger of missing answers","1998","
<p>We propose two declarative debuggers of missing answers with respect to C- and S-semantics. The debuggers are proved correct for every logic program. Moreover, they are complete and terminating with respect to a large class of programs, namely acceptable logic programs. The debuggers enhance existing proposals, which suffer from a problem due to the implementation of negation as failure. The proposed solution exploits decision procedures far C- and S-semantics introduced in [9].</p>","Computer Science, Software Engineering"
"WOS:000077564000016","A complete declarative debugger of missing answers","1998","
<p>We propose two declarative debuggers of missing answers with respect to C- and S-semantics. The debuggers are proved correct for every logic program. Moreover, they are complete and terminating with respect to a large class of programs, namely acceptable logic programs. The debuggers enhance existing proposals, which suffer from a problem due to the implementation of negation as failure. The proposed solution exploits decision procedures far C- and S-semantics introduced in [9].</p>","Computer Science, Theory & Methods"
"WOS:000077564000016","A complete declarative debugger of missing answers","1998","
<p>We propose two declarative debuggers of missing answers with respect to C- and S-semantics. The debuggers are proved correct for every logic program. Moreover, they are complete and terminating with respect to a large class of programs, namely acceptable logic programs. The debuggers enhance existing proposals, which suffer from a problem due to the implementation of negation as failure. The proposed solution exploits decision procedures far C- and S-semantics introduced in [9].</p>","Computer Science"
"WOS:000072857200004","A fuzzy approach to stability of fuzzy controllers","1998","
<p>This paper deals with stability analysis of fuzzy controllers. A new definition embedding the notion of stability into fuzzy set theory is suggested. In this context the introduction of a gradual stability is discussed. Investigating stability needs a model of the process. One of the essential assumptions for the model used here is that the process has an approximatively linear structure within a sufficiently small time interval dt. Possible deviations between the model and the real system are taken into account by specifying fuzzy parameters. For the sake of simplicity, however, only a simple version of the model is considered in the present paper. Moreover, some facts about the influence of the defuzzifier and the inference engine on the stability statement are presented. By using this model and the suggested definition of stability some criteria ensuring stability are derived. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science, Theory & Methods"
"WOS:000072857200004","A fuzzy approach to stability of fuzzy controllers","1998","
<p>This paper deals with stability analysis of fuzzy controllers. A new definition embedding the notion of stability into fuzzy set theory is suggested. In this context the introduction of a gradual stability is discussed. Investigating stability needs a model of the process. One of the essential assumptions for the model used here is that the process has an approximatively linear structure within a sufficiently small time interval dt. Possible deviations between the model and the real system are taken into account by specifying fuzzy parameters. For the sake of simplicity, however, only a simple version of the model is considered in the present paper. Moreover, some facts about the influence of the defuzzifier and the inference engine on the stability statement are presented. By using this model and the suggested definition of stability some criteria ensuring stability are derived. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science"
"WOS:000083673800011","Rules for designing multilevel Object-Oriented Databases","1998","
<p>When implementing a multilevel security policy for Object-Oriented Databases (OODBs), several aspects have to be investigated. One of these aspect is the design of multilevel OODBs. In an OODB, data are organized in a complex structure built using different constructs (classes, objects, attributes, links...). Therefore, a first problem is to determine what constructs of the object-oriented model should be associated with a security level. A second problem is then to define semantics for each assignment of a security level to an object-oriented construct. While assigning the security levels, we have also to be careful with the inference problems which may occur due to the integrity constraints inherent in the object-oriented paradigm. Therefore, a last purpose of this paper is to define a set of general rules to cope with this problem.</p>","Computer Science, Software Engineering"
"WOS:000083673800011","Rules for designing multilevel Object-Oriented Databases","1998","
<p>When implementing a multilevel security policy for Object-Oriented Databases (OODBs), several aspects have to be investigated. One of these aspect is the design of multilevel OODBs. In an OODB, data are organized in a complex structure built using different constructs (classes, objects, attributes, links...). Therefore, a first problem is to determine what constructs of the object-oriented model should be associated with a security level. A second problem is then to define semantics for each assignment of a security level to an object-oriented construct. While assigning the security levels, we have also to be careful with the inference problems which may occur due to the integrity constraints inherent in the object-oriented paradigm. Therefore, a last purpose of this paper is to define a set of general rules to cope with this problem.</p>","Computer Science"
"WOS:000083674000002","Combining introspection and communication with rationality and reactivity in agents","1998","
<p>We propose a logic-based language for programming agents that can reason about their own beliefs as well as the beliefs of other agents and can communicate with each other. The agents can be reactive, rational/deliberative or hybrid, combining both reactive and rational behaviour. We illustrate the language by means of examples.</p>","Computer Science, Artificial Intelligence"
"WOS:000083674000002","Combining introspection and communication with rationality and reactivity in agents","1998","
<p>We propose a logic-based language for programming agents that can reason about their own beliefs as well as the beliefs of other agents and can communicate with each other. The agents can be reactive, rational/deliberative or hybrid, combining both reactive and rational behaviour. We illustrate the language by means of examples.</p>","Computer Science"
"WOS:000074924100006","Genetic operators for a two-dimensional bonded molecular model","1998","
<p>This paper describes a real coded, parallel genetic algorithm implemented to find global minimum energy structures for a two-dimensional bonded molecular model. Starting from randomly generated structures, the genetic algorithm was able to find minimum energy conformations for most structures containing between 2 and 61 atoms. The importance of tailoring genetic operators to the problem domain is demonstrated by comparing the performance of this genetic algorithm with results obtained by another genetic algorithm and other optimisation methods. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000074924100006","Genetic operators for a two-dimensional bonded molecular model","1998","
<p>This paper describes a real coded, parallel genetic algorithm implemented to find global minimum energy structures for a two-dimensional bonded molecular model. Starting from randomly generated structures, the genetic algorithm was able to find minimum energy conformations for most structures containing between 2 and 61 atoms. The importance of tailoring genetic operators to the problem domain is demonstrated by comparing the performance of this genetic algorithm with results obtained by another genetic algorithm and other optimisation methods. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science"
"WOS:000077013500001","An empirical approach to temporal reference resolution","1998","
<p>Scheduling dialogs, during which people negotiate the times of appointments, are common in everyday life. This paper reports the results of an in-depth empirical investigation of resolving explicit temporal references in scheduling dialogs. There are four phases of this work: data annotation and evaluation, model development, system implementation and evaluation, and model evaluation and analysis. The system and model were developed primarily on one set of data, and then applied later to a much more complex data set, to assess the generalizability of the model for the task being performed. Many different types of empirical methods are applied to pinpoint the strengths and weaknesses of the approach. Detailed annotation instructions were developed and an intercoder reliability study was performed, showing that naive annotators can reliably perform the targeted annotations. A fully automatic system has been developed and evaluated on unseen test data, with good results on both data sets. We adopt a pure realization of a recency-based focus model to identify precisely when it is and is not adequate for the task being addressed. In addition to system results, an in-depth evaluation of the model itself is presented, based on detailed manual annotations. The results are that few errors occur specifically due to the model of focus being used, and the set of anaphoric relations defined in the model are low in ambiguity for both data sets.</p>","Computer Science, Artificial Intelligence"
"WOS:000077013500001","An empirical approach to temporal reference resolution","1998","
<p>Scheduling dialogs, during which people negotiate the times of appointments, are common in everyday life. This paper reports the results of an in-depth empirical investigation of resolving explicit temporal references in scheduling dialogs. There are four phases of this work: data annotation and evaluation, model development, system implementation and evaluation, and model evaluation and analysis. The system and model were developed primarily on one set of data, and then applied later to a much more complex data set, to assess the generalizability of the model for the task being performed. Many different types of empirical methods are applied to pinpoint the strengths and weaknesses of the approach. Detailed annotation instructions were developed and an intercoder reliability study was performed, showing that naive annotators can reliably perform the targeted annotations. A fully automatic system has been developed and evaluated on unseen test data, with good results on both data sets. We adopt a pure realization of a recency-based focus model to identify precisely when it is and is not adequate for the task being addressed. In addition to system results, an in-depth evaluation of the model itself is presented, based on detailed manual annotations. The results are that few errors occur specifically due to the model of focus being used, and the set of anaphoric relations defined in the model are low in ambiguity for both data sets.</p>","Computer Science"
"WOS:000073662000007","Cartesian spline interpolation for industrial robots","1998","
<p>We describe an algorithm for interpolation of positions by a rational spline motion. A reparameterization of the resulting motion is applied in order to achieve the desired distribution of the velocity. For the ease of presentation we discuss trapezoidal velocity profiles, i.e. piecewise constant and linear velocity distribution. The method can be generalized to more general velocity profiles. The whole spline scheme possesses some special features which make it a suitable tool for the control of industrial robots. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science, Software Engineering"
"WOS:000073662000007","Cartesian spline interpolation for industrial robots","1998","
<p>We describe an algorithm for interpolation of positions by a rational spline motion. A reparameterization of the resulting motion is applied in order to achieve the desired distribution of the velocity. For the ease of presentation we discuss trapezoidal velocity profiles, i.e. piecewise constant and linear velocity distribution. The method can be generalized to more general velocity profiles. The whole spline scheme possesses some special features which make it a suitable tool for the control of industrial robots. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science"
"WOS:000075656800004","Maximum-likelihood estimation of Rician distribution parameters","1998","
<p>The problem of parameter estimation from Rician distributed data (e.g., magnitude magnetic resonance images) is addressed. The properties of conventional estimation methods are discussed and compared to maximum-likelihood (ML) estimation which is known to yield optimal results asymptotically. In contrast to previously proposed methods, ML estimation is demonstrated to be unbiased for high signal-to-noise ratio (SNR) and to yield physical relevant results for low SNR.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000075656800004","Maximum-likelihood estimation of Rician distribution parameters","1998","
<p>The problem of parameter estimation from Rician distributed data (e.g., magnitude magnetic resonance images) is addressed. The properties of conventional estimation methods are discussed and compared to maximum-likelihood (ML) estimation which is known to yield optimal results asymptotically. In contrast to previously proposed methods, ML estimation is demonstrated to be unbiased for high signal-to-noise ratio (SNR) and to yield physical relevant results for low SNR.</p>","Computer Science"
"WOS:000076634800017","The problem of automatic understanding of full text documents in information retrieval","1998","
<p>New opportunities for retrieval and automatic processing of full text documents that arise in the use of modern intelligent systems are discussed. An overview of major limitations of the classical scheme of descriptive information retrieval is presented, and a direction of its modernization is proposed. This direction leans on computer reconstruction of concepts on the basis of documents that discuss them. The key role of an intelligent system of linguistic analysis and machine learning in this process is characterized. An overview of sample computer experiments in thematic classifications of natural language texts is presented. The perspectives of application of the proposed approach to a number of research and industrial projects are outlined.</p>","Computer Science, Artificial Intelligence"
"WOS:000076634800017","The problem of automatic understanding of full text documents in information retrieval","1998","
<p>New opportunities for retrieval and automatic processing of full text documents that arise in the use of modern intelligent systems are discussed. An overview of major limitations of the classical scheme of descriptive information retrieval is presented, and a direction of its modernization is proposed. This direction leans on computer reconstruction of concepts on the basis of documents that discuss them. The key role of an intelligent system of linguistic analysis and machine learning in this process is characterized. An overview of sample computer experiments in thematic classifications of natural language texts is presented. The perspectives of application of the proposed approach to a number of research and industrial projects are outlined.</p>","Computer Science, Cybernetics"
"WOS:000076634800017","The problem of automatic understanding of full text documents in information retrieval","1998","
<p>New opportunities for retrieval and automatic processing of full text documents that arise in the use of modern intelligent systems are discussed. An overview of major limitations of the classical scheme of descriptive information retrieval is presented, and a direction of its modernization is proposed. This direction leans on computer reconstruction of concepts on the basis of documents that discuss them. The key role of an intelligent system of linguistic analysis and machine learning in this process is characterized. An overview of sample computer experiments in thematic classifications of natural language texts is presented. The perspectives of application of the proposed approach to a number of research and industrial projects are outlined.</p>","Computer Science, Theory & Methods"
"WOS:000076634800017","The problem of automatic understanding of full text documents in information retrieval","1998","
<p>New opportunities for retrieval and automatic processing of full text documents that arise in the use of modern intelligent systems are discussed. An overview of major limitations of the classical scheme of descriptive information retrieval is presented, and a direction of its modernization is proposed. This direction leans on computer reconstruction of concepts on the basis of documents that discuss them. The key role of an intelligent system of linguistic analysis and machine learning in this process is characterized. An overview of sample computer experiments in thematic classifications of natural language texts is presented. The perspectives of application of the proposed approach to a number of research and industrial projects are outlined.</p>","Computer Science"
"WOS:000074516100009","A new gray level based Hough transform for region extraction: An application to IRS images","1998","
<p>A technique using the Hough transform is described for detection of homogeneous line segments directly from (i.e., without binarization of) gray level images. A definition of ""region"" in terms of these line segments, with constraints on its length and variance, is provided. The algorithm is able to extract gray level regions irrespective of their shape and size. The effectiveness of the method is demonstrated on Indian Remote-sensing Satellite (TRS) images. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science, Artificial Intelligence"
"WOS:000074516100009","A new gray level based Hough transform for region extraction: An application to IRS images","1998","
<p>A technique using the Hough transform is described for detection of homogeneous line segments directly from (i.e., without binarization of) gray level images. A definition of ""region"" in terms of these line segments, with constraints on its length and variance, is provided. The algorithm is able to extract gray level regions irrespective of their shape and size. The effectiveness of the method is demonstrated on Indian Remote-sensing Satellite (TRS) images. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science"
"WOS:000074385300006","Multiagent systems","1998","
<p>Agent-based systems technology has generated lots of excitement in recent years because of its promise as a new paradigm for conceptualizing, designing, and implementing software systems. This promise is particularly attractive for creating software that operates in environments that are distributed and open, such as the internet. Currently, the great majority of agent-based systems consist of a single agent. However, as the technology matures and addresses increasingly complex applications, the need for systems that consist of multiple agents that communicate in a peer-to-peer fashion is becoming apparent. Central to the design and effective operation of such multiagent systems (MASs) are a core set of issues and research questions that have been studied over the years by the distributed AI community. In this article, I present some of the critical notions in MASs and the research work that has addressed them. I organize these notions around the concept of problem-solving coherence, which I believe is one of the most critical overall characteristics that an MAS should exhibit.</p>","Computer Science, Artificial Intelligence"
"WOS:000074385300006","Multiagent systems","1998","
<p>Agent-based systems technology has generated lots of excitement in recent years because of its promise as a new paradigm for conceptualizing, designing, and implementing software systems. This promise is particularly attractive for creating software that operates in environments that are distributed and open, such as the internet. Currently, the great majority of agent-based systems consist of a single agent. However, as the technology matures and addresses increasingly complex applications, the need for systems that consist of multiple agents that communicate in a peer-to-peer fashion is becoming apparent. Central to the design and effective operation of such multiagent systems (MASs) are a core set of issues and research questions that have been studied over the years by the distributed AI community. In this article, I present some of the critical notions in MASs and the research work that has addressed them. I organize these notions around the concept of problem-solving coherence, which I believe is one of the most critical overall characteristics that an MAS should exhibit.</p>","Computer Science"
"WOS:000075894600014","Discrete models and sign-invariant structures of matrices","1998","
<p>The asymptotic behavior of linear discrete systems defined by nonpositive, so-called NZ-matrices is considered. Sign structures of such matrices (sign-invariant and pulsars) that can generate a nontrivial equilibrium or periodic mode, respectively, are defined. Results are applied to the study of nonlinear competition problems.</p>","Computer Science, Artificial Intelligence"
"WOS:000075894600014","Discrete models and sign-invariant structures of matrices","1998","
<p>The asymptotic behavior of linear discrete systems defined by nonpositive, so-called NZ-matrices is considered. Sign structures of such matrices (sign-invariant and pulsars) that can generate a nontrivial equilibrium or periodic mode, respectively, are defined. Results are applied to the study of nonlinear competition problems.</p>","Computer Science, Cybernetics"
"WOS:000075894600014","Discrete models and sign-invariant structures of matrices","1998","
<p>The asymptotic behavior of linear discrete systems defined by nonpositive, so-called NZ-matrices is considered. Sign structures of such matrices (sign-invariant and pulsars) that can generate a nontrivial equilibrium or periodic mode, respectively, are defined. Results are applied to the study of nonlinear competition problems.</p>","Computer Science, Theory & Methods"
"WOS:000075894600014","Discrete models and sign-invariant structures of matrices","1998","
<p>The asymptotic behavior of linear discrete systems defined by nonpositive, so-called NZ-matrices is considered. Sign structures of such matrices (sign-invariant and pulsars) that can generate a nontrivial equilibrium or periodic mode, respectively, are defined. Results are applied to the study of nonlinear competition problems.</p>","Computer Science"
"WOS:000074840000009","List-mode likelihood: EM algorithm and image quality estimation demonstrated on 2-D PET","1998","
<p>Using a theory of list-mode maximum-likelihood (ML) source reconstruction presented recently by Barrett et al. [1], this paper formulates a corresponding expectation-maximization (EM) algorithm, as well as a method for estimating noise properties at the ML estimate, List-mode ML is of interest in cases where the dimensionality of the measurement space impedes a binning of the measurement data. It can be advantageous in cases where a better forward model can be obtained by including more measurement coordinates provided by a given detector. Different figures of merit for the detector performance can be computed from the Fisher information matrix (FIM). This paper uses the observed FIM, which requires a single data set, thus, avoiding costly ensemble statistics, The proposed techniques are demonstrated for an idealized two-dimensional (2-D) positron emission tomography (PET) [2-D PET] detector. We compute from simulation data the improved image quality obtained by including the time of flight of the coincident quanta.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000074840000009","List-mode likelihood: EM algorithm and image quality estimation demonstrated on 2-D PET","1998","
<p>Using a theory of list-mode maximum-likelihood (ML) source reconstruction presented recently by Barrett et al. [1], this paper formulates a corresponding expectation-maximization (EM) algorithm, as well as a method for estimating noise properties at the ML estimate, List-mode ML is of interest in cases where the dimensionality of the measurement space impedes a binning of the measurement data. It can be advantageous in cases where a better forward model can be obtained by including more measurement coordinates provided by a given detector. Different figures of merit for the detector performance can be computed from the Fisher information matrix (FIM). This paper uses the observed FIM, which requires a single data set, thus, avoiding costly ensemble statistics, The proposed techniques are demonstrated for an idealized two-dimensional (2-D) positron emission tomography (PET) [2-D PET] detector. We compute from simulation data the improved image quality obtained by including the time of flight of the coincident quanta.</p>","Computer Science"
"WOS:000074568900004","Extranets: a tool for cost control in a value chain framework","1998","
<p>The purpose of this research is to show through a case study how the extranet has been used by one specific company to significantly reduce operating costs. The activities of the company are analyzed within the framework of the value chain concept developed by Porter. This, it is felt, will provide a greater insight into how the extranet can be used to improve profit margins. Prior research in this area has either been of a conceptual nature (explaining theoretically how the extranet should be employed) or of a survey nature (examining, by means of a survey instrument, the benefits accruing to companies that have adopted the extranet). This study is different in that it examines in detail, by means of a case study, how the extranet influences a retail company's chain of activities and reduces the attendant costs thereon.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000074568900004","Extranets: a tool for cost control in a value chain framework","1998","
<p>The purpose of this research is to show through a case study how the extranet has been used by one specific company to significantly reduce operating costs. The activities of the company are analyzed within the framework of the value chain concept developed by Porter. This, it is felt, will provide a greater insight into how the extranet can be used to improve profit margins. Prior research in this area has either been of a conceptual nature (explaining theoretically how the extranet should be employed) or of a survey nature (examining, by means of a survey instrument, the benefits accruing to companies that have adopted the extranet). This study is different in that it examines in detail, by means of a case study, how the extranet influences a retail company's chain of activities and reduces the attendant costs thereon.</p>","Computer Science"
"WOS:000078927600001","Securing threshold cryptosystems against chosen ciphertext attack","1998","
<p>For the most compelling applications of threshold cryptosystems, security against chosen ciphertext attack seems to be a requirement. However, there appear to be no practical threshold cryptosystems in the literature that are provably chosen-ciphertext secure, even in the idealized random hash function model. The contribution of this paper is to present two very practical threshold cryptosystems, and to prove that they are secure against chosen ciphertext attack in the random hash function model.</p>","Computer Science, Theory & Methods"
"WOS:000078927600001","Securing threshold cryptosystems against chosen ciphertext attack","1998","
<p>For the most compelling applications of threshold cryptosystems, security against chosen ciphertext attack seems to be a requirement. However, there appear to be no practical threshold cryptosystems in the literature that are provably chosen-ciphertext secure, even in the idealized random hash function model. The contribution of this paper is to present two very practical threshold cryptosystems, and to prove that they are secure against chosen ciphertext attack in the random hash function model.</p>","Computer Science"
"WOS:000082115900028","Multi-level strategy for computer-assisted transbronchial biopsy","1998","
<p>The Computer-Assisted Transbronchial Biopsy project involves the registration, without any external localization device, of a pre-operative 3D CT scan of the thoracic cavity (showing a tumor that requires a needle biopsy), and an intra-operative endoscopic 2D image sequence, in order to provide an assistance to a transbronchial puncture of the tumor. Because of the specific difficulties resulting from the processed data, original image processing methods were elaborated and a multi-level strategy is introduced. For each analysis level, the relevant information to process and the corresponding algorithms are defined. This multi-level strategy then achieves the best possible accuracy. The results presented here demonstrate that it is possible to localize precisely the endoscopic camera within the CT data coordinate system. The computer can thus synthesize in near real-time the CT-derived Virtual view that corresponds to the actual endoscopic real view.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000082115900028","Multi-level strategy for computer-assisted transbronchial biopsy","1998","
<p>The Computer-Assisted Transbronchial Biopsy project involves the registration, without any external localization device, of a pre-operative 3D CT scan of the thoracic cavity (showing a tumor that requires a needle biopsy), and an intra-operative endoscopic 2D image sequence, in order to provide an assistance to a transbronchial puncture of the tumor. Because of the specific difficulties resulting from the processed data, original image processing methods were elaborated and a multi-level strategy is introduced. For each analysis level, the relevant information to process and the corresponding algorithms are defined. This multi-level strategy then achieves the best possible accuracy. The results presented here demonstrate that it is possible to localize precisely the endoscopic camera within the CT data coordinate system. The computer can thus synthesize in near real-time the CT-derived Virtual view that corresponds to the actual endoscopic real view.</p>","Computer Science, Theory & Methods"
"WOS:000082115900028","Multi-level strategy for computer-assisted transbronchial biopsy","1998","
<p>The Computer-Assisted Transbronchial Biopsy project involves the registration, without any external localization device, of a pre-operative 3D CT scan of the thoracic cavity (showing a tumor that requires a needle biopsy), and an intra-operative endoscopic 2D image sequence, in order to provide an assistance to a transbronchial puncture of the tumor. Because of the specific difficulties resulting from the processed data, original image processing methods were elaborated and a multi-level strategy is introduced. For each analysis level, the relevant information to process and the corresponding algorithms are defined. This multi-level strategy then achieves the best possible accuracy. The results presented here demonstrate that it is possible to localize precisely the endoscopic camera within the CT data coordinate system. The computer can thus synthesize in near real-time the CT-derived Virtual view that corresponds to the actual endoscopic real view.</p>","Computer Science"
"WOS:000072660200002","Information retrieval, imaging and probabilistic logic","1998","
<p>Imaging is a class of non-Bayesian methods for the revision of probability density functions originally proposed as a semantics for conditional logic. Two of these revision functions, standard imaging and general imaging, have successfully been applied to modelling information retrieval by Crestani and van Rijsbergen. Due to the problematic nature of a ""direct"" implementation of imaging-revision functions, in this paper we propose their alternative implementation by representing the semantic structure that underlies imaging-based conditional logics in the language of a probabilistic (Bayesian) logic. Besides showing the potential of this ""Bayesian"" tool for the representation of non-Bayesian revision functions, recasting these models of information retrieval in such a general purpose knowledge representation and reasoning tool paves the way to a possible integration of these models with other more I(R-oriented models of IR, and to the exploitation of general-purpose domain-knowledge.</p>","Computer Science, Artificial Intelligence"
"WOS:000072660200002","Information retrieval, imaging and probabilistic logic","1998","
<p>Imaging is a class of non-Bayesian methods for the revision of probability density functions originally proposed as a semantics for conditional logic. Two of these revision functions, standard imaging and general imaging, have successfully been applied to modelling information retrieval by Crestani and van Rijsbergen. Due to the problematic nature of a ""direct"" implementation of imaging-revision functions, in this paper we propose their alternative implementation by representing the semantic structure that underlies imaging-based conditional logics in the language of a probabilistic (Bayesian) logic. Besides showing the potential of this ""Bayesian"" tool for the representation of non-Bayesian revision functions, recasting these models of information retrieval in such a general purpose knowledge representation and reasoning tool paves the way to a possible integration of these models with other more I(R-oriented models of IR, and to the exploitation of general-purpose domain-knowledge.</p>","Computer Science"
"WOS:000075611300028","Approximate frequency beam command of the RPFSR system in the ground based coordinate system","1998","
<p>The phase and frequency commands of a rotating radar system, that utilizes the frequency scanning and phase shifters to steer the beam in the azimuth and elevation directions, respectively, are derived in terms of the angles of the ground based coordinate system. The frequency equation derived is approximated to a simple form to reduce the calculation time for real time multi-function radar systems. It is shown that the approximate Frequency commands are in good agreement with the exact ones if the range of the azimuth scanning is not too wide.</p>","Computer Science, Hardware & Architecture"
"WOS:000075611300028","Approximate frequency beam command of the RPFSR system in the ground based coordinate system","1998","
<p>The phase and frequency commands of a rotating radar system, that utilizes the frequency scanning and phase shifters to steer the beam in the azimuth and elevation directions, respectively, are derived in terms of the angles of the ground based coordinate system. The frequency equation derived is approximated to a simple form to reduce the calculation time for real time multi-function radar systems. It is shown that the approximate Frequency commands are in good agreement with the exact ones if the range of the azimuth scanning is not too wide.</p>","Computer Science, Information Systems"
"WOS:000075611300028","Approximate frequency beam command of the RPFSR system in the ground based coordinate system","1998","
<p>The phase and frequency commands of a rotating radar system, that utilizes the frequency scanning and phase shifters to steer the beam in the azimuth and elevation directions, respectively, are derived in terms of the angles of the ground based coordinate system. The frequency equation derived is approximated to a simple form to reduce the calculation time for real time multi-function radar systems. It is shown that the approximate Frequency commands are in good agreement with the exact ones if the range of the azimuth scanning is not too wide.</p>","Computer Science"
"WOS:000077612600018","PACE: Processor architectures for circuit emulation","1998","
<p>We describe a family of reconfigurable parallel architectures for logic emulation. They are supposed to be applicable like conventional FPGAs, while covering a larger range of circuit sizes and clock frequencies. In order to evaluate the performance of such programmable designs, we also need software methods for code generation from circuit descriptions. We propose a combination of scheduling and routing algorithms for embedding calculations into the target architecture.</p>","Computer Science, Theory & Methods"
"WOS:000077612600018","PACE: Processor architectures for circuit emulation","1998","
<p>We describe a family of reconfigurable parallel architectures for logic emulation. They are supposed to be applicable like conventional FPGAs, while covering a larger range of circuit sizes and clock frequencies. In order to evaluate the performance of such programmable designs, we also need software methods for code generation from circuit descriptions. We propose a combination of scheduling and routing algorithms for embedding calculations into the target architecture.</p>","Computer Science"
"WOS:000076442900003","Software reuse and competition: Consumer preferences in a software component market","1998","
<p>Research on software reuse has traditionally focused on reuse among employees within the same organization. In contrast, our research investigates consumer preferences in a software component market with multiple producers and consumers of software components. We propose that empirically studying competition in software component markets is crucial to discovering success factors for improving software reuse. To facilitate such research, we have developed an experimental market laboratory called SofTrade; we report here the results of two empirical studies made possible by this environment. Our results support some existing reuse maxims, such as the concept that consumers prefer to reuse components that are general and flexible. Other established reuse beliefs are not supported by our results, such as the expectation that consumers will prefer objects with unique, complex features. Overall, the research reported here strongly supports the claim that the practice of exploring software reuse activity within real or artificial software component markets produces unique insights into how we can improve software reuse.</p>","Computer Science, Software Engineering"
"WOS:000076442900003","Software reuse and competition: Consumer preferences in a software component market","1998","
<p>Research on software reuse has traditionally focused on reuse among employees within the same organization. In contrast, our research investigates consumer preferences in a software component market with multiple producers and consumers of software components. We propose that empirically studying competition in software component markets is crucial to discovering success factors for improving software reuse. To facilitate such research, we have developed an experimental market laboratory called SofTrade; we report here the results of two empirical studies made possible by this environment. Our results support some existing reuse maxims, such as the concept that consumers prefer to reuse components that are general and flexible. Other established reuse beliefs are not supported by our results, such as the expectation that consumers will prefer objects with unique, complex features. Overall, the research reported here strongly supports the claim that the practice of exploring software reuse activity within real or artificial software component markets produces unique insights into how we can improve software reuse.</p>","Computer Science"
"WOS:000077879900016","Uniform-frequency optimization in problems of the roil-and-pitch damping of moving marine objects","1998","
<p>A method of uniform-frequency optimization (H-infinity-optimization) for the synthesis of regulators that guarantee the required quality of damping of narrow-band disturbances with previously unknown spectral characteristics is proposed and justified. These disturbances act on moving objects with several stabilized outputs and several controls. An example of the synthesis of a regulator of the longitudinal motion of a ship with underwater wings is given.</p>","Computer Science, Artificial Intelligence"
"WOS:000077879900016","Uniform-frequency optimization in problems of the roil-and-pitch damping of moving marine objects","1998","
<p>A method of uniform-frequency optimization (H-infinity-optimization) for the synthesis of regulators that guarantee the required quality of damping of narrow-band disturbances with previously unknown spectral characteristics is proposed and justified. These disturbances act on moving objects with several stabilized outputs and several controls. An example of the synthesis of a regulator of the longitudinal motion of a ship with underwater wings is given.</p>","Computer Science, Cybernetics"
"WOS:000077879900016","Uniform-frequency optimization in problems of the roil-and-pitch damping of moving marine objects","1998","
<p>A method of uniform-frequency optimization (H-infinity-optimization) for the synthesis of regulators that guarantee the required quality of damping of narrow-band disturbances with previously unknown spectral characteristics is proposed and justified. These disturbances act on moving objects with several stabilized outputs and several controls. An example of the synthesis of a regulator of the longitudinal motion of a ship with underwater wings is given.</p>","Computer Science, Theory & Methods"
"WOS:000077879900016","Uniform-frequency optimization in problems of the roil-and-pitch damping of moving marine objects","1998","
<p>A method of uniform-frequency optimization (H-infinity-optimization) for the synthesis of regulators that guarantee the required quality of damping of narrow-band disturbances with previously unknown spectral characteristics is proposed and justified. These disturbances act on moving objects with several stabilized outputs and several controls. An example of the synthesis of a regulator of the longitudinal motion of a ship with underwater wings is given.</p>","Computer Science"
"WOS:000074568900005","Objectives for adopting advanced manufacturing systems: promise and performance","1998","
<p>Presents the results of an exploratory investigation of the level of importance that firms place on several business and technical objectives when they are considering AMT adoption. Mail survey data obtained from 125 manufacturing firms in the USA that had adopted a wide variety of AMT are used in this analysis. On average, these firms had placed the highest levels of importance on improving product quality, reducing manufacturing leadtimes, reducing per unit production costs and improving responsiveness to changing customer needs. Lower levels of importance were reported for the market-based objectives of increasing market share and gaining earlier entrance to market. Ln addition, although firms had also ascribed lower levels of importance to the organizational adaptation benefits of developing an integrated organization and developing management expertise, they were satisfied that implementation of the technologies had a positive influence on achieving these benefits. Exploratory factor analysis revealed that the 15 benefits that were investigated in this study can be viewed as representing three interdependent dimensions: technical/operational objectives, total quality management-based objectives, and business or marketing-based objectives.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000074568900005","Objectives for adopting advanced manufacturing systems: promise and performance","1998","
<p>Presents the results of an exploratory investigation of the level of importance that firms place on several business and technical objectives when they are considering AMT adoption. Mail survey data obtained from 125 manufacturing firms in the USA that had adopted a wide variety of AMT are used in this analysis. On average, these firms had placed the highest levels of importance on improving product quality, reducing manufacturing leadtimes, reducing per unit production costs and improving responsiveness to changing customer needs. Lower levels of importance were reported for the market-based objectives of increasing market share and gaining earlier entrance to market. Ln addition, although firms had also ascribed lower levels of importance to the organizational adaptation benefits of developing an integrated organization and developing management expertise, they were satisfied that implementation of the technologies had a positive influence on achieving these benefits. Exploratory factor analysis revealed that the 15 benefits that were investigated in this study can be viewed as representing three interdependent dimensions: technical/operational objectives, total quality management-based objectives, and business or marketing-based objectives.</p>","Computer Science"
"WOS:000076374700001","Computing moments of objects enclosed by piecewise polynomial surfaces","1998","
<p>Combining a polynomial free-form surface representation with Gauss' divergence theorem allows efficient and exact calculation of the moments of the enclosed object. For example, for a cubic representation, volume, center of mass, and the inertia tensor can be computed in seconds even for complex objects with several thousand patches while changes due to local modification of the surface geometry can be computed in real-time as feedback for animation or design. Speed and simplicity of the approach allow solving the inverse problem of modeling to match prescribed moments.</p>","Computer Science, Software Engineering"
"WOS:000076374700001","Computing moments of objects enclosed by piecewise polynomial surfaces","1998","
<p>Combining a polynomial free-form surface representation with Gauss' divergence theorem allows efficient and exact calculation of the moments of the enclosed object. For example, for a cubic representation, volume, center of mass, and the inertia tensor can be computed in seconds even for complex objects with several thousand patches while changes due to local modification of the surface geometry can be computed in real-time as feedback for animation or design. Speed and simplicity of the approach allow solving the inverse problem of modeling to match prescribed moments.</p>","Computer Science"
"WOS:000075816000005","Sparseness and roughness of foreign exchange rates","1998","
<p>An accurate multiaffine analysis of 23 foreign currency exchange rates has been performed. The roughness exponent H-1 which characterizes the excursion of the exchange rate has been numerically measured. The degree of intermittency C-1 has been also estimated. In the (H-1,C-1) phase diagram, the currency exchange rates are dispersed in a wide region around the Brownian motion value (H-1 = 0.5, C-1 = 0) and have a significantly intermittent component (C1 not equal 0).</p>","Computer Science, Interdisciplinary Applications"
"WOS:000075816000005","Sparseness and roughness of foreign exchange rates","1998","
<p>An accurate multiaffine analysis of 23 foreign currency exchange rates has been performed. The roughness exponent H-1 which characterizes the excursion of the exchange rate has been numerically measured. The degree of intermittency C-1 has been also estimated. In the (H-1,C-1) phase diagram, the currency exchange rates are dispersed in a wide region around the Brownian motion value (H-1 = 0.5, C-1 = 0) and have a significantly intermittent component (C1 not equal 0).</p>","Computer Science"
"WOS:000077605500024","Synthesizing controllers for nonlinear hybrid systems","1998","
<p>Motivated by an example from aircraft conflict resolution we seek a methodology for synthesizing controllers for nonlinear hybrid automata. We first show how game theoretic methodologies developed for this purpose for finite automata and continuous systems can be cast in a unified framework. We then present a conceptual algorithm for extending them to the hybrid setting. We conclude with a discussion of computational issues.</p>","Computer Science, Theory & Methods"
"WOS:000077605500024","Synthesizing controllers for nonlinear hybrid systems","1998","
<p>Motivated by an example from aircraft conflict resolution we seek a methodology for synthesizing controllers for nonlinear hybrid automata. We first show how game theoretic methodologies developed for this purpose for finite automata and continuous systems can be cast in a unified framework. We then present a conceptual algorithm for extending them to the hybrid setting. We conclude with a discussion of computational issues.</p>","Computer Science"
"WOS:000076106600018","Computational chemical analysis of the highly sensitive detection of bromate in ion chromatography","1998","
<p>Computational chemistry that can predict the spectra of a variety of compounds that-cannot be obtained as pure compounds was used to study the highly sensitive detection of bromate in ion chromatography. Several possible ions, molecules; and their complexes were constructed by a molecular editor, and optimized by molecular mechanics (MM2) and MOPAC (PM3) calculations. The possible electronic spectra of these ions, molecules, and complexes were then obtained by the ZINDO (INDO)-Vizualyzer in the CAChe program. The lambda maximum (lambda(max)) of the spectra and the transition dipole were calculated using the ProjectLeader program. The comparison of the experimental and predicted results indicated that Br-3(-) was the probable reaction product, and that NO2- and ClO- accelerated the reaction.</p>","Computer Science, Information Systems"
"WOS:000076106600018","Computational chemical analysis of the highly sensitive detection of bromate in ion chromatography","1998","
<p>Computational chemistry that can predict the spectra of a variety of compounds that-cannot be obtained as pure compounds was used to study the highly sensitive detection of bromate in ion chromatography. Several possible ions, molecules; and their complexes were constructed by a molecular editor, and optimized by molecular mechanics (MM2) and MOPAC (PM3) calculations. The possible electronic spectra of these ions, molecules, and complexes were then obtained by the ZINDO (INDO)-Vizualyzer in the CAChe program. The lambda maximum (lambda(max)) of the spectra and the transition dipole were calculated using the ProjectLeader program. The comparison of the experimental and predicted results indicated that Br-3(-) was the probable reaction product, and that NO2- and ClO- accelerated the reaction.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000076106600018","Computational chemical analysis of the highly sensitive detection of bromate in ion chromatography","1998","
<p>Computational chemistry that can predict the spectra of a variety of compounds that-cannot be obtained as pure compounds was used to study the highly sensitive detection of bromate in ion chromatography. Several possible ions, molecules; and their complexes were constructed by a molecular editor, and optimized by molecular mechanics (MM2) and MOPAC (PM3) calculations. The possible electronic spectra of these ions, molecules, and complexes were then obtained by the ZINDO (INDO)-Vizualyzer in the CAChe program. The lambda maximum (lambda(max)) of the spectra and the transition dipole were calculated using the ProjectLeader program. The comparison of the experimental and predicted results indicated that Br-3(-) was the probable reaction product, and that NO2- and ClO- accelerated the reaction.</p>","Computer Science"
"WOS:000082521300034","Palmo: Field programmable analogue and mixed-signal VLSI for evolvable hardware","1998","
<p>This paper presents novel pulse based techniques initially intended to implement signal processing functions such as analogue and mixed-signal filters, data converters and amplitude modulators. Field programmable devices using these techniques have been implemented and used on a demonstration board to implement analogue and mixed-signal arrays. The rich mix of analogue and digital functionality provided by Palmo systems combined with the fact that they may accept random configuration bit streams makes them most attractive as platforms for evolvable hardware.</p>","Computer Science, Artificial Intelligence"
"WOS:000082521300034","Palmo: Field programmable analogue and mixed-signal VLSI for evolvable hardware","1998","
<p>This paper presents novel pulse based techniques initially intended to implement signal processing functions such as analogue and mixed-signal filters, data converters and amplitude modulators. Field programmable devices using these techniques have been implemented and used on a demonstration board to implement analogue and mixed-signal arrays. The rich mix of analogue and digital functionality provided by Palmo systems combined with the fact that they may accept random configuration bit streams makes them most attractive as platforms for evolvable hardware.</p>","Computer Science, Cybernetics"
"WOS:000082521300034","Palmo: Field programmable analogue and mixed-signal VLSI for evolvable hardware","1998","
<p>This paper presents novel pulse based techniques initially intended to implement signal processing functions such as analogue and mixed-signal filters, data converters and amplitude modulators. Field programmable devices using these techniques have been implemented and used on a demonstration board to implement analogue and mixed-signal arrays. The rich mix of analogue and digital functionality provided by Palmo systems combined with the fact that they may accept random configuration bit streams makes them most attractive as platforms for evolvable hardware.</p>","Computer Science, Hardware & Architecture"
"WOS:000082521300034","Palmo: Field programmable analogue and mixed-signal VLSI for evolvable hardware","1998","
<p>This paper presents novel pulse based techniques initially intended to implement signal processing functions such as analogue and mixed-signal filters, data converters and amplitude modulators. Field programmable devices using these techniques have been implemented and used on a demonstration board to implement analogue and mixed-signal arrays. The rich mix of analogue and digital functionality provided by Palmo systems combined with the fact that they may accept random configuration bit streams makes them most attractive as platforms for evolvable hardware.</p>","Computer Science, Theory & Methods"
"WOS:000082521300034","Palmo: Field programmable analogue and mixed-signal VLSI for evolvable hardware","1998","
<p>This paper presents novel pulse based techniques initially intended to implement signal processing functions such as analogue and mixed-signal filters, data converters and amplitude modulators. Field programmable devices using these techniques have been implemented and used on a demonstration board to implement analogue and mixed-signal arrays. The rich mix of analogue and digital functionality provided by Palmo systems combined with the fact that they may accept random configuration bit streams makes them most attractive as platforms for evolvable hardware.</p>","Computer Science"
"WOS:000079185700004","Approaches of digital signature legislation","1998","
<p>This paper introduces three basic approaches of digital signature legislation. The overall aim of these approaches is to establish a framework for trust and security in open networks. As nowadays distributed working and distributed businesses widely depend upon open netwoks, such frameworks can be understand as an enabler for the development of electronic commerce. The framework for the use of digital signatures and the framework for the corresponding ""trust infrastructure"" according to the German Digital Signature Act is described in depth as an example for an accomplished legislative effort.</p>","Computer Science, Information Systems"
"WOS:000079185700004","Approaches of digital signature legislation","1998","
<p>This paper introduces three basic approaches of digital signature legislation. The overall aim of these approaches is to establish a framework for trust and security in open networks. As nowadays distributed working and distributed businesses widely depend upon open netwoks, such frameworks can be understand as an enabler for the development of electronic commerce. The framework for the use of digital signatures and the framework for the corresponding ""trust infrastructure"" according to the German Digital Signature Act is described in depth as an example for an accomplished legislative effort.</p>","Computer Science, Theory & Methods"
"WOS:000079185700004","Approaches of digital signature legislation","1998","
<p>This paper introduces three basic approaches of digital signature legislation. The overall aim of these approaches is to establish a framework for trust and security in open networks. As nowadays distributed working and distributed businesses widely depend upon open netwoks, such frameworks can be understand as an enabler for the development of electronic commerce. The framework for the use of digital signatures and the framework for the corresponding ""trust infrastructure"" according to the German Digital Signature Act is described in depth as an example for an accomplished legislative effort.</p>","Computer Science"
"WOS:000082370100008","Physical computation and parallelism (constructive postmodern physics)","1998","
<p>There is increasing evidence that information may be the basic stuff of the Universe. We consider this proposition in the light of Bohm and Hiley's Quantum Potential, the work of the ANPA group on the Combinatorial Hierarchy, and the Natural Philosophies of Gabriel Kron and Maurice Jassel. We compare and contrast the philosophical backgrounds of both these and the more conventional Copenhagen interpretation. In. conclusion we suggest that our approach should be termed Constructive Postmodern Physics.</p>","Computer Science, Theory & Methods"
"WOS:000082370100008","Physical computation and parallelism (constructive postmodern physics)","1998","
<p>There is increasing evidence that information may be the basic stuff of the Universe. We consider this proposition in the light of Bohm and Hiley's Quantum Potential, the work of the ANPA group on the Combinatorial Hierarchy, and the Natural Philosophies of Gabriel Kron and Maurice Jassel. We compare and contrast the philosophical backgrounds of both these and the more conventional Copenhagen interpretation. In. conclusion we suggest that our approach should be termed Constructive Postmodern Physics.</p>","Computer Science"
"WOS:000075712800012","I Type of strong connectivity in L-fuzzy topological spaces","1998","
<p>The aim of the paper is mainly to introduce and study I type of strong connectivity in L-fuzzy topological space. It preserves many good properties of connected sets in general topological spaces: The sets between I type of strongly connected set and its semiclosure are I type of strongly connected. Any union of I type of strongly connected sets with non-null intersection is I type of strongly connected. The images of irresolute order-homomorphisms of I type of strongly connected sets are I type of strongly connected. For I type of strong connectivity the K. Fan's Theorem holds. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science, Theory & Methods"
"WOS:000075712800012","I Type of strong connectivity in L-fuzzy topological spaces","1998","
<p>The aim of the paper is mainly to introduce and study I type of strong connectivity in L-fuzzy topological space. It preserves many good properties of connected sets in general topological spaces: The sets between I type of strongly connected set and its semiclosure are I type of strongly connected. Any union of I type of strongly connected sets with non-null intersection is I type of strongly connected. The images of irresolute order-homomorphisms of I type of strongly connected sets are I type of strongly connected. For I type of strong connectivity the K. Fan's Theorem holds. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science"
"WOS:000077585300043","Mobilizing information in journal articles","1998","
<p>This paper explores the disaggregation and reaggregation of scientific and technical journal articles by students and faculty members. Journal article disaggregation refers to the ability to access and manipulate individual components of a document, such as its figures, conclusions, or references. In reaggregation, article components are compiled and integrated into a new written work. Data gathered in the NSF/DARPA/NASA Digital Library Initiatives (DLI) project at the University of Illinois [see the project homepage at http:Ndli.grainger.uiuc.edu/] are analyzed to describe how components are mobilized in the work of researchers as they identify, retrieve, read, and use material in articles of interest. Results lead to a discussion of the nature of metadata, the role of context in constraining component use, and the complex assemblage of information system use.</p>","Computer Science, Information Systems"
"WOS:000077585300043","Mobilizing information in journal articles","1998","
<p>This paper explores the disaggregation and reaggregation of scientific and technical journal articles by students and faculty members. Journal article disaggregation refers to the ability to access and manipulate individual components of a document, such as its figures, conclusions, or references. In reaggregation, article components are compiled and integrated into a new written work. Data gathered in the NSF/DARPA/NASA Digital Library Initiatives (DLI) project at the University of Illinois [see the project homepage at http:Ndli.grainger.uiuc.edu/] are analyzed to describe how components are mobilized in the work of researchers as they identify, retrieve, read, and use material in articles of interest. Results lead to a discussion of the nature of metadata, the role of context in constraining component use, and the complex assemblage of information system use.</p>","Computer Science"
"WOS:000082727600035","Defeasible constraint solving over the booleans","1998","
<p>This paper extends a constraint solver over the booleans to make it defeasible, and embeddable in a general architecture for defeasible constraint solving. This complements previous work on defeasible solvers over finite domains and rational numbers. Similar to the latter, one approach uses witness variables to detect minimal conflict sets of constraints, but adds important overhead. Other approaches use data dependencies, as in finite domains, to detect conflict sets. Although these are not minimal, such approaches seem more promising due to their less complexity.</p>","Computer Science, Artificial Intelligence"
"WOS:000082727600035","Defeasible constraint solving over the booleans","1998","
<p>This paper extends a constraint solver over the booleans to make it defeasible, and embeddable in a general architecture for defeasible constraint solving. This complements previous work on defeasible solvers over finite domains and rational numbers. Similar to the latter, one approach uses witness variables to detect minimal conflict sets of constraints, but adds important overhead. Other approaches use data dependencies, as in finite domains, to detect conflict sets. Although these are not minimal, such approaches seem more promising due to their less complexity.</p>","Computer Science"
"WOS:000076096100001","An efficient algorithm for performance-optimal FPGA technology mapping with retiming","1998","
<p>It is known that most field programmable gate array (FPGA) mapping algorithms consider only combinational circuits. Pan and Liu [22] recently proposed a novel algorithm, named SeqMapII, of technology mapping with retiming for clock period minimization, Their algorithm, however, requires O(K(3)n(5)log(Kn(2))log n) run time and O(K(2)n(2)) space for sequential circuits with n gates. In practice, these requirements are too high for targeting K-lookup-table-based FPGA's implementing medium or large designs. In this paper, we present three strategies to improve the performance of the SeqMapII algorithm significantly. Our algorithm works in O(K(2)nn\P-v\log n) run time and O(K\P-v/) space, where nl is the number of labeling iterations and \P-v\ is the size of the partial flow network. In practice, both n(1) and \P-v\ are less than n. Area minimization is also considered in our algorithm based on efficient low-cost K-cut computation.</p>","Computer Science, Hardware & Architecture"
"WOS:000076096100001","An efficient algorithm for performance-optimal FPGA technology mapping with retiming","1998","
<p>It is known that most field programmable gate array (FPGA) mapping algorithms consider only combinational circuits. Pan and Liu [22] recently proposed a novel algorithm, named SeqMapII, of technology mapping with retiming for clock period minimization, Their algorithm, however, requires O(K(3)n(5)log(Kn(2))log n) run time and O(K(2)n(2)) space for sequential circuits with n gates. In practice, these requirements are too high for targeting K-lookup-table-based FPGA's implementing medium or large designs. In this paper, we present three strategies to improve the performance of the SeqMapII algorithm significantly. Our algorithm works in O(K(2)nn\P-v\log n) run time and O(K\P-v/) space, where nl is the number of labeling iterations and \P-v\ is the size of the partial flow network. In practice, both n(1) and \P-v\ are less than n. Area minimization is also considered in our algorithm based on efficient low-cost K-cut computation.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000076096100001","An efficient algorithm for performance-optimal FPGA technology mapping with retiming","1998","
<p>It is known that most field programmable gate array (FPGA) mapping algorithms consider only combinational circuits. Pan and Liu [22] recently proposed a novel algorithm, named SeqMapII, of technology mapping with retiming for clock period minimization, Their algorithm, however, requires O(K(3)n(5)log(Kn(2))log n) run time and O(K(2)n(2)) space for sequential circuits with n gates. In practice, these requirements are too high for targeting K-lookup-table-based FPGA's implementing medium or large designs. In this paper, we present three strategies to improve the performance of the SeqMapII algorithm significantly. Our algorithm works in O(K(2)nn\P-v\log n) run time and O(K\P-v/) space, where nl is the number of labeling iterations and \P-v\ is the size of the partial flow network. In practice, both n(1) and \P-v\ are less than n. Area minimization is also considered in our algorithm based on efficient low-cost K-cut computation.</p>","Computer Science"
"WOS:000071490200016","Quantizing for minimum average misclassification risk","1998","
<p>In pattern classification, a decision rule is a labeled partition of the observation space, where labels represent classes, A way to establish a decision rule is to attach a label to each code vector of a vector quantizer (VQ). When a labeled VQ is adopted as a classifier, we have to design it in such a way that high classification performance is obtained by a given number of code vectors, In this paper we propose a learning algorithm which optimizes the position of labeled code vectors in the observation space under the minimum average misclassification risk criterion.</p>","Computer Science, Artificial Intelligence"
"WOS:000071490200016","Quantizing for minimum average misclassification risk","1998","
<p>In pattern classification, a decision rule is a labeled partition of the observation space, where labels represent classes, A way to establish a decision rule is to attach a label to each code vector of a vector quantizer (VQ). When a labeled VQ is adopted as a classifier, we have to design it in such a way that high classification performance is obtained by a given number of code vectors, In this paper we propose a learning algorithm which optimizes the position of labeled code vectors in the observation space under the minimum average misclassification risk criterion.</p>","Computer Science, Hardware & Architecture"
"WOS:000071490200016","Quantizing for minimum average misclassification risk","1998","
<p>In pattern classification, a decision rule is a labeled partition of the observation space, where labels represent classes, A way to establish a decision rule is to attach a label to each code vector of a vector quantizer (VQ). When a labeled VQ is adopted as a classifier, we have to design it in such a way that high classification performance is obtained by a given number of code vectors, In this paper we propose a learning algorithm which optimizes the position of labeled code vectors in the observation space under the minimum average misclassification risk criterion.</p>","Computer Science, Theory & Methods"
"WOS:000071490200016","Quantizing for minimum average misclassification risk","1998","
<p>In pattern classification, a decision rule is a labeled partition of the observation space, where labels represent classes, A way to establish a decision rule is to attach a label to each code vector of a vector quantizer (VQ). When a labeled VQ is adopted as a classifier, we have to design it in such a way that high classification performance is obtained by a given number of code vectors, In this paper we propose a learning algorithm which optimizes the position of labeled code vectors in the observation space under the minimum average misclassification risk criterion.</p>","Computer Science"
"WOS:000077585300013","Representing moving images: Implications for developers of digital video collections","1998","
<p>Technological advances in the way that video information is recorded, transmitted and stored have contributed to the widespread growth of digital video collections in many areas. However, technology for effective video retrieval has not kept pace with the technology of digital video production. An important factor in this retrieval is the extent to which representations share congruence with the videos from which they are derived. The use of text-based representations for visual information has in many cases been less than satisfactory and has generated increasing interest in image-based representations for the retrieval of videos. For moving image retrieval systems to be effective, the representations they employ must reflect the same types of distinctions and differentiation that users would make given the complete video. This paper reports on an investigation which utilized multidimensional scaling (MDS) of paired comparison judgments as a means of evaluating representations for moving image documents. The dimensional dispersion of 11,708 judgements for moving images and their text-based or image-based representations were analyzed for congruence between judgments for the full moving image documents and the various representations. This paper discusses the implications of the findings for visual information retrieval and the development of digital video collections.</p>","Computer Science, Information Systems"
"WOS:000077585300013","Representing moving images: Implications for developers of digital video collections","1998","
<p>Technological advances in the way that video information is recorded, transmitted and stored have contributed to the widespread growth of digital video collections in many areas. However, technology for effective video retrieval has not kept pace with the technology of digital video production. An important factor in this retrieval is the extent to which representations share congruence with the videos from which they are derived. The use of text-based representations for visual information has in many cases been less than satisfactory and has generated increasing interest in image-based representations for the retrieval of videos. For moving image retrieval systems to be effective, the representations they employ must reflect the same types of distinctions and differentiation that users would make given the complete video. This paper reports on an investigation which utilized multidimensional scaling (MDS) of paired comparison judgments as a means of evaluating representations for moving image documents. The dimensional dispersion of 11,708 judgements for moving images and their text-based or image-based representations were analyzed for congruence between judgments for the full moving image documents and the various representations. This paper discusses the implications of the findings for visual information retrieval and the development of digital video collections.</p>","Computer Science"
"WOS:000078978900001","Algorithm 786: Multiple-precision complex arithmetic and functions","1998","
<p>This article describes a collection of Fortran routines for multiple-precision complex arithmetic and elementary functions. The package provides good exception handling, flexible input and output, trace features, and results that are almost always correctly rounded. For best efficiency on different machines, the user can change the arithmetic type used to represent the multiple-precision numbers.</p>","Computer Science, Software Engineering"
"WOS:000078978900001","Algorithm 786: Multiple-precision complex arithmetic and functions","1998","
<p>This article describes a collection of Fortran routines for multiple-precision complex arithmetic and elementary functions. The package provides good exception handling, flexible input and output, trace features, and results that are almost always correctly rounded. For best efficiency on different machines, the user can change the arithmetic type used to represent the multiple-precision numbers.</p>","Computer Science"
"WOS:000073274600004","A multi-objective course scheduling model: Combining faculty preferences for courses and times","1998","
<p>This paper formulates a multi objective zero-one course scheduling model. Through an optimization procedure, the model seeks to maximize faculty preferences to courses and times. The model seeks to assign faculty members to courses and to allocate courses to time-blocks simultaneously. The core of the procedure is formed by a matrix, with rows divided into three sections indicating course priorities (first priority in the first line, second priority in the second line, and third priority in the third line); and with letters indicating priorities for a specific time-block (""a"" for first choice, ""b"" for second choice, and ""c"" for third choice). The paper then describes application of the model to the Department of Statistics, College of Business and Economics at the United Arab Emirates University. The results of the application demonstrate the model's capability to provide an assignment that satisfies departmental policies and procedures with regard to course offerings, as well as recognizing the personal preferences of the faculty for teaching particular courses during certain time-blocks. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000073274600004","A multi-objective course scheduling model: Combining faculty preferences for courses and times","1998","
<p>This paper formulates a multi objective zero-one course scheduling model. Through an optimization procedure, the model seeks to maximize faculty preferences to courses and times. The model seeks to assign faculty members to courses and to allocate courses to time-blocks simultaneously. The core of the procedure is formed by a matrix, with rows divided into three sections indicating course priorities (first priority in the first line, second priority in the second line, and third priority in the third line); and with letters indicating priorities for a specific time-block (""a"" for first choice, ""b"" for second choice, and ""c"" for third choice). The paper then describes application of the model to the Department of Statistics, College of Business and Economics at the United Arab Emirates University. The results of the application demonstrate the model's capability to provide an assignment that satisfies departmental policies and procedures with regard to course offerings, as well as recognizing the personal preferences of the faculty for teaching particular courses during certain time-blocks. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science"
"WOS:000071143000013","A note to the sum of fuzzy variables","1998","
<p>We generalize the results of Rao and Rashed ((1981), 285-292) and the results of Nahmias ((1978), 97-110) for the membership function of a finite sum of mutually unrelated fuzzy variables. The main result is to give a necessary and sufficient condition to the following problem: if X-1,X-2,...,X-n are mutually unrelated fuzzy variables with common membership function mu and alpha(1),alpha(2),...alpha(n) are real numbers satisfying alpha(i) greater than or equal to 0 for every i and Sigma(i=1)(n) alpha(i), when does Z = Sigma(i=1)(n), alpha(i)X(i) have the same membership function mu? (C) 1998 Elsevier Science B.V.</p>","Computer Science, Theory & Methods"
"WOS:000071143000013","A note to the sum of fuzzy variables","1998","
<p>We generalize the results of Rao and Rashed ((1981), 285-292) and the results of Nahmias ((1978), 97-110) for the membership function of a finite sum of mutually unrelated fuzzy variables. The main result is to give a necessary and sufficient condition to the following problem: if X-1,X-2,...,X-n are mutually unrelated fuzzy variables with common membership function mu and alpha(1),alpha(2),...alpha(n) are real numbers satisfying alpha(i) greater than or equal to 0 for every i and Sigma(i=1)(n) alpha(i), when does Z = Sigma(i=1)(n), alpha(i)X(i) have the same membership function mu? (C) 1998 Elsevier Science B.V.</p>","Computer Science"
"WOS:000073115500007","Estimating coverage of radio transmission into and within buildings at 900, 1800, and 2300 MHz","1998","
<p>Investigations of propagation into and within buildings at 900, 1800, and 2300 MHz have been undertaken, using buildings both in the University of Liverpool precinct and in the commercial center of the city of Liverpool, United Kingdom. The emphasis of this article is the modeling of radio transmission into buildings that uses the measured penetration loss values in order to adjust the propagation models developed for the outside areas, and the modeling of radio transmission within buildings, starting with the simple distance-power law. Those models are very useful for economical reasons and are sufficiently fast for rapidly estimating coverage regions. However, more accurate models have been developed from experimental results, and are presented here. For those more precise models to work properly, it is necessary to know the environment and all the variables that may influence the propagation of the radio signals.</p>","Computer Science, Hardware & Architecture"
"WOS:000073115500007","Estimating coverage of radio transmission into and within buildings at 900, 1800, and 2300 MHz","1998","
<p>Investigations of propagation into and within buildings at 900, 1800, and 2300 MHz have been undertaken, using buildings both in the University of Liverpool precinct and in the commercial center of the city of Liverpool, United Kingdom. The emphasis of this article is the modeling of radio transmission into buildings that uses the measured penetration loss values in order to adjust the propagation models developed for the outside areas, and the modeling of radio transmission within buildings, starting with the simple distance-power law. Those models are very useful for economical reasons and are sufficiently fast for rapidly estimating coverage regions. However, more accurate models have been developed from experimental results, and are presented here. For those more precise models to work properly, it is necessary to know the environment and all the variables that may influence the propagation of the radio signals.</p>","Computer Science, Information Systems"
"WOS:000073115500007","Estimating coverage of radio transmission into and within buildings at 900, 1800, and 2300 MHz","1998","
<p>Investigations of propagation into and within buildings at 900, 1800, and 2300 MHz have been undertaken, using buildings both in the University of Liverpool precinct and in the commercial center of the city of Liverpool, United Kingdom. The emphasis of this article is the modeling of radio transmission into buildings that uses the measured penetration loss values in order to adjust the propagation models developed for the outside areas, and the modeling of radio transmission within buildings, starting with the simple distance-power law. Those models are very useful for economical reasons and are sufficiently fast for rapidly estimating coverage regions. However, more accurate models have been developed from experimental results, and are presented here. For those more precise models to work properly, it is necessary to know the environment and all the variables that may influence the propagation of the radio signals.</p>","Computer Science"
"WOS:000076335200001","Reliable and efficient computation of optical flow","1998","
<p>In this paper, we present two very efficient and accurate algorithms for computing optical flow. The first is a modified gradient-based regularization method, and the other is an SSD-based regularization method. For the gradient-based method, to amend the errors in the discrete image flow equation caused by numerical differentiation as well as temporal and spatial aliasing in the brightness function, we propose to selectively combine the image flow constraint and a contour-based flow constraint into the data constraint by using a reliability measure. Each data constraint is appropriately normalized to obtain an approximate minimum distance (of the data point to the linear flow equation) constraint instead of the conventional linear flow constraint. These modifications lead to robust and accurate optical flow estimation. We propose an incomplete Cholesky preconditioned conjugate gradient algorithm to solve the resulting large and sparse linear system efficiently. Our SSD-based regularization method uses a normalized SSD measure (based on a similar reasoning as in the gradient-based scheme) as the data constraint in a regularization framework. The nonlinear conjugate gradient algorithm in conjunction with an incomplete Cholesky preconditioning is developed to solve the resulting nonlinear minimization problem. Experimental results on synthetic and real image sequences for these two algorithms are given to demonstrate their performance in comparison with competing methods reported in literature.</p>","Computer Science, Artificial Intelligence"
"WOS:000076335200001","Reliable and efficient computation of optical flow","1998","
<p>In this paper, we present two very efficient and accurate algorithms for computing optical flow. The first is a modified gradient-based regularization method, and the other is an SSD-based regularization method. For the gradient-based method, to amend the errors in the discrete image flow equation caused by numerical differentiation as well as temporal and spatial aliasing in the brightness function, we propose to selectively combine the image flow constraint and a contour-based flow constraint into the data constraint by using a reliability measure. Each data constraint is appropriately normalized to obtain an approximate minimum distance (of the data point to the linear flow equation) constraint instead of the conventional linear flow constraint. These modifications lead to robust and accurate optical flow estimation. We propose an incomplete Cholesky preconditioned conjugate gradient algorithm to solve the resulting large and sparse linear system efficiently. Our SSD-based regularization method uses a normalized SSD measure (based on a similar reasoning as in the gradient-based scheme) as the data constraint in a regularization framework. The nonlinear conjugate gradient algorithm in conjunction with an incomplete Cholesky preconditioning is developed to solve the resulting nonlinear minimization problem. Experimental results on synthetic and real image sequences for these two algorithms are given to demonstrate their performance in comparison with competing methods reported in literature.</p>","Computer Science"
"WOS:000076679000011","Molecular dynamics simulation of zone melting","1998","
<p>We use molecular dynamics technique for simulating the zone melting process. By tuning the parameters in the molecule-molecule potential we were able to reproduce segregation and fingering at the interface.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000076679000011","Molecular dynamics simulation of zone melting","1998","
<p>We use molecular dynamics technique for simulating the zone melting process. By tuning the parameters in the molecule-molecule potential we were able to reproduce segregation and fingering at the interface.</p>","Computer Science"
"WOS:000077271700007","Fuzzy clustering of spatial binary data","1998","
<p>An iterative fuzzy clustering method is proposed to partition a set of multivariate binary observation vectors located at neighboring geographic sites. The method described here applies in a binary setup a recently proposed algorithm, called Neighborhood EM, which seeks a a partition that is both well clustered in the feature space and spatially regular [2]. This approach is derived from the EM algorithm applied to mixture models [9], viewed as an alternate optimization method [12]. The criterion optimized by EM is penalized by a spatial smoothing term that favors classes having many neighbors. The resulting algorithm has a structure similar to EM, with an unchanged. M-step and an iterative E-step. The criterion optimized by Neighborhood EM is closely related to a posterior distribution with a multilevel logistic Markov random field as prior [5, 10]. The application of this approach to binary data relies on a mixture of multivariate Bernoulli distributions [11]. Experiments on simulated spatial binary data yield encouraging results.</p>","Computer Science, Cybernetics"
"WOS:000077271700007","Fuzzy clustering of spatial binary data","1998","
<p>An iterative fuzzy clustering method is proposed to partition a set of multivariate binary observation vectors located at neighboring geographic sites. The method described here applies in a binary setup a recently proposed algorithm, called Neighborhood EM, which seeks a a partition that is both well clustered in the feature space and spatially regular [2]. This approach is derived from the EM algorithm applied to mixture models [9], viewed as an alternate optimization method [12]. The criterion optimized by EM is penalized by a spatial smoothing term that favors classes having many neighbors. The resulting algorithm has a structure similar to EM, with an unchanged. M-step and an iterative E-step. The criterion optimized by Neighborhood EM is closely related to a posterior distribution with a multilevel logistic Markov random field as prior [5, 10]. The application of this approach to binary data relies on a mixture of multivariate Bernoulli distributions [11]. Experiments on simulated spatial binary data yield encouraging results.</p>","Computer Science"
"WOS:000077581300128","NetLink: Software libraries at your fingertip","1998","
<p>NetLink unifies a local file system with a large distributed database, preserving file system access methods, and thereby making database access transparent to user programs. One objective of NetLink is to reduce software library installation and maintenance costs by providing a distributed database of software libraries transparently accessible directly by unmodified compilers and linkers. The NetLink distributed database is divided into, not necessarily disjoint, logical agent groups called secure virtual domains, each defining a trusted extranet ensuring accurate data and making it possible to include not only public domain software, but also commercial modules into NetLink.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000077581300128","NetLink: Software libraries at your fingertip","1998","
<p>NetLink unifies a local file system with a large distributed database, preserving file system access methods, and thereby making database access transparent to user programs. One objective of NetLink is to reduce software library installation and maintenance costs by providing a distributed database of software libraries transparently accessible directly by unmodified compilers and linkers. The NetLink distributed database is divided into, not necessarily disjoint, logical agent groups called secure virtual domains, each defining a trusted extranet ensuring accurate data and making it possible to include not only public domain software, but also commercial modules into NetLink.</p>","Computer Science, Software Engineering"
"WOS:000077581300128","NetLink: Software libraries at your fingertip","1998","
<p>NetLink unifies a local file system with a large distributed database, preserving file system access methods, and thereby making database access transparent to user programs. One objective of NetLink is to reduce software library installation and maintenance costs by providing a distributed database of software libraries transparently accessible directly by unmodified compilers and linkers. The NetLink distributed database is divided into, not necessarily disjoint, logical agent groups called secure virtual domains, each defining a trusted extranet ensuring accurate data and making it possible to include not only public domain software, but also commercial modules into NetLink.</p>","Computer Science, Theory & Methods"
"WOS:000077581300128","NetLink: Software libraries at your fingertip","1998","
<p>NetLink unifies a local file system with a large distributed database, preserving file system access methods, and thereby making database access transparent to user programs. One objective of NetLink is to reduce software library installation and maintenance costs by providing a distributed database of software libraries transparently accessible directly by unmodified compilers and linkers. The NetLink distributed database is divided into, not necessarily disjoint, logical agent groups called secure virtual domains, each defining a trusted extranet ensuring accurate data and making it possible to include not only public domain software, but also commercial modules into NetLink.</p>","Computer Science"
"WOS:000072740500001","Dendritic and star polymers: Classification, nomenclature, structure representation, and registration in the DuPont SCION database","1998","
<p>Classification, nomenclature, and structure representation are presented for dendritic and star polymers in SCION, a DuPont proprietary scientific and technical online database. The problems of registering dendritic and hyperbranched structural repeating units (SRUs) with odd numbers of crossing bonds are described, and solutions to circumvent registration system Limitations are discussed. A systematic approach to the classification, nomenclature, and structure representation in SCION of 20 categories of star polymers is described.</p>","Computer Science, Information Systems"
"WOS:000072740500001","Dendritic and star polymers: Classification, nomenclature, structure representation, and registration in the DuPont SCION database","1998","
<p>Classification, nomenclature, and structure representation are presented for dendritic and star polymers in SCION, a DuPont proprietary scientific and technical online database. The problems of registering dendritic and hyperbranched structural repeating units (SRUs) with odd numbers of crossing bonds are described, and solutions to circumvent registration system Limitations are discussed. A systematic approach to the classification, nomenclature, and structure representation in SCION of 20 categories of star polymers is described.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000072740500001","Dendritic and star polymers: Classification, nomenclature, structure representation, and registration in the DuPont SCION database","1998","
<p>Classification, nomenclature, and structure representation are presented for dendritic and star polymers in SCION, a DuPont proprietary scientific and technical online database. The problems of registering dendritic and hyperbranched structural repeating units (SRUs) with odd numbers of crossing bonds are described, and solutions to circumvent registration system Limitations are discussed. A systematic approach to the classification, nomenclature, and structure representation in SCION of 20 categories of star polymers is described.</p>","Computer Science"
"WOS:000075358800031","A Lattice Boltzmann scheme for semiconductor dynamics","1998","
<p>We discuss an extension of the Lattice Boltzmann method which may prove useful for the numerical study of electron transport in semiconductors.</p>","Computer Science, Hardware & Architecture"
"WOS:000075358800031","A Lattice Boltzmann scheme for semiconductor dynamics","1998","
<p>We discuss an extension of the Lattice Boltzmann method which may prove useful for the numerical study of electron transport in semiconductors.</p>","Computer Science"
"WOS:000071111000002","On computing derivatives for C-1 interpolating schemes: an optimization","1998","
<p>The application of Powell-Sabin's or Clough-Tocher's schemes to scattered data problems, as known requires the knowledge of the partial derivatives of first order at the vertices of an underlying triangulation. We study a local method for generating partial derivatives based on the minimization of the energy functional on the star of triangles sharing a node that we called a cell. The functional is associated to some piecewise polynomial function interpolating the points. The proposed method combines the global Method II by Renka and Cline (cf. [16, pp. 230-231]) with the variational approach suggested by Alfeld (cf. [2]) with care to efficiency in the computations. The locality together with some implementation strategies produces a method well suited for the treatment of a big amount of data. An improvement of the estimates is also proposed.</p>","Computer Science, Theory & Methods"
"WOS:000071111000002","On computing derivatives for C-1 interpolating schemes: an optimization","1998","
<p>The application of Powell-Sabin's or Clough-Tocher's schemes to scattered data problems, as known requires the knowledge of the partial derivatives of first order at the vertices of an underlying triangulation. We study a local method for generating partial derivatives based on the minimization of the energy functional on the star of triangles sharing a node that we called a cell. The functional is associated to some piecewise polynomial function interpolating the points. The proposed method combines the global Method II by Renka and Cline (cf. [16, pp. 230-231]) with the variational approach suggested by Alfeld (cf. [2]) with care to efficiency in the computations. The locality together with some implementation strategies produces a method well suited for the treatment of a big amount of data. An improvement of the estimates is also proposed.</p>","Computer Science"
"WOS:000073150800007","Software quality management strategies - The IBM lesson","1998","
<p>Despite continuous improvements during the past decades, controlling software quality remains the major challenge in software development projects. A look at the strategies, process, and management techniques used to improve the quality of software during the development of the AS/400 Operating System Release 1 (OS/400 R.1) at IBM Rochester yields valuable lessons and success factors for today's software development and quality efforts.</p>","Computer Science, Information Systems"
"WOS:000073150800007","Software quality management strategies - The IBM lesson","1998","
<p>Despite continuous improvements during the past decades, controlling software quality remains the major challenge in software development projects. A look at the strategies, process, and management techniques used to improve the quality of software during the development of the AS/400 Operating System Release 1 (OS/400 R.1) at IBM Rochester yields valuable lessons and success factors for today's software development and quality efforts.</p>","Computer Science"
"WOS:000074349800007","Scope Consistency: A bridge between Release Consistency and Entry Consistency","1998","
<p>Systems that maintain coherence at large granularity, such as shared virtual memory systems, suffer from false sharing and extra communication, Relaxed memory consistency models have been used to alleviate these problems, but at a cost in programming complexity. Release Consistency (RC) and Lazy Release Consistency (LRC) are accepted to offer a reasonable tradeoff between performance and programming complexity. Entry Consistency (EC) offers a more relaxed consistency model, but it requires explicit association of shared data objects with synchronization variables. The programming burden of providing such associations can be substantial.</p>
<p>This paper proposes a new consistency model for such systems, called Scope Consistency (ScC), which offers most of the performance advantages of the EC model without requiring explicit bindings between data and synchronization variables, Instead, ScC dynamically detects the associations implied by the programmer, using a programming interface similar to that of RC or LRC. We propose two ScC protocols: one that uses hardware support for fine-grained remote writes (automatic updates or AU) and the other, an all-software protocol. We compare the AU-based ScC protocol with Automatic Update Release Consistency (AURC), a modified LRC protocol that also takes advantage of automatic update support. AURC already improves performance substantially over an all-software LRC protocol. For three of the five applications we used, ScC further improves the speedups achieved by AURC by about 10%.</p>","Computer Science, Theory & Methods"
"WOS:000074349800007","Scope Consistency: A bridge between Release Consistency and Entry Consistency","1998","
<p>Systems that maintain coherence at large granularity, such as shared virtual memory systems, suffer from false sharing and extra communication, Relaxed memory consistency models have been used to alleviate these problems, but at a cost in programming complexity. Release Consistency (RC) and Lazy Release Consistency (LRC) are accepted to offer a reasonable tradeoff between performance and programming complexity. Entry Consistency (EC) offers a more relaxed consistency model, but it requires explicit association of shared data objects with synchronization variables. The programming burden of providing such associations can be substantial.</p>
<p>This paper proposes a new consistency model for such systems, called Scope Consistency (ScC), which offers most of the performance advantages of the EC model without requiring explicit bindings between data and synchronization variables, Instead, ScC dynamically detects the associations implied by the programmer, using a programming interface similar to that of RC or LRC. We propose two ScC protocols: one that uses hardware support for fine-grained remote writes (automatic updates or AU) and the other, an all-software protocol. We compare the AU-based ScC protocol with Automatic Update Release Consistency (AURC), a modified LRC protocol that also takes advantage of automatic update support. AURC already improves performance substantially over an all-software LRC protocol. For three of the five applications we used, ScC further improves the speedups achieved by AURC by about 10%.</p>","Computer Science"
"WOS:000076981700004","Data extraction and ad hoc query of an entity - Attribute - Value database","1998","
<p>Entity-attribute-value (EAV) tables form the major component of several mainstream electronic patient record systems (EPRSs). Such systems have been optimized for real-time retrieval of individual patient data. Data warehousing, on the other hand, involves cross-patient data retrieval based on values of patient attributes, with a focus on ad hoc query. Attribute-centric query is inherently more difficult when data are stored in EAV form than when they are stored conventionally. The authors illustrate their approach to the attribute-centric query problem with ACT/DB, a database for managing clinical trials data. This approach is based on meta data supporting a query front end that essentially hides the EAV/non-EAV nature of individual attributes from the user. The authors' work does not close the query problem, and they identify several complex subproblems that are still to be solved.</p>","Computer Science, Information Systems"
"WOS:000076981700004","Data extraction and ad hoc query of an entity - Attribute - Value database","1998","
<p>Entity-attribute-value (EAV) tables form the major component of several mainstream electronic patient record systems (EPRSs). Such systems have been optimized for real-time retrieval of individual patient data. Data warehousing, on the other hand, involves cross-patient data retrieval based on values of patient attributes, with a focus on ad hoc query. Attribute-centric query is inherently more difficult when data are stored in EAV form than when they are stored conventionally. The authors illustrate their approach to the attribute-centric query problem with ACT/DB, a database for managing clinical trials data. This approach is based on meta data supporting a query front end that essentially hides the EAV/non-EAV nature of individual attributes from the user. The authors' work does not close the query problem, and they identify several complex subproblems that are still to be solved.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000076981700004","Data extraction and ad hoc query of an entity - Attribute - Value database","1998","
<p>Entity-attribute-value (EAV) tables form the major component of several mainstream electronic patient record systems (EPRSs). Such systems have been optimized for real-time retrieval of individual patient data. Data warehousing, on the other hand, involves cross-patient data retrieval based on values of patient attributes, with a focus on ad hoc query. Attribute-centric query is inherently more difficult when data are stored in EAV form than when they are stored conventionally. The authors illustrate their approach to the attribute-centric query problem with ACT/DB, a database for managing clinical trials data. This approach is based on meta data supporting a query front end that essentially hides the EAV/non-EAV nature of individual attributes from the user. The authors' work does not close the query problem, and they identify several complex subproblems that are still to be solved.</p>","Computer Science"
"WOS:000078575100005","Towards logic programming based coordination in virtual worlds","1998","
<p>We propose a unified framework for coordination in multiuser Virtual Worlds, based on our experience with LogiMOO, a BinProlog+Linda based programmable shared Virtual World. LogiMOO adopts unification for pattern retrieval while using exclusively deterministic operations, unlike mast Prolog based Linda systems. Moving beyond the Linda framework LogiMOO is based on, we describe a coordination logic for agent programming based on ideas from (affine) linear logic and Java's synchronized object system as well as a set of new primitives describing uniformly Linda operation, database updates and hypothetical assumptions.</p>
<p>The main novelty is that our constructs emphasize an 'object based' approach, with synchronization information built in 'container objects' and allowing more flexible wait/notify negotiations between consumer/producer agent components, as well as inheritance and agent component reuse.</p>
<p>Our experiments with agent coding in LogiMOO and Java and interaction with external visual and logic components, show the practicality of our constructs for real life programming.</p>","Computer Science, Information Systems"
"WOS:000078575100005","Towards logic programming based coordination in virtual worlds","1998","
<p>We propose a unified framework for coordination in multiuser Virtual Worlds, based on our experience with LogiMOO, a BinProlog+Linda based programmable shared Virtual World. LogiMOO adopts unification for pattern retrieval while using exclusively deterministic operations, unlike mast Prolog based Linda systems. Moving beyond the Linda framework LogiMOO is based on, we describe a coordination logic for agent programming based on ideas from (affine) linear logic and Java's synchronized object system as well as a set of new primitives describing uniformly Linda operation, database updates and hypothetical assumptions.</p>
<p>The main novelty is that our constructs emphasize an 'object based' approach, with synchronization information built in 'container objects' and allowing more flexible wait/notify negotiations between consumer/producer agent components, as well as inheritance and agent component reuse.</p>
<p>Our experiments with agent coding in LogiMOO and Java and interaction with external visual and logic components, show the practicality of our constructs for real life programming.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000078575100005","Towards logic programming based coordination in virtual worlds","1998","
<p>We propose a unified framework for coordination in multiuser Virtual Worlds, based on our experience with LogiMOO, a BinProlog+Linda based programmable shared Virtual World. LogiMOO adopts unification for pattern retrieval while using exclusively deterministic operations, unlike mast Prolog based Linda systems. Moving beyond the Linda framework LogiMOO is based on, we describe a coordination logic for agent programming based on ideas from (affine) linear logic and Java's synchronized object system as well as a set of new primitives describing uniformly Linda operation, database updates and hypothetical assumptions.</p>
<p>The main novelty is that our constructs emphasize an 'object based' approach, with synchronization information built in 'container objects' and allowing more flexible wait/notify negotiations between consumer/producer agent components, as well as inheritance and agent component reuse.</p>
<p>Our experiments with agent coding in LogiMOO and Java and interaction with external visual and logic components, show the practicality of our constructs for real life programming.</p>","Computer Science, Theory & Methods"
"WOS:000078575100005","Towards logic programming based coordination in virtual worlds","1998","
<p>We propose a unified framework for coordination in multiuser Virtual Worlds, based on our experience with LogiMOO, a BinProlog+Linda based programmable shared Virtual World. LogiMOO adopts unification for pattern retrieval while using exclusively deterministic operations, unlike mast Prolog based Linda systems. Moving beyond the Linda framework LogiMOO is based on, we describe a coordination logic for agent programming based on ideas from (affine) linear logic and Java's synchronized object system as well as a set of new primitives describing uniformly Linda operation, database updates and hypothetical assumptions.</p>
<p>The main novelty is that our constructs emphasize an 'object based' approach, with synchronization information built in 'container objects' and allowing more flexible wait/notify negotiations between consumer/producer agent components, as well as inheritance and agent component reuse.</p>
<p>Our experiments with agent coding in LogiMOO and Java and interaction with external visual and logic components, show the practicality of our constructs for real life programming.</p>","Computer Science"
"WOS:000078928400011","Specifying the reuse context of scenario method chunks","1998","
<p>There has been considerable recent interest in scenarios for accompanying many of the various activities occurring in the development life cycle of computer based systems. Besides the integration of scenarios in methods such as Objectory and software tools such as Rationale Rose has proven useful and successful. Consequently, there is a demand for adapting existing methods to support specific design activities using scenario based approaches. The view developed in this paper is that scenario based approaches should be looked upon as reusable components. Our concern is therefore twofold: first, to represent scenario based approaches in a modular way which eases their reusability and second, to specify the design context in which these approaches can be reused in order to facilitate their integration in existing methods. The paper concentrates on these two aspects, presents an implementation of our proposal using SGML to store available scenario based approaches in a multimedia hypertext document and illustrates the retrieval of components meeting the requirements of the user by the means of SgmlQL queries.</p>","Computer Science, Theory & Methods"
"WOS:000078928400011","Specifying the reuse context of scenario method chunks","1998","
<p>There has been considerable recent interest in scenarios for accompanying many of the various activities occurring in the development life cycle of computer based systems. Besides the integration of scenarios in methods such as Objectory and software tools such as Rationale Rose has proven useful and successful. Consequently, there is a demand for adapting existing methods to support specific design activities using scenario based approaches. The view developed in this paper is that scenario based approaches should be looked upon as reusable components. Our concern is therefore twofold: first, to represent scenario based approaches in a modular way which eases their reusability and second, to specify the design context in which these approaches can be reused in order to facilitate their integration in existing methods. The paper concentrates on these two aspects, presents an implementation of our proposal using SGML to store available scenario based approaches in a multimedia hypertext document and illustrates the retrieval of components meeting the requirements of the user by the means of SgmlQL queries.</p>","Computer Science"
"WOS:000071521300003","Fuzzy model and decision of COD control for an activated sludge process","1998","
<p>An approach for fuzzy modeling and decision is presented based upon fuzzy logic inference. Modeling and decision making for process control in a real-world activated sludge process were studied. The modeling work for the process is based upon some historical on-line measurable and off-line sampling data from the process, and process operational experience. Some calculated data for the modeling resulted from estimation of the developed model. The method of estimation is also briefly introduced in this article. Performance of the fuzzy strategies or fuzzy decisions has been tested by comparing some results of fuzzy decision against results of simulation on dynamic mathematical models for the same process. The test results are satisfactory. (C) 1998 Elsevier Science B.V.</p>","Computer Science, Theory & Methods"
"WOS:000071521300003","Fuzzy model and decision of COD control for an activated sludge process","1998","
<p>An approach for fuzzy modeling and decision is presented based upon fuzzy logic inference. Modeling and decision making for process control in a real-world activated sludge process were studied. The modeling work for the process is based upon some historical on-line measurable and off-line sampling data from the process, and process operational experience. Some calculated data for the modeling resulted from estimation of the developed model. The method of estimation is also briefly introduced in this article. Performance of the fuzzy strategies or fuzzy decisions has been tested by comparing some results of fuzzy decision against results of simulation on dynamic mathematical models for the same process. The test results are satisfactory. (C) 1998 Elsevier Science B.V.</p>","Computer Science"
"WOS:000082482800069","High performance protocols for clusters of commodity workstations","1998","
<p>Over the last few years technological advances in microprocessor and network technology have improved dramatically the performance achieved in clusters of commodity workstations. Despite those impressive improvements the cost of communication processing is still high. Traditional layered structured network protocols fail to achieve high throughputs because they access data several times. Network protocols which avoid routing through the kernel can remove this limit on communication performance and support very high transmission speeds which are comparable to the proprietary interconnection found in Massively Parallel Processors.</p>","Computer Science, Hardware & Architecture"
"WOS:000082482800069","High performance protocols for clusters of commodity workstations","1998","
<p>Over the last few years technological advances in microprocessor and network technology have improved dramatically the performance achieved in clusters of commodity workstations. Despite those impressive improvements the cost of communication processing is still high. Traditional layered structured network protocols fail to achieve high throughputs because they access data several times. Network protocols which avoid routing through the kernel can remove this limit on communication performance and support very high transmission speeds which are comparable to the proprietary interconnection found in Massively Parallel Processors.</p>","Computer Science, Information Systems"
"WOS:000082482800069","High performance protocols for clusters of commodity workstations","1998","
<p>Over the last few years technological advances in microprocessor and network technology have improved dramatically the performance achieved in clusters of commodity workstations. Despite those impressive improvements the cost of communication processing is still high. Traditional layered structured network protocols fail to achieve high throughputs because they access data several times. Network protocols which avoid routing through the kernel can remove this limit on communication performance and support very high transmission speeds which are comparable to the proprietary interconnection found in Massively Parallel Processors.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000082482800069","High performance protocols for clusters of commodity workstations","1998","
<p>Over the last few years technological advances in microprocessor and network technology have improved dramatically the performance achieved in clusters of commodity workstations. Despite those impressive improvements the cost of communication processing is still high. Traditional layered structured network protocols fail to achieve high throughputs because they access data several times. Network protocols which avoid routing through the kernel can remove this limit on communication performance and support very high transmission speeds which are comparable to the proprietary interconnection found in Massively Parallel Processors.</p>","Computer Science, Software Engineering"
"WOS:000082482800069","High performance protocols for clusters of commodity workstations","1998","
<p>Over the last few years technological advances in microprocessor and network technology have improved dramatically the performance achieved in clusters of commodity workstations. Despite those impressive improvements the cost of communication processing is still high. Traditional layered structured network protocols fail to achieve high throughputs because they access data several times. Network protocols which avoid routing through the kernel can remove this limit on communication performance and support very high transmission speeds which are comparable to the proprietary interconnection found in Massively Parallel Processors.</p>","Computer Science, Theory & Methods"
"WOS:000082482800069","High performance protocols for clusters of commodity workstations","1998","
<p>Over the last few years technological advances in microprocessor and network technology have improved dramatically the performance achieved in clusters of commodity workstations. Despite those impressive improvements the cost of communication processing is still high. Traditional layered structured network protocols fail to achieve high throughputs because they access data several times. Network protocols which avoid routing through the kernel can remove this limit on communication performance and support very high transmission speeds which are comparable to the proprietary interconnection found in Massively Parallel Processors.</p>","Computer Science"
"WOS:000074562800008","An efficient fault-tolerant out-patient order entry system based on special distributed client/server architecture","1998","
<p>An automatic order entry system is very important for processing out-patient information. This system not only helps physicians to enter their orders directly, bur can also reduce order communication error and thus improve medical quality. Therefore, many hospitals have high aspirations to generate and implement direct order entry systems, but they are also concerned about the setbacks of system failure. In this paper, we present an effective and efficient fault-tolerant order entry system based on special distribution client/server architecture that satisfies the requirements of out-patient order entry very well. From the experimental results carried out on a prototype, we found that this system can improve the system response time of order entry and can also generate an operational method having a user friendly interface. The physicians can enter their orders easily, accurately, directly, flexibly and at a faster rate by making choices from standardized and personalized menus in this system.</p>","Computer Science, Information Systems"
"WOS:000074562800008","An efficient fault-tolerant out-patient order entry system based on special distributed client/server architecture","1998","
<p>An automatic order entry system is very important for processing out-patient information. This system not only helps physicians to enter their orders directly, bur can also reduce order communication error and thus improve medical quality. Therefore, many hospitals have high aspirations to generate and implement direct order entry systems, but they are also concerned about the setbacks of system failure. In this paper, we present an effective and efficient fault-tolerant order entry system based on special distribution client/server architecture that satisfies the requirements of out-patient order entry very well. From the experimental results carried out on a prototype, we found that this system can improve the system response time of order entry and can also generate an operational method having a user friendly interface. The physicians can enter their orders easily, accurately, directly, flexibly and at a faster rate by making choices from standardized and personalized menus in this system.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000074562800008","An efficient fault-tolerant out-patient order entry system based on special distributed client/server architecture","1998","
<p>An automatic order entry system is very important for processing out-patient information. This system not only helps physicians to enter their orders directly, bur can also reduce order communication error and thus improve medical quality. Therefore, many hospitals have high aspirations to generate and implement direct order entry systems, but they are also concerned about the setbacks of system failure. In this paper, we present an effective and efficient fault-tolerant order entry system based on special distribution client/server architecture that satisfies the requirements of out-patient order entry very well. From the experimental results carried out on a prototype, we found that this system can improve the system response time of order entry and can also generate an operational method having a user friendly interface. The physicians can enter their orders easily, accurately, directly, flexibly and at a faster rate by making choices from standardized and personalized menus in this system.</p>","Computer Science"
"WOS:000078575400037","Optimal broadcasting in almost trees and partial k-trees","1998","
<p>We consider message broadcasting in networks that have almost tree topology. The source node of the input network has a single message which has to be broadcasted to all nodes of the network. In every time unit each node that has already received the message can send it to one of its neighbors. A broadcasting scheme prescribes in which time unit a given node should send a message to which neighbor. It is minimum if it achieves the smallest possible time for broadcasting the message from the source to all nodes. We give the following algorithms to construct a minimum broadcasting scheme for different types of weakly cyclic networks:</p>
<p>A linear-time algorithm for networks whose cycles are node-disjoint and in which any simple path intersects at most O(1) cycles.</p>
<p>An O(n log n)-time algorithm for networks whose cycles are edge-disjoint and in which a node can belong to at most O(1) cycles.</p>
<p>An O(n(k) log n)-time algorithm for networks whose each edge-biconnected component is convertible to a tree by removal of at nest k edges.</p>
<p>We also present an O(n(4k+5))-time algorithm for constructing a minimum broadcasting scheme for partial k-trees.</p>","Computer Science, Theory & Methods"
"WOS:000078575400037","Optimal broadcasting in almost trees and partial k-trees","1998","
<p>We consider message broadcasting in networks that have almost tree topology. The source node of the input network has a single message which has to be broadcasted to all nodes of the network. In every time unit each node that has already received the message can send it to one of its neighbors. A broadcasting scheme prescribes in which time unit a given node should send a message to which neighbor. It is minimum if it achieves the smallest possible time for broadcasting the message from the source to all nodes. We give the following algorithms to construct a minimum broadcasting scheme for different types of weakly cyclic networks:</p>
<p>A linear-time algorithm for networks whose cycles are node-disjoint and in which any simple path intersects at most O(1) cycles.</p>
<p>An O(n log n)-time algorithm for networks whose cycles are edge-disjoint and in which a node can belong to at most O(1) cycles.</p>
<p>An O(n(k) log n)-time algorithm for networks whose each edge-biconnected component is convertible to a tree by removal of at nest k edges.</p>
<p>We also present an O(n(4k+5))-time algorithm for constructing a minimum broadcasting scheme for partial k-trees.</p>","Computer Science"
"WOS:000075799800002","Automatic tuning system for integrator-based continuous-time filters","1998","
<p>This paper proposes an automatic tuning system to adjust frequency characteristics of integrated continuous-time filters especially at high frequencies. Frequency characteristic deterioration of a filter using integrators with electrically controllable unity-gain frequencies can be easily evaluated and compensated even when they are affected by deviations of element values and parasitic elements. The compensation requires detection of both frequency and excess phase shifts of the integrators. Their two values are electrically detected by two detection systems usually used in the conventional frequency tuning system, The proposed system is stable, simple and easy to be implemented on an integrated circuit. As an example a 4th-order biquad bandpass filter with 10 MHz center frequency, 2 MHz passband width, and 0.5 dB passband ripples is designed using a bipolar process. Simulation results by SPICE show the effectiveness of the proposed system.</p>","Computer Science, Hardware & Architecture"
"WOS:000075799800002","Automatic tuning system for integrator-based continuous-time filters","1998","
<p>This paper proposes an automatic tuning system to adjust frequency characteristics of integrated continuous-time filters especially at high frequencies. Frequency characteristic deterioration of a filter using integrators with electrically controllable unity-gain frequencies can be easily evaluated and compensated even when they are affected by deviations of element values and parasitic elements. The compensation requires detection of both frequency and excess phase shifts of the integrators. Their two values are electrically detected by two detection systems usually used in the conventional frequency tuning system, The proposed system is stable, simple and easy to be implemented on an integrated circuit. As an example a 4th-order biquad bandpass filter with 10 MHz center frequency, 2 MHz passband width, and 0.5 dB passband ripples is designed using a bipolar process. Simulation results by SPICE show the effectiveness of the proposed system.</p>","Computer Science"
"WOS:000077087600003","Organising competitive intelligence activities in a corporate organisation","1998","
<p>This study discusses the problems of organising competitive intelligence activities in a corporate organisation. Traditionally in many large corporations the collection, interpretation and analysis of competitive information has been assigned to a specialised intelligence or competitor analysis unit in order to exploit the synergy created by centralisation. This organising mechanism has, however serious shortcomings that are considered in this study. It is debated that this centralised and systematic approach to managing and exploiting competitive information ignores the actual ways that managers and other knowledge workers utilise information resources in their work processes. An empirical study was made in a multinationally operating Finnish forest industry company in order to examine, what kind of competitive information managers and other knowledge workers need in their work processes, what were the most valuable information sources and how this information was actually utilised and communicated inside the corporation. The results of this empirical study are discussed. Some guidelines are provided to improve the process of coordinating and combining both systematically and unsystematically collected competitive information into a coherent organisational mechanism.</p>","Computer Science, Information Systems"
"WOS:000077087600003","Organising competitive intelligence activities in a corporate organisation","1998","
<p>This study discusses the problems of organising competitive intelligence activities in a corporate organisation. Traditionally in many large corporations the collection, interpretation and analysis of competitive information has been assigned to a specialised intelligence or competitor analysis unit in order to exploit the synergy created by centralisation. This organising mechanism has, however serious shortcomings that are considered in this study. It is debated that this centralised and systematic approach to managing and exploiting competitive information ignores the actual ways that managers and other knowledge workers utilise information resources in their work processes. An empirical study was made in a multinationally operating Finnish forest industry company in order to examine, what kind of competitive information managers and other knowledge workers need in their work processes, what were the most valuable information sources and how this information was actually utilised and communicated inside the corporation. The results of this empirical study are discussed. Some guidelines are provided to improve the process of coordinating and combining both systematically and unsystematically collected competitive information into a coherent organisational mechanism.</p>","Computer Science"
"WOS:000077581300065","A framework for parallel programming in Java","1998","
<p>To ease the task of programming parallel, and distributed applications, the Do! project aims at the automatic generation of distributed code from multi-threaded Java programs. We provide a parallel programming model, embedded in a framework that constraints parallelism without any extension to the Java language. This framework is described here and is used as a basis to generate distributed programs.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000077581300065","A framework for parallel programming in Java","1998","
<p>To ease the task of programming parallel, and distributed applications, the Do! project aims at the automatic generation of distributed code from multi-threaded Java programs. We provide a parallel programming model, embedded in a framework that constraints parallelism without any extension to the Java language. This framework is described here and is used as a basis to generate distributed programs.</p>","Computer Science, Software Engineering"
"WOS:000077581300065","A framework for parallel programming in Java","1998","
<p>To ease the task of programming parallel, and distributed applications, the Do! project aims at the automatic generation of distributed code from multi-threaded Java programs. We provide a parallel programming model, embedded in a framework that constraints parallelism without any extension to the Java language. This framework is described here and is used as a basis to generate distributed programs.</p>","Computer Science, Theory & Methods"
"WOS:000077581300065","A framework for parallel programming in Java","1998","
<p>To ease the task of programming parallel, and distributed applications, the Do! project aims at the automatic generation of distributed code from multi-threaded Java programs. We provide a parallel programming model, embedded in a framework that constraints parallelism without any extension to the Java language. This framework is described here and is used as a basis to generate distributed programs.</p>","Computer Science"
"WOS:000073997500004","Telecooperation within virtual organisations - potentials of ubiquitous meeting systems","1998","
<p>This paper presents an approach for ubiquitous collaboration support of dispersed work groups in a Virtual Organisation (VO) and an implementation of a prototypical environment. The paper characterises collaboration in Virtual Organisations, derives a set of requirements for collaboration support and the design of collaboration tools. Then, the concept of ""Ubiquitous Meetings"" and ""Ubiquitous Meeting Systems"" (UMS) enabling distributed group meetings as a central aspect of Virtual Organisations is presented. Finally, a prototype of a UMS developed in the project ""Virtual Meetings based on ATM"" of the GMD institutes IPSI and TKT is described.</p>","Computer Science, Information Systems"
"WOS:000073997500004","Telecooperation within virtual organisations - potentials of ubiquitous meeting systems","1998","
<p>This paper presents an approach for ubiquitous collaboration support of dispersed work groups in a Virtual Organisation (VO) and an implementation of a prototypical environment. The paper characterises collaboration in Virtual Organisations, derives a set of requirements for collaboration support and the design of collaboration tools. Then, the concept of ""Ubiquitous Meetings"" and ""Ubiquitous Meeting Systems"" (UMS) enabling distributed group meetings as a central aspect of Virtual Organisations is presented. Finally, a prototype of a UMS developed in the project ""Virtual Meetings based on ATM"" of the GMD institutes IPSI and TKT is described.</p>","Computer Science"
"WOS:000072948200005","Digital modelling and robust digital redesign of sampled-data uncertain system with input time delay","1998","
<p>Using the bilinear and tuning bilinear approximation together with the interval arithmetic operation, the discrete-time uncertain model and robust digital redesign of sampled-data uncertain system with input time delay are presented. The system matrices characterizing the state-space representation of the original uncertain system are assumed to be interval matrices. The develop discrete-time interval model tightly encloses the exactly discretized continuous-time input time-delay uncertain system, and it can be utilized for digital simulation. Based on bilinear approximation, a tuning bilinear approximation is presented to find the digitally redesign controller of the digitally controlled sampled-data uncertain system with input time delay. The digitally redesigned predictor controller is able to closely match the states of the digitally redesigned uncertain sampled-data system with those of the original continuous-time predictor controlled uncertain system.</p>","Computer Science, Theory & Methods"
"WOS:000072948200005","Digital modelling and robust digital redesign of sampled-data uncertain system with input time delay","1998","
<p>Using the bilinear and tuning bilinear approximation together with the interval arithmetic operation, the discrete-time uncertain model and robust digital redesign of sampled-data uncertain system with input time delay are presented. The system matrices characterizing the state-space representation of the original uncertain system are assumed to be interval matrices. The develop discrete-time interval model tightly encloses the exactly discretized continuous-time input time-delay uncertain system, and it can be utilized for digital simulation. Based on bilinear approximation, a tuning bilinear approximation is presented to find the digitally redesign controller of the digitally controlled sampled-data uncertain system with input time delay. The digitally redesigned predictor controller is able to closely match the states of the digitally redesigned uncertain sampled-data system with those of the original continuous-time predictor controlled uncertain system.</p>","Computer Science"
"WOS:000076216500004","Fault tolerant Faddeeva algorithm","1998","
<p>We present an algorithm based fault tolerant scheme suitable for array implementations of the Faddeeva algorithm. Our technique corrects errors due to multiple transient, intermittent, or permanent faults provided these are restricted to a single column of the array. We show how to find the location of the faulty column and to determine the correct Schur complement from the erroneous one. The fault recovery algorithm is of quadratic complexity in the number of rows of the input matrix while the hardware overhead is approximately four times the number of rows. (C) British Crown Copyright 1998.</p>","Computer Science, Theory & Methods"
"WOS:000076216500004","Fault tolerant Faddeeva algorithm","1998","
<p>We present an algorithm based fault tolerant scheme suitable for array implementations of the Faddeeva algorithm. Our technique corrects errors due to multiple transient, intermittent, or permanent faults provided these are restricted to a single column of the array. We show how to find the location of the faulty column and to determine the correct Schur complement from the erroneous one. The fault recovery algorithm is of quadratic complexity in the number of rows of the input matrix while the hardware overhead is approximately four times the number of rows. (C) British Crown Copyright 1998.</p>","Computer Science"
"WOS:000072985900001","Snowball: Scalable storage on networks of workstations with balanced load","1998","
<p>Networks of workstations are an emerging architectural paradigm for high-performance parallel and distributed systems. Exploiting networks of workstations for massive data management poses exciting challenges. We consider here the problem of managing record-structured data in such an environment. For example, managing collections of HTML documents on a cluster of WWW servers is an important application for which our approach provides support. The records are accessed by a dynamically growing set of clients based on a search key (e.g.,a URL). To scale up the throughput of client accesses with approximately constant response time, the records and thus also their access load are dynamically redistributed across a growing set of workstations. The paper addresses two problems of realistic workloads: skewed access frequencies to the records and evolving access patterns where previously cold records may become hot and vice versa. Our solution incorporates load tracking at different levels of granularity and automatically chooses the appropriate granularity for dynamic data migrations. Experimental results based on a detailed simulation model show that our method is indeed successful in providing scalable cost/performance and explicitly controlling its level.</p>","Computer Science, Information Systems"
"WOS:000072985900001","Snowball: Scalable storage on networks of workstations with balanced load","1998","
<p>Networks of workstations are an emerging architectural paradigm for high-performance parallel and distributed systems. Exploiting networks of workstations for massive data management poses exciting challenges. We consider here the problem of managing record-structured data in such an environment. For example, managing collections of HTML documents on a cluster of WWW servers is an important application for which our approach provides support. The records are accessed by a dynamically growing set of clients based on a search key (e.g.,a URL). To scale up the throughput of client accesses with approximately constant response time, the records and thus also their access load are dynamically redistributed across a growing set of workstations. The paper addresses two problems of realistic workloads: skewed access frequencies to the records and evolving access patterns where previously cold records may become hot and vice versa. Our solution incorporates load tracking at different levels of granularity and automatically chooses the appropriate granularity for dynamic data migrations. Experimental results based on a detailed simulation model show that our method is indeed successful in providing scalable cost/performance and explicitly controlling its level.</p>","Computer Science, Theory & Methods"
"WOS:000072985900001","Snowball: Scalable storage on networks of workstations with balanced load","1998","
<p>Networks of workstations are an emerging architectural paradigm for high-performance parallel and distributed systems. Exploiting networks of workstations for massive data management poses exciting challenges. We consider here the problem of managing record-structured data in such an environment. For example, managing collections of HTML documents on a cluster of WWW servers is an important application for which our approach provides support. The records are accessed by a dynamically growing set of clients based on a search key (e.g.,a URL). To scale up the throughput of client accesses with approximately constant response time, the records and thus also their access load are dynamically redistributed across a growing set of workstations. The paper addresses two problems of realistic workloads: skewed access frequencies to the records and evolving access patterns where previously cold records may become hot and vice versa. Our solution incorporates load tracking at different levels of granularity and automatically chooses the appropriate granularity for dynamic data migrations. Experimental results based on a detailed simulation model show that our method is indeed successful in providing scalable cost/performance and explicitly controlling its level.</p>","Computer Science"
"WOS:000071791000007","An improved methodology for evaluating the producibility of partially specified part designs","1998","
<p>Within the concurrent engineering (CE) paradigm, products must be designed in light of downstream lifecycle preferences and capabilities. One CE challenge is the need to estimate the downstream implications of partially specified part designs as early as possible in the design cycle. In this research, the hierarchical evaluation methodology for early design (HEMED) was developed to address this challenge for the fixed-principle design environment in the piece-part manufacturing industry. A prescriptive decision-modelling approach was used to develop this methodology. HEMED models a company's preferences and priorities using a weighted hierarchy of business objectives, and it uses utility scoring logic to evaluate a part design's performance relative to these priorities. Utility scores are aggregated in a bottom-up manner to generate the part design's HEMED rating, which consists of both a score and its associated uncertainty factor for each objective in the hierarchy. A prototype, proof-of-concept HEMED design-rating system was implemented and verified at a local aerospace manufacturing company. Several of the company's experienced production engineers believed that the HEMED prototype, while limited in scope, accurately represented their organization's preferences and priorities. They also believed that the prototypes test part ratings were accurate compared to their first-hand experience with these parts. HEMED was also compared to the other documented and validated early design evaluation methodology by a total of 28 engineers and technical managers from two aerospace manufacturing companies. For 11 of the 12 criteria that were used to rate the two methodologies, the evaluators believed that HEMED was an improvement within the fixed-principle design environment. This research demonstrated that HEMED was an improvement over the previously documented early design-rating system within the aerospace manufacturing industry. With additional testing, the researchers strongly believe that HEMED can be successfully applied and demonstrated as an improved early design evaluation approach throughout the piece-part manufacturing industry.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000071791000007","An improved methodology for evaluating the producibility of partially specified part designs","1998","
<p>Within the concurrent engineering (CE) paradigm, products must be designed in light of downstream lifecycle preferences and capabilities. One CE challenge is the need to estimate the downstream implications of partially specified part designs as early as possible in the design cycle. In this research, the hierarchical evaluation methodology for early design (HEMED) was developed to address this challenge for the fixed-principle design environment in the piece-part manufacturing industry. A prescriptive decision-modelling approach was used to develop this methodology. HEMED models a company's preferences and priorities using a weighted hierarchy of business objectives, and it uses utility scoring logic to evaluate a part design's performance relative to these priorities. Utility scores are aggregated in a bottom-up manner to generate the part design's HEMED rating, which consists of both a score and its associated uncertainty factor for each objective in the hierarchy. A prototype, proof-of-concept HEMED design-rating system was implemented and verified at a local aerospace manufacturing company. Several of the company's experienced production engineers believed that the HEMED prototype, while limited in scope, accurately represented their organization's preferences and priorities. They also believed that the prototypes test part ratings were accurate compared to their first-hand experience with these parts. HEMED was also compared to the other documented and validated early design evaluation methodology by a total of 28 engineers and technical managers from two aerospace manufacturing companies. For 11 of the 12 criteria that were used to rate the two methodologies, the evaluators believed that HEMED was an improvement within the fixed-principle design environment. This research demonstrated that HEMED was an improvement over the previously documented early design-rating system within the aerospace manufacturing industry. With additional testing, the researchers strongly believe that HEMED can be successfully applied and demonstrated as an improved early design evaluation approach throughout the piece-part manufacturing industry.</p>","Computer Science"
"WOS:000072857200009","A note on the conditional probability of fuzzy subsets of a continuous domain","1998","
<p>The notion of a mass assignment corresponding to a discrete fuzzy set is generalised to include all fuzzy sets. The definition is motivated by considering a possible mechanism by which an intelligent agent could extend set theoretic operations to fuzzy sets. This same mechanism also suggests an alternative definition for the conditional probability of fuzzy sets. It is noted that this definition is probability/possibility consistent and it is shown that in the case, where the probability distribution on the domain has a density function then the conditional probability of f given g can be expressed as the probability of f calculated relative to a conditional density function dependent on g. This result is used to develop an analytical method for calculating conditional probabilities of fuzzy subsets of the reals, where the membership functions of f and g and the density function are continuous piecewise linear functions. Such a method has applications to the development of a general semantic unification in Fril. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science, Theory & Methods"
"WOS:000072857200009","A note on the conditional probability of fuzzy subsets of a continuous domain","1998","
<p>The notion of a mass assignment corresponding to a discrete fuzzy set is generalised to include all fuzzy sets. The definition is motivated by considering a possible mechanism by which an intelligent agent could extend set theoretic operations to fuzzy sets. This same mechanism also suggests an alternative definition for the conditional probability of fuzzy sets. It is noted that this definition is probability/possibility consistent and it is shown that in the case, where the probability distribution on the domain has a density function then the conditional probability of f given g can be expressed as the probability of f calculated relative to a conditional density function dependent on g. This result is used to develop an analytical method for calculating conditional probabilities of fuzzy subsets of the reals, where the membership functions of f and g and the density function are continuous piecewise linear functions. Such a method has applications to the development of a general semantic unification in Fril. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science"
"WOS:000077438600007","Artificial neural networks for short-term energy forecasting: Accuracy and economic value","1998","
<p>Sixteen electric utilities surveyed state that use of ANNs significantly reduced errors in daily electric load forecasts, while only three found otherwise. Data for five gas utilities reinforces this result: the mean absolute percentage error (MAPE) for ANN daily gas demand forecasts was 6.4%, a 1.9% improvement over previous methods. Yet ANNs were not always best, implying opportunities for further improvement. The economic value of error reduction for electric utilities was assessed by examining operating decisions. For 19 utilities surveyed, an average of $800000/year per utility is estimated to be saved from use of ANN-based forecasts. Most benefits resulted from improved generating unit scheduling; the utilities estimated such benefits to be up to $143 annually per peak MW of demand for each 1% improvement in MAPE. This estimated worth of accuracy improvement (roughly 0.1% of annual generation O&M costs) is confirmed by solving generation scheduling and dispatch models under various levels of forecast accuracy. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science, Artificial Intelligence"
"WOS:000077438600007","Artificial neural networks for short-term energy forecasting: Accuracy and economic value","1998","
<p>Sixteen electric utilities surveyed state that use of ANNs significantly reduced errors in daily electric load forecasts, while only three found otherwise. Data for five gas utilities reinforces this result: the mean absolute percentage error (MAPE) for ANN daily gas demand forecasts was 6.4%, a 1.9% improvement over previous methods. Yet ANNs were not always best, implying opportunities for further improvement. The economic value of error reduction for electric utilities was assessed by examining operating decisions. For 19 utilities surveyed, an average of $800000/year per utility is estimated to be saved from use of ANN-based forecasts. Most benefits resulted from improved generating unit scheduling; the utilities estimated such benefits to be up to $143 annually per peak MW of demand for each 1% improvement in MAPE. This estimated worth of accuracy improvement (roughly 0.1% of annual generation O&M costs) is confirmed by solving generation scheduling and dispatch models under various levels of forecast accuracy. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science"
"WOS:000075358800060","Non local impact ionization effects in semiconductor devices","1998","
<p>Impact ionization processes define the breakdown characteristics of semiconductor devices. An accurate description of such phenomenon, however, is limited to very sophisticated device simulators such as Monte Carlo, A new physical model for the impact ionization process is presented, which accounts for dead space effects and high energy carrier transport at a Drift Diffusion level. Such model allows to define universal impact ionization coefficients which are device-geometry independent. By using available experimental data these parameters have been calculated for In0.53Ga0.47As.</p>","Computer Science, Hardware & Architecture"
"WOS:000075358800060","Non local impact ionization effects in semiconductor devices","1998","
<p>Impact ionization processes define the breakdown characteristics of semiconductor devices. An accurate description of such phenomenon, however, is limited to very sophisticated device simulators such as Monte Carlo, A new physical model for the impact ionization process is presented, which accounts for dead space effects and high energy carrier transport at a Drift Diffusion level. Such model allows to define universal impact ionization coefficients which are device-geometry independent. By using available experimental data these parameters have been calculated for In0.53Ga0.47As.</p>","Computer Science"
"WOS:000077502400008","Experimental evaluation of intelligent assistance for navigation","1998","
<p>Modern user interfaces make extensive use of navigation, a metaphor based on wayfinding in a physical space. Navigation can be an effective solution for many problems in understanding and manipulating a complex information space. Unfortunately, current research toward domain-independent, intelligent assistance for navigation lacks some important conceptual and practical tools for evaluation. This article describes an empirical study of assisted navigation that investigates the relationship between the quality of the assistance and overall performance. Our definition of quality is based on the information retrieval concepts of precision and recall. We use the limitations of the study results to motivate the development of a general-purpose navigation testbed. By separating the concerns of retrieving navigational data, maintaining and managing the resulting information space, and providing different views into the space, the testbed facilitates the development of navigational mechanisms and supports their evaluation. The testbed will act as a foundation for future work toward the automated generation of intelligent tools for navigational assistance. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science, Artificial Intelligence"
"WOS:000077502400008","Experimental evaluation of intelligent assistance for navigation","1998","
<p>Modern user interfaces make extensive use of navigation, a metaphor based on wayfinding in a physical space. Navigation can be an effective solution for many problems in understanding and manipulating a complex information space. Unfortunately, current research toward domain-independent, intelligent assistance for navigation lacks some important conceptual and practical tools for evaluation. This article describes an empirical study of assisted navigation that investigates the relationship between the quality of the assistance and overall performance. Our definition of quality is based on the information retrieval concepts of precision and recall. We use the limitations of the study results to motivate the development of a general-purpose navigation testbed. By separating the concerns of retrieving navigational data, maintaining and managing the resulting information space, and providing different views into the space, the testbed facilitates the development of navigational mechanisms and supports their evaluation. The testbed will act as a foundation for future work toward the automated generation of intelligent tools for navigational assistance. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science"
"WOS:000077525800016","A generalized one-dimensional fast multipole method with application to filtering of spherical harmonics","1998","
<p>The need to filter functions defined on the sphere arises in a number of applications, such as climate modeling, electromagnetic and acoustic scattering, and several other areas. Recently, it has been observed that the problem of uniform resolution filtering on the sphere can be performed efficiently via the fast multipole method (FMM) in one dimension. In this paper, we introduce a generalization of the FMM that leads to an accelerated version of the filtering process. Instead of multipole expansions, the scheme uses special-purpose bases constructed via the singular value decomposition of appropriately chosen submatrices of the filtering matrix. The algorithm is applicable to a fairly wide class of projection operators; its performance is illustrated with several numerical examples. (C) 1998 Academic Press.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000077525800016","A generalized one-dimensional fast multipole method with application to filtering of spherical harmonics","1998","
<p>The need to filter functions defined on the sphere arises in a number of applications, such as climate modeling, electromagnetic and acoustic scattering, and several other areas. Recently, it has been observed that the problem of uniform resolution filtering on the sphere can be performed efficiently via the fast multipole method (FMM) in one dimension. In this paper, we introduce a generalization of the FMM that leads to an accelerated version of the filtering process. Instead of multipole expansions, the scheme uses special-purpose bases constructed via the singular value decomposition of appropriately chosen submatrices of the filtering matrix. The algorithm is applicable to a fairly wide class of projection operators; its performance is illustrated with several numerical examples. (C) 1998 Academic Press.</p>","Computer Science"
"WOS:000072877000001","A multivariate analysis of publication trends in the 1980s with special reference to south-east Asia","1998","
<p>This study is a follow-up to a published Correspondence Factor Analysis (CFA) of a dataset of over 6 million bibliometric entries. In the previous paper, CFA was used to show how the 48 most prolific countries stand in relation to each with regard to their publication interests in 17 specific disciplinary areas and one multidisciplinary field over the period 1981-1992. In this paper, we illustrate how the publication profiles of these 48 countries evolved over rime during this period. We have (i) shown how analysis of the dataset highlights cutting edge versus ancient disciplines; (ii) identified the countries whose publication patterns underwent the most marked changes (e.g. the Asian dragons who chose to focus on engineering, materials sciences, computer sciences and molecular biology), and (iii) revealed the widespread attraction exerted by the publication pattern of the USA. There is, without doubt, an overall shift toward an American-style pattern that may be a true reflection of research interests worldwide but that may also be explained by the hegemony of those who hold the reins of international publication.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000072877000001","A multivariate analysis of publication trends in the 1980s with special reference to south-east Asia","1998","
<p>This study is a follow-up to a published Correspondence Factor Analysis (CFA) of a dataset of over 6 million bibliometric entries. In the previous paper, CFA was used to show how the 48 most prolific countries stand in relation to each with regard to their publication interests in 17 specific disciplinary areas and one multidisciplinary field over the period 1981-1992. In this paper, we illustrate how the publication profiles of these 48 countries evolved over rime during this period. We have (i) shown how analysis of the dataset highlights cutting edge versus ancient disciplines; (ii) identified the countries whose publication patterns underwent the most marked changes (e.g. the Asian dragons who chose to focus on engineering, materials sciences, computer sciences and molecular biology), and (iii) revealed the widespread attraction exerted by the publication pattern of the USA. There is, without doubt, an overall shift toward an American-style pattern that may be a true reflection of research interests worldwide but that may also be explained by the hegemony of those who hold the reins of international publication.</p>","Computer Science"
"WOS:000076676800008","Modelling social action for AI agents","1998","
<p>In the new AI of the 90s an important stream is artificial social intelligence. In this work basic ontological categories for social action, structure, and mind are introduced. Sociality (social action, social structure) is let emerge from the action and intelligence of individual agents in a common world. Also some aspects of the way-down - how emergent collective phenomena shape the individual mind - are examined. First, interference and dependence are defined, and then different kinds of coordination (reactive versus anticipatory; unilateral versus bilateral; selfish versus collaborative) are characterised. ""Weak social action"", based on beliefs about the mind of the other agents, and ""strong social action"", based on goals about others' minds and their actions, are distinguished. Special attention is paid to Goal Delegation and Goal Adoption that are considered as the basic ingredients of social commitment and contract, and then of exchange, cooperation, group action, and organisation. Different levels of delegation and then of autonomy of the delegated agent are described; and different levels of goal-adoption are shown to characterise true collaboration. Social goals in the minds of the group members are argued to be the real glue of joint activity, and the notion of social commitment, as different from individual and from collective commitment, is underlined. The necessity for modelling social objective structures and constraints is emphasised and the ""shared mind"" view of groups and organisations is criticised. The spontaneous and unaware emergence of a dependence structure is explained, as well as its feedback on the participants' minds and behaviours. Critical observations are presented on current confusions such as that between ""social"" and ""collective"" action, or between communication and social action.</p>
<p>The main claims of the paper are the following: (a) The real foundation of all sociality (cooperation, competition, groups, organisation, etc.) is the individual social action and mind. One cannot reduce or connect action at the collective level to action at the individual level unless one passes through the social character of the individual action. (b) Important levels of coordination and cooperation necessarily require minds and cognitive agents (beliefs, desires, intentions, etc.). (c) However, cognition, communication and agreement are not enough for modelling and implementing cooperation: emergent pre-cognitive structures and constraints should be formalised, and emergent forms of cooperation are needed also among planning and deliberative agents. (d) We are going towards a synthetic pamdigm in AI and Cognitive Science, reconciling situatedness and plans, reactivity and mental representations, cognition, emergence and self-organisation. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science, Artificial Intelligence"
"WOS:000076676800008","Modelling social action for AI agents","1998","
<p>In the new AI of the 90s an important stream is artificial social intelligence. In this work basic ontological categories for social action, structure, and mind are introduced. Sociality (social action, social structure) is let emerge from the action and intelligence of individual agents in a common world. Also some aspects of the way-down - how emergent collective phenomena shape the individual mind - are examined. First, interference and dependence are defined, and then different kinds of coordination (reactive versus anticipatory; unilateral versus bilateral; selfish versus collaborative) are characterised. ""Weak social action"", based on beliefs about the mind of the other agents, and ""strong social action"", based on goals about others' minds and their actions, are distinguished. Special attention is paid to Goal Delegation and Goal Adoption that are considered as the basic ingredients of social commitment and contract, and then of exchange, cooperation, group action, and organisation. Different levels of delegation and then of autonomy of the delegated agent are described; and different levels of goal-adoption are shown to characterise true collaboration. Social goals in the minds of the group members are argued to be the real glue of joint activity, and the notion of social commitment, as different from individual and from collective commitment, is underlined. The necessity for modelling social objective structures and constraints is emphasised and the ""shared mind"" view of groups and organisations is criticised. The spontaneous and unaware emergence of a dependence structure is explained, as well as its feedback on the participants' minds and behaviours. Critical observations are presented on current confusions such as that between ""social"" and ""collective"" action, or between communication and social action.</p>
<p>The main claims of the paper are the following: (a) The real foundation of all sociality (cooperation, competition, groups, organisation, etc.) is the individual social action and mind. One cannot reduce or connect action at the collective level to action at the individual level unless one passes through the social character of the individual action. (b) Important levels of coordination and cooperation necessarily require minds and cognitive agents (beliefs, desires, intentions, etc.). (c) However, cognition, communication and agreement are not enough for modelling and implementing cooperation: emergent pre-cognitive structures and constraints should be formalised, and emergent forms of cooperation are needed also among planning and deliberative agents. (d) We are going towards a synthetic pamdigm in AI and Cognitive Science, reconciling situatedness and plans, reactivity and mental representations, cognition, emergence and self-organisation. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science"
"WOS:000075449700004","Buffer management in real-time active database systems","1998","
<p>Real-time, active database systems (RTADBSs) have attracted the attention of researchers in recent times. Such systems are envisioned as control systems for environments as diverse as process control, network management and automated financial trading. Sensors distributed throughout the system report the state of the system to the database. Unacceptable state reports typically result in corrective actions being triggered with deadlines. Thus RTADBSs incorporate both real-time as well as active characteristics. In this paper we study buffer management in RTADBSs. Buffer management is recognized as not being a well studied area in real-time systems. As a result of our work, we postulate (Prefetching Anticipatorily and Priority basEd Replacement) (PAPER), a new buffer management scheme that relies on two strategies: prefetching and priority based buffer replacement. We report the result of studies of the performance of PAPER, as compared to that of existing buffer management algorithms. The insights derived from this paper impact both real-time database systems as well as RTADBSs. (C) 1998 Elsevier Science Inc. All rights reserved.</p>","Computer Science, Software Engineering"
"WOS:000075449700004","Buffer management in real-time active database systems","1998","
<p>Real-time, active database systems (RTADBSs) have attracted the attention of researchers in recent times. Such systems are envisioned as control systems for environments as diverse as process control, network management and automated financial trading. Sensors distributed throughout the system report the state of the system to the database. Unacceptable state reports typically result in corrective actions being triggered with deadlines. Thus RTADBSs incorporate both real-time as well as active characteristics. In this paper we study buffer management in RTADBSs. Buffer management is recognized as not being a well studied area in real-time systems. As a result of our work, we postulate (Prefetching Anticipatorily and Priority basEd Replacement) (PAPER), a new buffer management scheme that relies on two strategies: prefetching and priority based buffer replacement. We report the result of studies of the performance of PAPER, as compared to that of existing buffer management algorithms. The insights derived from this paper impact both real-time database systems as well as RTADBSs. (C) 1998 Elsevier Science Inc. All rights reserved.</p>","Computer Science, Theory & Methods"
"WOS:000075449700004","Buffer management in real-time active database systems","1998","
<p>Real-time, active database systems (RTADBSs) have attracted the attention of researchers in recent times. Such systems are envisioned as control systems for environments as diverse as process control, network management and automated financial trading. Sensors distributed throughout the system report the state of the system to the database. Unacceptable state reports typically result in corrective actions being triggered with deadlines. Thus RTADBSs incorporate both real-time as well as active characteristics. In this paper we study buffer management in RTADBSs. Buffer management is recognized as not being a well studied area in real-time systems. As a result of our work, we postulate (Prefetching Anticipatorily and Priority basEd Replacement) (PAPER), a new buffer management scheme that relies on two strategies: prefetching and priority based buffer replacement. We report the result of studies of the performance of PAPER, as compared to that of existing buffer management algorithms. The insights derived from this paper impact both real-time database systems as well as RTADBSs. (C) 1998 Elsevier Science Inc. All rights reserved.</p>","Computer Science"
"WOS:000072722900010","Isosceles planar subsets","1998","
<p>A finite planar set is k-isosceles for k greater than or equal to 3 if every k-point subset of the set contains a point equidistant from two others. There are three nonsimilar 3-isosceles sets with five points and one with six points. Eleven 4-isosceles sets with eight points are noted, and it is conjectured that no 4-isosceles set has nine points. Exactly one 4-isosceles 8-set has four points on a line, and every 4-isosceles set that includes the vertices of a square has fewer than eight points.</p>","Computer Science, Theory & Methods"
"WOS:000072722900010","Isosceles planar subsets","1998","
<p>A finite planar set is k-isosceles for k greater than or equal to 3 if every k-point subset of the set contains a point equidistant from two others. There are three nonsimilar 3-isosceles sets with five points and one with six points. Eleven 4-isosceles sets with eight points are noted, and it is conjectured that no 4-isosceles set has nine points. Exactly one 4-isosceles 8-set has four points on a line, and every 4-isosceles set that includes the vertices of a square has fewer than eight points.</p>","Computer Science"
"WOS:000073728900009","Software tools for complex distributed systems: Toward integrated tool environments","1998","
<p>The demands on software tools for the design and testing of complex distributed systems are considerable. An environment that integrates domain-specific and commercial off-the-shelf tools and that supports rapid prototyping of application-specific tools can greatly increase the functionality and usability of such tools.</p>","Computer Science, Theory & Methods"
"WOS:000073728900009","Software tools for complex distributed systems: Toward integrated tool environments","1998","
<p>The demands on software tools for the design and testing of complex distributed systems are considerable. An environment that integrates domain-specific and commercial off-the-shelf tools and that supports rapid prototyping of application-specific tools can greatly increase the functionality and usability of such tools.</p>","Computer Science"
"WOS:000077390700001","Parallelism in ILU-preconditioned GMRES","1998","
<p>A parallel implementation of the preconditioned GMRES method is described. The method is used to solve the discretized incompressible Navier-Stokes equations. A parallel implementation of the inner product is given, which appears to be scalable on a massively parallel computer. The most difficult part to parallelize is the ILU-preconditioner. We parallelize the preconditioner using ideas proposed by Bastian and Horton (P. Bastian, G. Horton, SIAM. J. Stat. Comput. 12 (1991) 1457-1470). Contrary to some other parallel methods, the required number of iterations is independent of the number of processors used. A model is presented to predict the efficiency of the method. Experiments are done on the Gray T3D, computing the solution of a two-dimensional incompressible flow. Predictions of: computing time show good correspondence with measurements. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science, Theory & Methods"
"WOS:000077390700001","Parallelism in ILU-preconditioned GMRES","1998","
<p>A parallel implementation of the preconditioned GMRES method is described. The method is used to solve the discretized incompressible Navier-Stokes equations. A parallel implementation of the inner product is given, which appears to be scalable on a massively parallel computer. The most difficult part to parallelize is the ILU-preconditioner. We parallelize the preconditioner using ideas proposed by Bastian and Horton (P. Bastian, G. Horton, SIAM. J. Stat. Comput. 12 (1991) 1457-1470). Contrary to some other parallel methods, the required number of iterations is independent of the number of processors used. A model is presented to predict the efficiency of the method. Experiments are done on the Gray T3D, computing the solution of a two-dimensional incompressible flow. Predictions of: computing time show good correspondence with measurements. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science"
"WOS:000074421500017","Evaluation of complication rates after coronary artery bypass surgery using administrative data","1998","
<p>Our objectives were (1) to determine if studying hospital complication rates after coronary artery bypass graft (CABG) surgery provides information not available when only mortality is studied, and (2) to reexplore the utility of ICD-9-CM administrative data for CABG outcomes assessment. Using data from Massachusetts, we identified CABG cohorts from 1990 and 1992 to respectively develop and validate multivariate risk adjustment models predicting in-hospital mortality and complications. The resulting models had good discrimination and calibration. In 1992, adjusted hospital complication rates ranged widely from 13.0% to 57.6%, while mortality rates ranged from 1.4% to 6.1%. Hospitals with high complication rates tended to have high mortality (r = 0.74, p = 0.006), but 2 of the 12 hospitals studied ranked quite differently when judged by complications rather than mortality. We conclude that (1) complications after CABG occur frequently and may provide information about hospital quality beyond that obtained from hospital mortality rates, and that (2) administrative data continue to be a promising resource for outcomes research.</p>","Computer Science, Information Systems"
"WOS:000074421500017","Evaluation of complication rates after coronary artery bypass surgery using administrative data","1998","
<p>Our objectives were (1) to determine if studying hospital complication rates after coronary artery bypass graft (CABG) surgery provides information not available when only mortality is studied, and (2) to reexplore the utility of ICD-9-CM administrative data for CABG outcomes assessment. Using data from Massachusetts, we identified CABG cohorts from 1990 and 1992 to respectively develop and validate multivariate risk adjustment models predicting in-hospital mortality and complications. The resulting models had good discrimination and calibration. In 1992, adjusted hospital complication rates ranged widely from 13.0% to 57.6%, while mortality rates ranged from 1.4% to 6.1%. Hospitals with high complication rates tended to have high mortality (r = 0.74, p = 0.006), but 2 of the 12 hospitals studied ranked quite differently when judged by complications rather than mortality. We conclude that (1) complications after CABG occur frequently and may provide information about hospital quality beyond that obtained from hospital mortality rates, and that (2) administrative data continue to be a promising resource for outcomes research.</p>","Computer Science"
"WOS:000074094600001","Central executive function in working memory: event-related brain potential studies","1998","
<p>Visual event-related brain potentials (ERPs) were recorded during a running memory task, in which subjects dynamically revised (updated) memory stores, and a control task not requiring maintenance of a changing memory set but utilising identical stimulus sequences and response patterns. In three experiments, ERPs associated with cognitive processes were isolated through subtraction of control potentials from ERPs acquired during updating. We provide evidence that resultant difference ERPs primarily reflected processing or processing control, as opposed to storage. These findings are consistent both with Baddeley's working memory model, which postulates separate storage and control modules, and Morris and Jones' behavioral evidence for specific involvement of Baddeley's central executive in memory updating. In addition, our ERP data indicate that updating requires processes not suggested by Morris and Jones' behavioural studies; possibly control processes engaged to reduce the effects of proactive interference. Overall the data are consistent with the discovery of an ERP correlate of central executive activity. (C) 1998 Elsevier Science B.V.</p>","Computer Science, Artificial Intelligence"
"WOS:000074094600001","Central executive function in working memory: event-related brain potential studies","1998","
<p>Visual event-related brain potentials (ERPs) were recorded during a running memory task, in which subjects dynamically revised (updated) memory stores, and a control task not requiring maintenance of a changing memory set but utilising identical stimulus sequences and response patterns. In three experiments, ERPs associated with cognitive processes were isolated through subtraction of control potentials from ERPs acquired during updating. We provide evidence that resultant difference ERPs primarily reflected processing or processing control, as opposed to storage. These findings are consistent both with Baddeley's working memory model, which postulates separate storage and control modules, and Morris and Jones' behavioral evidence for specific involvement of Baddeley's central executive in memory updating. In addition, our ERP data indicate that updating requires processes not suggested by Morris and Jones' behavioural studies; possibly control processes engaged to reduce the effects of proactive interference. Overall the data are consistent with the discovery of an ERP correlate of central executive activity. (C) 1998 Elsevier Science B.V.</p>","Computer Science"
"WOS:000078575000002","Achievements of relational database schema design theory revisited","1998","
<p>Database schema design is seen as to decide on formats for time-varying instances, on rules for supporting inferences and on semantic constraints. Schema design aims at both faithful formalization of the application and optimization at design time. It is guided by four heuristics: Separation of Aspects, Separation of Specializations, Inferential Completeness and Unique Flavor. A theory of schema design is to investigate these heuristics and to provide insight into how syntactic properties of schemas are related to worthwhile semantic properties, how desirable syntactic properties can be decided or achieved algorithmically, and how the syntactic properties determine costs of storage, queries and updates. Some well-known achievements of design theory for relational databases are reviewed: normal forms, view support, deciding implications of semantic constraints, acyclicity, design algorithms removing forbidden substructures.</p>","Computer Science, Information Systems"
"WOS:000078575000002","Achievements of relational database schema design theory revisited","1998","
<p>Database schema design is seen as to decide on formats for time-varying instances, on rules for supporting inferences and on semantic constraints. Schema design aims at both faithful formalization of the application and optimization at design time. It is guided by four heuristics: Separation of Aspects, Separation of Specializations, Inferential Completeness and Unique Flavor. A theory of schema design is to investigate these heuristics and to provide insight into how syntactic properties of schemas are related to worthwhile semantic properties, how desirable syntactic properties can be decided or achieved algorithmically, and how the syntactic properties determine costs of storage, queries and updates. Some well-known achievements of design theory for relational databases are reviewed: normal forms, view support, deciding implications of semantic constraints, acyclicity, design algorithms removing forbidden substructures.</p>","Computer Science, Theory & Methods"
"WOS:000078575000002","Achievements of relational database schema design theory revisited","1998","
<p>Database schema design is seen as to decide on formats for time-varying instances, on rules for supporting inferences and on semantic constraints. Schema design aims at both faithful formalization of the application and optimization at design time. It is guided by four heuristics: Separation of Aspects, Separation of Specializations, Inferential Completeness and Unique Flavor. A theory of schema design is to investigate these heuristics and to provide insight into how syntactic properties of schemas are related to worthwhile semantic properties, how desirable syntactic properties can be decided or achieved algorithmically, and how the syntactic properties determine costs of storage, queries and updates. Some well-known achievements of design theory for relational databases are reviewed: normal forms, view support, deciding implications of semantic constraints, acyclicity, design algorithms removing forbidden substructures.</p>","Computer Science"
"WOS:000082774600018","Reasoning about asynchronous communication in dynamically evolving object structures","1998","
<p>This paper introduces a compositional Hoare logics for reasoning about the correctness of systems composed of a dynamically evolving collection of processes (also called objects) which interact only via an asynchronous communication mechanism based on FIFO buffers.</p>","Computer Science, Theory & Methods"
"WOS:000082774600018","Reasoning about asynchronous communication in dynamically evolving object structures","1998","
<p>This paper introduces a compositional Hoare logics for reasoning about the correctness of systems composed of a dynamically evolving collection of processes (also called objects) which interact only via an asynchronous communication mechanism based on FIFO buffers.</p>","Computer Science"
"WOS:000074609600005","Neurofuzzy approaches and their application to nuclear power systems","1998","
<p>Neurofuzzy approaches (NFA) utilize a variety of neural and fuzzy synergisms that exploit a measured tolerance for imprecision and uncertainty for the purpose of enhancing flexibility and tractability in models and systems. It is theoretically expected and empirically confirmed that neurofuzzy approaches when appropriately structured allow for improved control over the modeling economy or parsimony resulting in easier to develop and modify systems. Hence, they hold considerable promise for significant enhancements in the control and safety of nuclear plant appurtenances, components and systems. Two nuclear power system applications are presented in this paper. The first is in the reactor control area. It uses neural networks to predict power trajectories and fuzzy rules that incorporate such predictions in proactive or anticipatory strategies in order to improve power maneuvers during reactor startup. The second is in the area of safety,where neural mappings are used to produce fuzzy values for epistemic variables. The methodology is extending the notion of measurement to variables with functional or operational significance and hence is referred to as virtual measurement; it is applied to flow visualization and holds considerable promise fbr improving diagnostics and hence safety in nuclear reactors.</p>","Computer Science, Artificial Intelligence"
"WOS:000074609600005","Neurofuzzy approaches and their application to nuclear power systems","1998","
<p>Neurofuzzy approaches (NFA) utilize a variety of neural and fuzzy synergisms that exploit a measured tolerance for imprecision and uncertainty for the purpose of enhancing flexibility and tractability in models and systems. It is theoretically expected and empirically confirmed that neurofuzzy approaches when appropriately structured allow for improved control over the modeling economy or parsimony resulting in easier to develop and modify systems. Hence, they hold considerable promise for significant enhancements in the control and safety of nuclear plant appurtenances, components and systems. Two nuclear power system applications are presented in this paper. The first is in the reactor control area. It uses neural networks to predict power trajectories and fuzzy rules that incorporate such predictions in proactive or anticipatory strategies in order to improve power maneuvers during reactor startup. The second is in the area of safety,where neural mappings are used to produce fuzzy values for epistemic variables. The methodology is extending the notion of measurement to variables with functional or operational significance and hence is referred to as virtual measurement; it is applied to flow visualization and holds considerable promise fbr improving diagnostics and hence safety in nuclear reactors.</p>","Computer Science"
"WOS:000077769900028","New high-order Associative Memory System based on Newton's forward interpolation","1998","
<p>A double-layer Associative Memory System (AMS) based on the Cerebella Model Articulation Controller (CMAC) (CMAC-AMS), owing to its advantages of simple structures, fast searching procedures and strong mapping capability between multidimensional input/output vectors, has been successfully used in such applications as real-time intelligent control, signal processing and pattern recognition. However, it is still suffering from its requirement for a large memory size and relatively low precision. Furthermore, the hash code used in its addressing mechanism for memory size reduction can cause a data-collision problem. In this paper, a new high-order Associative Memory System based on the Newton's forward interpolation formula (NFI-AMS) is proposed. The NFI-AMS is capable of implementing high-precision approximation to multivariable functions with arbitrarily given sampling data. A learning algorithm and a convergence theorem of the NFI-AMS are proposed. The network structure and the scheme of its learning algorithm reveal that the NFI-AMS has advantages over the conventional CMAC-type AMS in terms of high precision of learning, much less required memory size without the data-collision problem, and also has advantages over the multilayer Back Propagation (BP) neural networks in terms of much less computational effort for learning and fast convergence rate. Numerical simulations verify these advantages. The proposed NFI-AMS, therefore, has potential in many application areas as a new kind of associative memory system.</p>","Computer Science, Hardware & Architecture"
"WOS:000077769900028","New high-order Associative Memory System based on Newton's forward interpolation","1998","
<p>A double-layer Associative Memory System (AMS) based on the Cerebella Model Articulation Controller (CMAC) (CMAC-AMS), owing to its advantages of simple structures, fast searching procedures and strong mapping capability between multidimensional input/output vectors, has been successfully used in such applications as real-time intelligent control, signal processing and pattern recognition. However, it is still suffering from its requirement for a large memory size and relatively low precision. Furthermore, the hash code used in its addressing mechanism for memory size reduction can cause a data-collision problem. In this paper, a new high-order Associative Memory System based on the Newton's forward interpolation formula (NFI-AMS) is proposed. The NFI-AMS is capable of implementing high-precision approximation to multivariable functions with arbitrarily given sampling data. A learning algorithm and a convergence theorem of the NFI-AMS are proposed. The network structure and the scheme of its learning algorithm reveal that the NFI-AMS has advantages over the conventional CMAC-type AMS in terms of high precision of learning, much less required memory size without the data-collision problem, and also has advantages over the multilayer Back Propagation (BP) neural networks in terms of much less computational effort for learning and fast convergence rate. Numerical simulations verify these advantages. The proposed NFI-AMS, therefore, has potential in many application areas as a new kind of associative memory system.</p>","Computer Science, Information Systems"
"WOS:000077769900028","New high-order Associative Memory System based on Newton's forward interpolation","1998","
<p>A double-layer Associative Memory System (AMS) based on the Cerebella Model Articulation Controller (CMAC) (CMAC-AMS), owing to its advantages of simple structures, fast searching procedures and strong mapping capability between multidimensional input/output vectors, has been successfully used in such applications as real-time intelligent control, signal processing and pattern recognition. However, it is still suffering from its requirement for a large memory size and relatively low precision. Furthermore, the hash code used in its addressing mechanism for memory size reduction can cause a data-collision problem. In this paper, a new high-order Associative Memory System based on the Newton's forward interpolation formula (NFI-AMS) is proposed. The NFI-AMS is capable of implementing high-precision approximation to multivariable functions with arbitrarily given sampling data. A learning algorithm and a convergence theorem of the NFI-AMS are proposed. The network structure and the scheme of its learning algorithm reveal that the NFI-AMS has advantages over the conventional CMAC-type AMS in terms of high precision of learning, much less required memory size without the data-collision problem, and also has advantages over the multilayer Back Propagation (BP) neural networks in terms of much less computational effort for learning and fast convergence rate. Numerical simulations verify these advantages. The proposed NFI-AMS, therefore, has potential in many application areas as a new kind of associative memory system.</p>","Computer Science"
"WOS:000073764000020","Neural network correction of PM3-predicted infrared spectra","1998","
<p>We describe the application of neural networks to a theoretical problem: the correction of inaccuracies in infrared spectra as predicted by the PM3 semiempirical method. Twenty-eight ""peak-correcting"" backpropagation neural networks were trained to predict the location of a characteristic infrared peak when given a scaled topological map of one of 1116 literature spectra. The infrared spectra of 200 aliphatics were then calculated using PM3, displayed graphically in Infrared Spectrum Comparison, and submitted to the appropriate ""peak-correcting"" neural network(s) based on a rule set implemented via an interface to HyperCube's HyperChem software. Results show an average 8-fold decrease in prediction error between PM3-predicted and neural network-corrected peak locations.</p>","Computer Science, Information Systems"
"WOS:000073764000020","Neural network correction of PM3-predicted infrared spectra","1998","
<p>We describe the application of neural networks to a theoretical problem: the correction of inaccuracies in infrared spectra as predicted by the PM3 semiempirical method. Twenty-eight ""peak-correcting"" backpropagation neural networks were trained to predict the location of a characteristic infrared peak when given a scaled topological map of one of 1116 literature spectra. The infrared spectra of 200 aliphatics were then calculated using PM3, displayed graphically in Infrared Spectrum Comparison, and submitted to the appropriate ""peak-correcting"" neural network(s) based on a rule set implemented via an interface to HyperCube's HyperChem software. Results show an average 8-fold decrease in prediction error between PM3-predicted and neural network-corrected peak locations.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000073764000020","Neural network correction of PM3-predicted infrared spectra","1998","
<p>We describe the application of neural networks to a theoretical problem: the correction of inaccuracies in infrared spectra as predicted by the PM3 semiempirical method. Twenty-eight ""peak-correcting"" backpropagation neural networks were trained to predict the location of a characteristic infrared peak when given a scaled topological map of one of 1116 literature spectra. The infrared spectra of 200 aliphatics were then calculated using PM3, displayed graphically in Infrared Spectrum Comparison, and submitted to the appropriate ""peak-correcting"" neural network(s) based on a rule set implemented via an interface to HyperCube's HyperChem software. Results show an average 8-fold decrease in prediction error between PM3-predicted and neural network-corrected peak locations.</p>","Computer Science"
"WOS:000075922600008","A consensus-function artificial neural network for map-coloring","1998","
<p>A harmony theory artificial neural network solution to the map-coloring problem is presented. Map coloring aims at assigning a unique color to each area of a given map so that no two adjacent areas receive identical colors. The harmony theory implementation is able to determine whether the map-coloring problem can be solved with a predefined number of colors as well as which is the smallest number of colors that can solve the map-coloring problem. The present implementation directly encodes the given problem into the artificial neural network so that a solution is represented simply by node activation. Additionally, the consensus function of harmony theory produces a quick and definite solution to the colorability problem, obviating the need for manual validation of the result.</p>","Computer Science, Artificial Intelligence"
"WOS:000075922600008","A consensus-function artificial neural network for map-coloring","1998","
<p>A harmony theory artificial neural network solution to the map-coloring problem is presented. Map coloring aims at assigning a unique color to each area of a given map so that no two adjacent areas receive identical colors. The harmony theory implementation is able to determine whether the map-coloring problem can be solved with a predefined number of colors as well as which is the smallest number of colors that can solve the map-coloring problem. The present implementation directly encodes the given problem into the artificial neural network so that a solution is represented simply by node activation. Additionally, the consensus function of harmony theory produces a quick and definite solution to the colorability problem, obviating the need for manual validation of the result.</p>","Computer Science, Cybernetics"
"WOS:000075922600008","A consensus-function artificial neural network for map-coloring","1998","
<p>A harmony theory artificial neural network solution to the map-coloring problem is presented. Map coloring aims at assigning a unique color to each area of a given map so that no two adjacent areas receive identical colors. The harmony theory implementation is able to determine whether the map-coloring problem can be solved with a predefined number of colors as well as which is the smallest number of colors that can solve the map-coloring problem. The present implementation directly encodes the given problem into the artificial neural network so that a solution is represented simply by node activation. Additionally, the consensus function of harmony theory produces a quick and definite solution to the colorability problem, obviating the need for manual validation of the result.</p>","Computer Science"
"WOS:000073661200005","POLYAN: A computer program for polyparametric analysis of cardio-respiratory variability signals","1998","
<p>A polyparametric approach to the study of cardiovascular and respiratory systems, with the analysis of several simultaneously recorded signals and of their mutual relationships, is essential for the assessment of autonomic regulation. A computer program which implements all the procedures used in this investigative context and also makes the approach easy for researchers without an engineering background is described. Spectral techniques (univariate and bivariate), time domain techniques and non invasive baroreflex sensitivity assessment are available and integrated in the same package. In order to allow the flexibility and easy expandability required in a research tool, POLYAN was written in Matlab. POLYAN is currently used in our Hospital by several research groups and is a fundamental research tool for the interdisciplinary assessment of the autonomic nervous system. Establishing a common analysis framework, POLYAN has made sharing new findings and co-operating in research work much easier. (C) 1998 Elsevier Science Ireland Ltd. All rights reserved.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000073661200005","POLYAN: A computer program for polyparametric analysis of cardio-respiratory variability signals","1998","
<p>A polyparametric approach to the study of cardiovascular and respiratory systems, with the analysis of several simultaneously recorded signals and of their mutual relationships, is essential for the assessment of autonomic regulation. A computer program which implements all the procedures used in this investigative context and also makes the approach easy for researchers without an engineering background is described. Spectral techniques (univariate and bivariate), time domain techniques and non invasive baroreflex sensitivity assessment are available and integrated in the same package. In order to allow the flexibility and easy expandability required in a research tool, POLYAN was written in Matlab. POLYAN is currently used in our Hospital by several research groups and is a fundamental research tool for the interdisciplinary assessment of the autonomic nervous system. Establishing a common analysis framework, POLYAN has made sharing new findings and co-operating in research work much easier. (C) 1998 Elsevier Science Ireland Ltd. All rights reserved.</p>","Computer Science, Theory & Methods"
"WOS:000073661200005","POLYAN: A computer program for polyparametric analysis of cardio-respiratory variability signals","1998","
<p>A polyparametric approach to the study of cardiovascular and respiratory systems, with the analysis of several simultaneously recorded signals and of their mutual relationships, is essential for the assessment of autonomic regulation. A computer program which implements all the procedures used in this investigative context and also makes the approach easy for researchers without an engineering background is described. Spectral techniques (univariate and bivariate), time domain techniques and non invasive baroreflex sensitivity assessment are available and integrated in the same package. In order to allow the flexibility and easy expandability required in a research tool, POLYAN was written in Matlab. POLYAN is currently used in our Hospital by several research groups and is a fundamental research tool for the interdisciplinary assessment of the autonomic nervous system. Establishing a common analysis framework, POLYAN has made sharing new findings and co-operating in research work much easier. (C) 1998 Elsevier Science Ireland Ltd. All rights reserved.</p>","Computer Science"
"WOS:000082522800014","WMPI - Message passing interface for Win32 clusters","1998","
<p>This paper describes WMPI1, the first full implementation of the Message Passing Interface standard (MPI) for clusters of Microsoft's Windows platforms (Win32). Its internal architecture and user interface, and some performance test results (for release v1.1), that evaluates how much of the total underlying system capacity for communication is delivered to the MPI based parallel applications, are presented. WMPI is based on MPICH, a portable implementation of the MPI standard for UNIX(R) machines from the Argonne National Laboratory and, even when performance requisites cannot be satisfied, it is a useful tool for application developing, teaching and training. WMPI processes are also compatible with MPICH processes running on Unix workstations.</p>","Computer Science, Software Engineering"
"WOS:000082522800014","WMPI - Message passing interface for Win32 clusters","1998","
<p>This paper describes WMPI1, the first full implementation of the Message Passing Interface standard (MPI) for clusters of Microsoft's Windows platforms (Win32). Its internal architecture and user interface, and some performance test results (for release v1.1), that evaluates how much of the total underlying system capacity for communication is delivered to the MPI based parallel applications, are presented. WMPI is based on MPICH, a portable implementation of the MPI standard for UNIX(R) machines from the Argonne National Laboratory and, even when performance requisites cannot be satisfied, it is a useful tool for application developing, teaching and training. WMPI processes are also compatible with MPICH processes running on Unix workstations.</p>","Computer Science, Theory & Methods"
"WOS:000082522800014","WMPI - Message passing interface for Win32 clusters","1998","
<p>This paper describes WMPI1, the first full implementation of the Message Passing Interface standard (MPI) for clusters of Microsoft's Windows platforms (Win32). Its internal architecture and user interface, and some performance test results (for release v1.1), that evaluates how much of the total underlying system capacity for communication is delivered to the MPI based parallel applications, are presented. WMPI is based on MPICH, a portable implementation of the MPI standard for UNIX(R) machines from the Argonne National Laboratory and, even when performance requisites cannot be satisfied, it is a useful tool for application developing, teaching and training. WMPI processes are also compatible with MPICH processes running on Unix workstations.</p>","Computer Science"
"WOS:000078575100004","A cooperative approach to distributed applications engineering","1998","
<p>A central problem that researchers and industry analysts are facing is how existing distributed computing technology can be used to derive the most benefits for organisations. There is no consensus as to what constitutes a good distributed application architecture, and there seems to be a lack of understanding of how the different components of a distributed application should appropriately use the communication infrastructure to interact with one another while cooperating to achieve a common goal. Although the model of processing and transactional interactions has served well the traditional applications and database arenas, we contend here that other interaction models should be contemplated when considering a distributed application architecture. In this paper we analyse the nature of communication middleware with respect to the strength of interaction induced by different modes of distributed communication. We model an enterprise as a collection of cooperating distributed application domains, deriving a federated architecture in context to support enterprise-wide distributed computing, where tightly and loosely integrated groups of applications, as well as legacy applications, can co-exist and interoperate. This approach intends to minimise the architectural mismatch between the software architecture and organisational reality. The proposed architecture has been successfully applied in Australian industry as a framework for distributed computing.</p>","Computer Science, Information Systems"
"WOS:000078575100004","A cooperative approach to distributed applications engineering","1998","
<p>A central problem that researchers and industry analysts are facing is how existing distributed computing technology can be used to derive the most benefits for organisations. There is no consensus as to what constitutes a good distributed application architecture, and there seems to be a lack of understanding of how the different components of a distributed application should appropriately use the communication infrastructure to interact with one another while cooperating to achieve a common goal. Although the model of processing and transactional interactions has served well the traditional applications and database arenas, we contend here that other interaction models should be contemplated when considering a distributed application architecture. In this paper we analyse the nature of communication middleware with respect to the strength of interaction induced by different modes of distributed communication. We model an enterprise as a collection of cooperating distributed application domains, deriving a federated architecture in context to support enterprise-wide distributed computing, where tightly and loosely integrated groups of applications, as well as legacy applications, can co-exist and interoperate. This approach intends to minimise the architectural mismatch between the software architecture and organisational reality. The proposed architecture has been successfully applied in Australian industry as a framework for distributed computing.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000078575100004","A cooperative approach to distributed applications engineering","1998","
<p>A central problem that researchers and industry analysts are facing is how existing distributed computing technology can be used to derive the most benefits for organisations. There is no consensus as to what constitutes a good distributed application architecture, and there seems to be a lack of understanding of how the different components of a distributed application should appropriately use the communication infrastructure to interact with one another while cooperating to achieve a common goal. Although the model of processing and transactional interactions has served well the traditional applications and database arenas, we contend here that other interaction models should be contemplated when considering a distributed application architecture. In this paper we analyse the nature of communication middleware with respect to the strength of interaction induced by different modes of distributed communication. We model an enterprise as a collection of cooperating distributed application domains, deriving a federated architecture in context to support enterprise-wide distributed computing, where tightly and loosely integrated groups of applications, as well as legacy applications, can co-exist and interoperate. This approach intends to minimise the architectural mismatch between the software architecture and organisational reality. The proposed architecture has been successfully applied in Australian industry as a framework for distributed computing.</p>","Computer Science, Theory & Methods"
"WOS:000078575100004","A cooperative approach to distributed applications engineering","1998","
<p>A central problem that researchers and industry analysts are facing is how existing distributed computing technology can be used to derive the most benefits for organisations. There is no consensus as to what constitutes a good distributed application architecture, and there seems to be a lack of understanding of how the different components of a distributed application should appropriately use the communication infrastructure to interact with one another while cooperating to achieve a common goal. Although the model of processing and transactional interactions has served well the traditional applications and database arenas, we contend here that other interaction models should be contemplated when considering a distributed application architecture. In this paper we analyse the nature of communication middleware with respect to the strength of interaction induced by different modes of distributed communication. We model an enterprise as a collection of cooperating distributed application domains, deriving a federated architecture in context to support enterprise-wide distributed computing, where tightly and loosely integrated groups of applications, as well as legacy applications, can co-exist and interoperate. This approach intends to minimise the architectural mismatch between the software architecture and organisational reality. The proposed architecture has been successfully applied in Australian industry as a framework for distributed computing.</p>","Computer Science"
"WOS:000073433900012","Coloring update methods","1998","
<p>For linear update methods (such as SOR), a coloring method is introduced for which the multicolor iteration matrix has the same spectrum as the original iteration matrix. It applies to general linear systems, not necessarily arising from PDEs. When the iteration matrices are nonsingular, it is shown that they are similar to each other.</p>","Computer Science, Software Engineering"
"WOS:000073433900012","Coloring update methods","1998","
<p>For linear update methods (such as SOR), a coloring method is introduced for which the multicolor iteration matrix has the same spectrum as the original iteration matrix. It applies to general linear systems, not necessarily arising from PDEs. When the iteration matrices are nonsingular, it is shown that they are similar to each other.</p>","Computer Science"
"WOS:000074951900004","A strategy for using genetic algorithms to automate branch and fault-based testing","1998","
<p>Genetic algorithms have been used successfully to generate software test data automatically; all branches were covered with substantially fewer generated tests than simple random testing. We generated test sets which executed all branches in a variety of programs including a quadratic equation solver, remainder, linear and binary search procedures, and a triangle classifier comprising a system of five procedures. We regard the generation of test sets as a search through the input domain for appropriate inputs. The genetic algorithms generated test data to give 100% branch coverage in up to two orders of magnitude fewer tests than random testing. Whilst some of this benefit is offset by increased computation effort, the adequacy of the test data is improved by the genetic algorithm's ability to generate test sets which are at or close to the input subdomain boundaries. Genetic algorithms may be used for fault-based testing where faults associated with mistakes in branch predicates are revealed. The software has been deliberately seeded with faults in the branch predicates (i.e. mutation testing), and our system successfully killed 97% of the mutants.</p>","Computer Science, Hardware & Architecture"
"WOS:000074951900004","A strategy for using genetic algorithms to automate branch and fault-based testing","1998","
<p>Genetic algorithms have been used successfully to generate software test data automatically; all branches were covered with substantially fewer generated tests than simple random testing. We generated test sets which executed all branches in a variety of programs including a quadratic equation solver, remainder, linear and binary search procedures, and a triangle classifier comprising a system of five procedures. We regard the generation of test sets as a search through the input domain for appropriate inputs. The genetic algorithms generated test data to give 100% branch coverage in up to two orders of magnitude fewer tests than random testing. Whilst some of this benefit is offset by increased computation effort, the adequacy of the test data is improved by the genetic algorithm's ability to generate test sets which are at or close to the input subdomain boundaries. Genetic algorithms may be used for fault-based testing where faults associated with mistakes in branch predicates are revealed. The software has been deliberately seeded with faults in the branch predicates (i.e. mutation testing), and our system successfully killed 97% of the mutants.</p>","Computer Science, Information Systems"
"WOS:000074951900004","A strategy for using genetic algorithms to automate branch and fault-based testing","1998","
<p>Genetic algorithms have been used successfully to generate software test data automatically; all branches were covered with substantially fewer generated tests than simple random testing. We generated test sets which executed all branches in a variety of programs including a quadratic equation solver, remainder, linear and binary search procedures, and a triangle classifier comprising a system of five procedures. We regard the generation of test sets as a search through the input domain for appropriate inputs. The genetic algorithms generated test data to give 100% branch coverage in up to two orders of magnitude fewer tests than random testing. Whilst some of this benefit is offset by increased computation effort, the adequacy of the test data is improved by the genetic algorithm's ability to generate test sets which are at or close to the input subdomain boundaries. Genetic algorithms may be used for fault-based testing where faults associated with mistakes in branch predicates are revealed. The software has been deliberately seeded with faults in the branch predicates (i.e. mutation testing), and our system successfully killed 97% of the mutants.</p>","Computer Science, Software Engineering"
"WOS:000074951900004","A strategy for using genetic algorithms to automate branch and fault-based testing","1998","
<p>Genetic algorithms have been used successfully to generate software test data automatically; all branches were covered with substantially fewer generated tests than simple random testing. We generated test sets which executed all branches in a variety of programs including a quadratic equation solver, remainder, linear and binary search procedures, and a triangle classifier comprising a system of five procedures. We regard the generation of test sets as a search through the input domain for appropriate inputs. The genetic algorithms generated test data to give 100% branch coverage in up to two orders of magnitude fewer tests than random testing. Whilst some of this benefit is offset by increased computation effort, the adequacy of the test data is improved by the genetic algorithm's ability to generate test sets which are at or close to the input subdomain boundaries. Genetic algorithms may be used for fault-based testing where faults associated with mistakes in branch predicates are revealed. The software has been deliberately seeded with faults in the branch predicates (i.e. mutation testing), and our system successfully killed 97% of the mutants.</p>","Computer Science, Theory & Methods"
"WOS:000074951900004","A strategy for using genetic algorithms to automate branch and fault-based testing","1998","
<p>Genetic algorithms have been used successfully to generate software test data automatically; all branches were covered with substantially fewer generated tests than simple random testing. We generated test sets which executed all branches in a variety of programs including a quadratic equation solver, remainder, linear and binary search procedures, and a triangle classifier comprising a system of five procedures. We regard the generation of test sets as a search through the input domain for appropriate inputs. The genetic algorithms generated test data to give 100% branch coverage in up to two orders of magnitude fewer tests than random testing. Whilst some of this benefit is offset by increased computation effort, the adequacy of the test data is improved by the genetic algorithm's ability to generate test sets which are at or close to the input subdomain boundaries. Genetic algorithms may be used for fault-based testing where faults associated with mistakes in branch predicates are revealed. The software has been deliberately seeded with faults in the branch predicates (i.e. mutation testing), and our system successfully killed 97% of the mutants.</p>","Computer Science"
"WOS:000073200400004","A linear-time algorithm for connected r-domination and Steiner tree on distance-hereditary graphs","1998","
<p>A distance-hereditary graph is a connected graph in which every induced path is isometric, i.e., the distance of any two vertices in an induced path equals their distance in the graph. We present a linear time labeling algorithm for the minimum cardinality connected r-dominating set and Steiner tree problems on distance-hereditary graphs. (C) 1998 John Wiley & Sons, Inc.</p>","Computer Science, Hardware & Architecture"
"WOS:000073200400004","A linear-time algorithm for connected r-domination and Steiner tree on distance-hereditary graphs","1998","
<p>A distance-hereditary graph is a connected graph in which every induced path is isometric, i.e., the distance of any two vertices in an induced path equals their distance in the graph. We present a linear time labeling algorithm for the minimum cardinality connected r-dominating set and Steiner tree problems on distance-hereditary graphs. (C) 1998 John Wiley & Sons, Inc.</p>","Computer Science"
"WOS:000077569000001","A simple cellular automaton that solves the density and ordering problems","1998","
<p>We show that there exists a simple solution to the density problem in cellular automata, under fixed boundary conditions, in contrast to previously used periodic ones.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000077569000001","A simple cellular automaton that solves the density and ordering problems","1998","
<p>We show that there exists a simple solution to the density problem in cellular automata, under fixed boundary conditions, in contrast to previously used periodic ones.</p>","Computer Science"
"WOS:000075611000010","Termination of order-sorted rewriting with non-minimal signatures","1998","
<p>In this paper, we extend the Gnaedig's results [2],[3] on termination of order-sorted rewriting. Gnaedig required a condition for order-sorted signatures, called minimality, for the termination proof. We get rid of this restriction by introducing a transformation from a TRS with an arbitrary order-sorted signature to another TRS with a minimal signature, and proving that this transformation preserves termination.</p>","Computer Science, Information Systems"
"WOS:000075611000010","Termination of order-sorted rewriting with non-minimal signatures","1998","
<p>In this paper, we extend the Gnaedig's results [2],[3] on termination of order-sorted rewriting. Gnaedig required a condition for order-sorted signatures, called minimality, for the termination proof. We get rid of this restriction by introducing a transformation from a TRS with an arbitrary order-sorted signature to another TRS with a minimal signature, and proving that this transformation preserves termination.</p>","Computer Science, Software Engineering"
"WOS:000075611000010","Termination of order-sorted rewriting with non-minimal signatures","1998","
<p>In this paper, we extend the Gnaedig's results [2],[3] on termination of order-sorted rewriting. Gnaedig required a condition for order-sorted signatures, called minimality, for the termination proof. We get rid of this restriction by introducing a transformation from a TRS with an arbitrary order-sorted signature to another TRS with a minimal signature, and proving that this transformation preserves termination.</p>","Computer Science"
"WOS:000072208100002","Can the case for CASE technology be advanced by Process Improvement?","1998","
<p>Findings of a case study that focused on understanding how software development proceeded in a small Information Systems Department (ISD) located within a major public sector service are presented. The observations collected relate to a period prior to, during and after the introduction of a CASE( Computer Aided Software Engineering) tool. They are based on a combination of minutes from quality circle meetings, interviews and regular on-site observations by the first author. The study used a framework loosely based on SEI's CMM model to characterize the state of practice before and after the introduction of the tool and to assess process improvement.</p>
<p>Prior to the introduction of the first CASE tool a typical ISD software development task was a stand-alone single reporting application. The software development process model consisted simply of requirements acquisition and program development. Both these activities for a particular application were carried out by an individual developer within a few months. The former activity involved the client/user, with the level of participation dependent on the developer. Similarly, program development varied from one developer to another, with most effort being expended on coding and testing. Not surprisingly, maintenance was locked to the original developer.</p>
<p>A decision that ISD should develop larger and more complex applications triggered the purchase of a CASE tool. Typically, the larger applications would be developed by a team of about five people over a period of twelve or more months. The introduction of CASE tools, first Case Designer 5.1 then Designer 2000, had a marked effect on the working practices of ISD. Specifically two more stages were introduced into the development activity: designing via an entity-attribute relationship model and validation via rapid prototyping; and greater attention was paid to the management of testing and fault reporting.</p>
<p>The paper explores whether the benefits accrued by ISD can be attributed to the CASE tool, the changes in work practices, or a combination of both. (C) 1998 Chapman & Hall.</p>","Computer Science, Software Engineering"
"WOS:000072208100002","Can the case for CASE technology be advanced by Process Improvement?","1998","
<p>Findings of a case study that focused on understanding how software development proceeded in a small Information Systems Department (ISD) located within a major public sector service are presented. The observations collected relate to a period prior to, during and after the introduction of a CASE( Computer Aided Software Engineering) tool. They are based on a combination of minutes from quality circle meetings, interviews and regular on-site observations by the first author. The study used a framework loosely based on SEI's CMM model to characterize the state of practice before and after the introduction of the tool and to assess process improvement.</p>
<p>Prior to the introduction of the first CASE tool a typical ISD software development task was a stand-alone single reporting application. The software development process model consisted simply of requirements acquisition and program development. Both these activities for a particular application were carried out by an individual developer within a few months. The former activity involved the client/user, with the level of participation dependent on the developer. Similarly, program development varied from one developer to another, with most effort being expended on coding and testing. Not surprisingly, maintenance was locked to the original developer.</p>
<p>A decision that ISD should develop larger and more complex applications triggered the purchase of a CASE tool. Typically, the larger applications would be developed by a team of about five people over a period of twelve or more months. The introduction of CASE tools, first Case Designer 5.1 then Designer 2000, had a marked effect on the working practices of ISD. Specifically two more stages were introduced into the development activity: designing via an entity-attribute relationship model and validation via rapid prototyping; and greater attention was paid to the management of testing and fault reporting.</p>
<p>The paper explores whether the benefits accrued by ISD can be attributed to the CASE tool, the changes in work practices, or a combination of both. (C) 1998 Chapman & Hall.</p>","Computer Science"
"WOS:000076651900003","Towards a declarative approach for reusing domain ontologies","1998","
<p>To ensure the reuse of domain ontologies, we propose an approach which uses a unique translator whatever the target system. It relies on a meta-language which allows a declarative description of any model based on frames, objects, relations, or description logics. Its basic concept is the meta-relation concept which, associated to a definition, represents the signification of the construct being defined. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science, Information Systems"
"WOS:000076651900003","Towards a declarative approach for reusing domain ontologies","1998","
<p>To ensure the reuse of domain ontologies, we propose an approach which uses a unique translator whatever the target system. It relies on a meta-language which allows a declarative description of any model based on frames, objects, relations, or description logics. Its basic concept is the meta-relation concept which, associated to a definition, represents the signification of the construct being defined. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science"
"WOS:000073729100013","5-S practice: a new tool for industrial management","1998","
<p>It has been recognised that Japanese firms are clean and orderly. The same is true for high quality Western firms. Over the last two decades, the Japanese have formalised the technique and named it as 5-S practice. They believe that it is the base-line for industrial management. As the name is new to most Western societies, the objective of this paper is to explain the intricacy of the 5-S so that it can be understood easily and adopted readily by those who may find the tool useful. In Hong Kong, the government industry department has promoted the 5-S practice since 1994. Many seminars and workshops have been conducted and they were all very well received by the business community. As a result of the success, the department invited the author to commission a 5-S practice workbook with ten successful case studies from the manufacturing, services and public sectors. The experience will also be shared in this article.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000073729100013","5-S practice: a new tool for industrial management","1998","
<p>It has been recognised that Japanese firms are clean and orderly. The same is true for high quality Western firms. Over the last two decades, the Japanese have formalised the technique and named it as 5-S practice. They believe that it is the base-line for industrial management. As the name is new to most Western societies, the objective of this paper is to explain the intricacy of the 5-S so that it can be understood easily and adopted readily by those who may find the tool useful. In Hong Kong, the government industry department has promoted the 5-S practice since 1994. Many seminars and workshops have been conducted and they were all very well received by the business community. As a result of the success, the department invited the author to commission a 5-S practice workbook with ten successful case studies from the manufacturing, services and public sectors. The experience will also be shared in this article.</p>","Computer Science"
"WOS:000084730100040","Structuralization of case-bases, using concept hierarchy","1998","
<p>The case-based reasoning makes it possible to construct easy management mechanism for manipulating knowledge resources successfully. Some prototyping systems have been developed and applied to individual domains. However, it is important to structuralize effectively the organization in point of retrieval efficiency. In this paper, the structuralization method is addressed. Our structuralization method is characterized with regard to two concepts: categorization, with knowledge about concept hierarchy; and organization based on the common features among two or more attributes. Our case categorization structure is represented by a direct acyclic graph, and can be organized automatically on the basis of the above concepts. This paper describes a case structuralization method and evaluates the retrieval efficiency through experiments.</p>","Computer Science, Artificial Intelligence"
"WOS:000084730100040","Structuralization of case-bases, using concept hierarchy","1998","
<p>The case-based reasoning makes it possible to construct easy management mechanism for manipulating knowledge resources successfully. Some prototyping systems have been developed and applied to individual domains. However, it is important to structuralize effectively the organization in point of retrieval efficiency. In this paper, the structuralization method is addressed. Our structuralization method is characterized with regard to two concepts: categorization, with knowledge about concept hierarchy; and organization based on the common features among two or more attributes. Our case categorization structure is represented by a direct acyclic graph, and can be organized automatically on the basis of the above concepts. This paper describes a case structuralization method and evaluates the retrieval efficiency through experiments.</p>","Computer Science"
"WOS:000078245100003","Lexicographic priorities in default logic","1998","
<p>Resolving conflicts between default rules is a major subtask in performing default reasoning. A declarative way of controlling the resolution of conflicts is to assign priorities to default rules, and to prevent conflict resolution in ways that violate the priorities. This work extends Reiter's default logic with a priority mechanism that is based on lexicographic comparison. Given a default theory and a partial ordering on the defaults, the preferred extensions are the lexicographically best extensions. We discuss alternative ways of using lexicographic comparison, and investigate their properties and relations between them. The applicability of the priority mechanism to inheritance reasoning is investigated by presenting two translations from inheritance networks to prioritized default theories, and relating them to inheritance theories presented earlier by Gelfond and Przymusinska and by Brewka. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science, Artificial Intelligence"
"WOS:000078245100003","Lexicographic priorities in default logic","1998","
<p>Resolving conflicts between default rules is a major subtask in performing default reasoning. A declarative way of controlling the resolution of conflicts is to assign priorities to default rules, and to prevent conflict resolution in ways that violate the priorities. This work extends Reiter's default logic with a priority mechanism that is based on lexicographic comparison. Given a default theory and a partial ordering on the defaults, the preferred extensions are the lexicographically best extensions. We discuss alternative ways of using lexicographic comparison, and investigate their properties and relations between them. The applicability of the priority mechanism to inheritance reasoning is investigated by presenting two translations from inheritance networks to prioritized default theories, and relating them to inheritance theories presented earlier by Gelfond and Przymusinska and by Brewka. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science"
"WOS:000077085100005","Representation of 3-D elevation in terrain databases using hierarchical triangulated irregular networks: a comparative analysis","1998","
<p>3-D terrain representation plays an important role in a number of terrain database applications. Hierarchical Triangulated Irregular Networks (TINs) provide a variable-resolution terrain representation that is based on a nested triangulation of the terrain. This paper compares and analyzes existing hierarchical triangulation techniques. The comparative analysis takes into account how aesthetically appealing and accurate the resulting terrain representation is. Parameters, such as adjacency, slivers, and streaks, are used to provide a measure on how aesthetically appealing the terrain representation is. Slivers occur when the triangulation produces thin and slivery triangles. Streaks appear when there are too many triangulations done at a given vertex. Simple mathematical expressions are derived for these parameters, thereby providing a fairer and a more easily duplicated comparison. In addition to meeting the adjacency requirement, an aesthetically pleasant hierarchical TINs generation algorithm is expected to reduce both slivers and streaks while maintaining accuracy. A comparative analysis of a number of existing approaches shows that a variant of a method originally proposed by Scarlatos exhibits better overall performance.</p>","Computer Science, Information Systems"
"WOS:000077085100005","Representation of 3-D elevation in terrain databases using hierarchical triangulated irregular networks: a comparative analysis","1998","
<p>3-D terrain representation plays an important role in a number of terrain database applications. Hierarchical Triangulated Irregular Networks (TINs) provide a variable-resolution terrain representation that is based on a nested triangulation of the terrain. This paper compares and analyzes existing hierarchical triangulation techniques. The comparative analysis takes into account how aesthetically appealing and accurate the resulting terrain representation is. Parameters, such as adjacency, slivers, and streaks, are used to provide a measure on how aesthetically appealing the terrain representation is. Slivers occur when the triangulation produces thin and slivery triangles. Streaks appear when there are too many triangulations done at a given vertex. Simple mathematical expressions are derived for these parameters, thereby providing a fairer and a more easily duplicated comparison. In addition to meeting the adjacency requirement, an aesthetically pleasant hierarchical TINs generation algorithm is expected to reduce both slivers and streaks while maintaining accuracy. A comparative analysis of a number of existing approaches shows that a variant of a method originally proposed by Scarlatos exhibits better overall performance.</p>","Computer Science"
"WOS:000075817800002","Diffeomorphisms groups and pattern matching in image analysis","1998","
<p>In a previous paper, it was proposed to see the deformations of a common pattern as the action of an infinite dimensional group. We show in this paper that this approach can be applied numerically for pattern matching in image analysis of digital images. Using Lie group ideas, we construct a distance between deformations defined through a metric given the cost of infinitesimal deformations. Then we propose a numerical scheme to solve a variational problem involving this distance and leading to a sub-optimal gradient pattern matching. Its links with fluid models are established.</p>","Computer Science, Artificial Intelligence"
"WOS:000075817800002","Diffeomorphisms groups and pattern matching in image analysis","1998","
<p>In a previous paper, it was proposed to see the deformations of a common pattern as the action of an infinite dimensional group. We show in this paper that this approach can be applied numerically for pattern matching in image analysis of digital images. Using Lie group ideas, we construct a distance between deformations defined through a metric given the cost of infinitesimal deformations. Then we propose a numerical scheme to solve a variational problem involving this distance and leading to a sub-optimal gradient pattern matching. Its links with fluid models are established.</p>","Computer Science"
"WOS:000074105300020","Estimation of myocardial glucose utilisation with PET using the left ventricular time-activity curve as a non-invasive input function","1998","
<p>The validation study is described of a new modelling method that has been developed, using tracer kinetic modelling with positron emission tomography (PET) to achieve non-invasive measurement of myocardial metabolic rate of glucose (MMRGlc). Eight data sets obtained from dynamic cardiac PET 2-[F-18]fluoro-2-deoxy-D-glucose (FDG) studies on human subjects are employed, and the estimation of MMRGlc using both the new and traditional methods is compared. The results from all eight human FDG studies are consistent with those from previous computer simulations. With the new method, the estimated mean of K (a parameter directly proportional to MMRGlc) increases by about 8%, and that of k(4) (the rate constant of FDG dephosphorylation) decreases by about 48%. The approach should be more suitable for use in dynamic cardiac PET studies when non-invasive means are used to obtain the plasma time-activity curve from left-ventricle PET images.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000074105300020","Estimation of myocardial glucose utilisation with PET using the left ventricular time-activity curve as a non-invasive input function","1998","
<p>The validation study is described of a new modelling method that has been developed, using tracer kinetic modelling with positron emission tomography (PET) to achieve non-invasive measurement of myocardial metabolic rate of glucose (MMRGlc). Eight data sets obtained from dynamic cardiac PET 2-[F-18]fluoro-2-deoxy-D-glucose (FDG) studies on human subjects are employed, and the estimation of MMRGlc using both the new and traditional methods is compared. The results from all eight human FDG studies are consistent with those from previous computer simulations. With the new method, the estimated mean of K (a parameter directly proportional to MMRGlc) increases by about 8%, and that of k(4) (the rate constant of FDG dephosphorylation) decreases by about 48%. The approach should be more suitable for use in dynamic cardiac PET studies when non-invasive means are used to obtain the plasma time-activity curve from left-ventricle PET images.</p>","Computer Science"
"WOS:000071451000008","An informatics-based chronic disease practice: Case study of a 35-year computer-based longitudinal record system","1998","
<p>The authors present the case study of a 35-year informatics-based single subspecialty practice for the management of patients with chronic thyroid disease. This extensive experience provides a paradigm for the organization of longitudinal medical information by integrating individual patient care with clinical research and education. The kernel of the process is a set of worksheets Easily completed by the physician during the patient encounter. It is a structured medical record that has been computerized since 1972, enabling analysis of different groups of patients to answer questions about chronic conditions and the effects of therapeutic interventions. The recording process and resulting studies serve as an important vehicle for medical education about the nuances of clinical practice. The authors suggest ways in which computerized medical records can become an integral part of medical practice, rather than a luxury or novelty.</p>","Computer Science, Information Systems"
"WOS:000071451000008","An informatics-based chronic disease practice: Case study of a 35-year computer-based longitudinal record system","1998","
<p>The authors present the case study of a 35-year informatics-based single subspecialty practice for the management of patients with chronic thyroid disease. This extensive experience provides a paradigm for the organization of longitudinal medical information by integrating individual patient care with clinical research and education. The kernel of the process is a set of worksheets Easily completed by the physician during the patient encounter. It is a structured medical record that has been computerized since 1972, enabling analysis of different groups of patients to answer questions about chronic conditions and the effects of therapeutic interventions. The recording process and resulting studies serve as an important vehicle for medical education about the nuances of clinical practice. The authors suggest ways in which computerized medical records can become an integral part of medical practice, rather than a luxury or novelty.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000071451000008","An informatics-based chronic disease practice: Case study of a 35-year computer-based longitudinal record system","1998","
<p>The authors present the case study of a 35-year informatics-based single subspecialty practice for the management of patients with chronic thyroid disease. This extensive experience provides a paradigm for the organization of longitudinal medical information by integrating individual patient care with clinical research and education. The kernel of the process is a set of worksheets Easily completed by the physician during the patient encounter. It is a structured medical record that has been computerized since 1972, enabling analysis of different groups of patients to answer questions about chronic conditions and the effects of therapeutic interventions. The recording process and resulting studies serve as an important vehicle for medical education about the nuances of clinical practice. The authors suggest ways in which computerized medical records can become an integral part of medical practice, rather than a luxury or novelty.</p>","Computer Science"
"WOS:000077186300010","Chemical information instruction in academe: Recent and current trends","1998","
<p>As a continuation in a series about chemical information instruction in academe, this article describes the results of the 1993 survey of information instruction activities in 390 United States chemistry departments conducted by the American Chemical Society (ACS) Division of Chemical Information's Education Committee and makes comparisons with the previous 1984 survey. Current trends, especially in rapidly changing chemical resources, are identified and resulting implications for information instruction programs are discussed. Ways of overcoming increasing costs of information resources are offered, as well as suggestions for involving faculty and librarians as active supporters and participants in such programs.</p>","Computer Science, Information Systems"
"WOS:000077186300010","Chemical information instruction in academe: Recent and current trends","1998","
<p>As a continuation in a series about chemical information instruction in academe, this article describes the results of the 1993 survey of information instruction activities in 390 United States chemistry departments conducted by the American Chemical Society (ACS) Division of Chemical Information's Education Committee and makes comparisons with the previous 1984 survey. Current trends, especially in rapidly changing chemical resources, are identified and resulting implications for information instruction programs are discussed. Ways of overcoming increasing costs of information resources are offered, as well as suggestions for involving faculty and librarians as active supporters and participants in such programs.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000077186300010","Chemical information instruction in academe: Recent and current trends","1998","
<p>As a continuation in a series about chemical information instruction in academe, this article describes the results of the 1993 survey of information instruction activities in 390 United States chemistry departments conducted by the American Chemical Society (ACS) Division of Chemical Information's Education Committee and makes comparisons with the previous 1984 survey. Current trends, especially in rapidly changing chemical resources, are identified and resulting implications for information instruction programs are discussed. Ways of overcoming increasing costs of information resources are offered, as well as suggestions for involving faculty and librarians as active supporters and participants in such programs.</p>","Computer Science"
"WOS:000076973800002","A competence theory approach to problem solving method construction","1998","
<p>This paper presents a theory of the construction process of problem-solving methods (PSMs) on the basis of the competence theory approach. This approach describes the refinement process of an initial, abstract formalization of the required competence of a PSM, towards an operational version of the PSM. Three major steps in this process are identified: specification of the required competence theory, refinement of the theory into a form that fits a PSM paradigm and the operationalization of the theory into a form that is close to an executable specification. As an example, the ontological commitments and assumptions underlying some problem-solving methods for classification problems are investigated and their operational forms are presented. (C) 1998 Academic Press.</p>","Computer Science, Cybernetics"
"WOS:000076973800002","A competence theory approach to problem solving method construction","1998","
<p>This paper presents a theory of the construction process of problem-solving methods (PSMs) on the basis of the competence theory approach. This approach describes the refinement process of an initial, abstract formalization of the required competence of a PSM, towards an operational version of the PSM. Three major steps in this process are identified: specification of the required competence theory, refinement of the theory into a form that fits a PSM paradigm and the operationalization of the theory into a form that is close to an executable specification. As an example, the ontological commitments and assumptions underlying some problem-solving methods for classification problems are investigated and their operational forms are presented. (C) 1998 Academic Press.</p>","Computer Science"
"WOS:000075427400002","Obstacle avoidance in a dynamic environment: A collision cone approach","1998","
<p>A novel collision cone approach is proposed as an aid to collision detection and avoidance between irregularly shaped moving objects with unknown trajectories. It is shown that the collision cone can be effectively used to determine whether collision between a robot and an obstacle (both moving in a dynamic environment) is imminent. No restrictions are placed on the shapes of either the robot or the obstacle, i.e., they can both be of any arbitrary shape. The collision cone concept is developed in a phased manner starting from existing analytical results-available in aerospace literature-that enable prediction of collision between two moving point objects. These results are extended to predict collision between a point and a circular object, between a point and an irregularly shaped object, between two circular objects, and finally between two irregularly shaped objects. Using the collision cone approach, several strategies that the robot can follow in order to avoid collision, are presented. A discussion on how the shapes of the robot and obstacles can be approximated in order to reduce computational burden is also presented. A number of examples are given to illustrate both collision prediction and avoidance strategies of the robot.</p>","Computer Science, Cybernetics"
"WOS:000075427400002","Obstacle avoidance in a dynamic environment: A collision cone approach","1998","
<p>A novel collision cone approach is proposed as an aid to collision detection and avoidance between irregularly shaped moving objects with unknown trajectories. It is shown that the collision cone can be effectively used to determine whether collision between a robot and an obstacle (both moving in a dynamic environment) is imminent. No restrictions are placed on the shapes of either the robot or the obstacle, i.e., they can both be of any arbitrary shape. The collision cone concept is developed in a phased manner starting from existing analytical results-available in aerospace literature-that enable prediction of collision between two moving point objects. These results are extended to predict collision between a point and a circular object, between a point and an irregularly shaped object, between two circular objects, and finally between two irregularly shaped objects. Using the collision cone approach, several strategies that the robot can follow in order to avoid collision, are presented. A discussion on how the shapes of the robot and obstacles can be approximated in order to reduce computational burden is also presented. A number of examples are given to illustrate both collision prediction and avoidance strategies of the robot.</p>","Computer Science, Theory & Methods"
"WOS:000075427400002","Obstacle avoidance in a dynamic environment: A collision cone approach","1998","
<p>A novel collision cone approach is proposed as an aid to collision detection and avoidance between irregularly shaped moving objects with unknown trajectories. It is shown that the collision cone can be effectively used to determine whether collision between a robot and an obstacle (both moving in a dynamic environment) is imminent. No restrictions are placed on the shapes of either the robot or the obstacle, i.e., they can both be of any arbitrary shape. The collision cone concept is developed in a phased manner starting from existing analytical results-available in aerospace literature-that enable prediction of collision between two moving point objects. These results are extended to predict collision between a point and a circular object, between a point and an irregularly shaped object, between two circular objects, and finally between two irregularly shaped objects. Using the collision cone approach, several strategies that the robot can follow in order to avoid collision, are presented. A discussion on how the shapes of the robot and obstacles can be approximated in order to reduce computational burden is also presented. A number of examples are given to illustrate both collision prediction and avoidance strategies of the robot.</p>","Computer Science"
"WOS:000077581300089","Size segregation of granular materials in a 3D rotating drum","1998","
<p>Particle simulations were among the first applications to be implemented on scalar computers over forty years ago, and have since played an important role in many science and engineering applications [1, 4]. Because of the inherent parallelism in all particle algorithms the advent of parallel computers has revolutionized this field: basically, the same set of calculations has to be performed for every particle in the system. At present, realistic simulations with a few million particles are possible using large parallel computers.</p>
<p>In this paper the parallel simulation of the size segregation of a binary mixture of granular materials in a half-filled three-dimensional rotating drum using the discrete element method with linear contact forces is investigated. Performance results of an implementation in Fortran 90 using MPI for data communication on GRAY T3D, T3E-600, T3E-900, and T3E-1200 are presented.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000077581300089","Size segregation of granular materials in a 3D rotating drum","1998","
<p>Particle simulations were among the first applications to be implemented on scalar computers over forty years ago, and have since played an important role in many science and engineering applications [1, 4]. Because of the inherent parallelism in all particle algorithms the advent of parallel computers has revolutionized this field: basically, the same set of calculations has to be performed for every particle in the system. At present, realistic simulations with a few million particles are possible using large parallel computers.</p>
<p>In this paper the parallel simulation of the size segregation of a binary mixture of granular materials in a half-filled three-dimensional rotating drum using the discrete element method with linear contact forces is investigated. Performance results of an implementation in Fortran 90 using MPI for data communication on GRAY T3D, T3E-600, T3E-900, and T3E-1200 are presented.</p>","Computer Science, Software Engineering"
"WOS:000077581300089","Size segregation of granular materials in a 3D rotating drum","1998","
<p>Particle simulations were among the first applications to be implemented on scalar computers over forty years ago, and have since played an important role in many science and engineering applications [1, 4]. Because of the inherent parallelism in all particle algorithms the advent of parallel computers has revolutionized this field: basically, the same set of calculations has to be performed for every particle in the system. At present, realistic simulations with a few million particles are possible using large parallel computers.</p>
<p>In this paper the parallel simulation of the size segregation of a binary mixture of granular materials in a half-filled three-dimensional rotating drum using the discrete element method with linear contact forces is investigated. Performance results of an implementation in Fortran 90 using MPI for data communication on GRAY T3D, T3E-600, T3E-900, and T3E-1200 are presented.</p>","Computer Science, Theory & Methods"
"WOS:000077581300089","Size segregation of granular materials in a 3D rotating drum","1998","
<p>Particle simulations were among the first applications to be implemented on scalar computers over forty years ago, and have since played an important role in many science and engineering applications [1, 4]. Because of the inherent parallelism in all particle algorithms the advent of parallel computers has revolutionized this field: basically, the same set of calculations has to be performed for every particle in the system. At present, realistic simulations with a few million particles are possible using large parallel computers.</p>
<p>In this paper the parallel simulation of the size segregation of a binary mixture of granular materials in a half-filled three-dimensional rotating drum using the discrete element method with linear contact forces is investigated. Performance results of an implementation in Fortran 90 using MPI for data communication on GRAY T3D, T3E-600, T3E-900, and T3E-1200 are presented.</p>","Computer Science"
"WOS:000077931500006","On the use of laboratory ocean circulation models to simulate mesoscale (10-100 km) spreading","1998","
<p>The development of hydrodynamic numerical models for environmental studies depends on good benchmarks to calibrate and validate the physics and numerical codes. Laboratory models of non-linear and coupled physics in topography for which no analytical solutions are available can provide such valuable benchmarks. Although field data are necessary for a final validation, they are often of less Value for developing numerical models, since a truly synoptic coverage of a scenario is seldom found, knowledge of the forcing conditions is imperfect and average conditions of a non-linear system are seldom obtained by applying average boundary conditions.</p>
<p>The role of laboratory models and experiments for providing information on turbulence in complicated topography is indisputable. The high topographical resolution of these models reveals how narrow and filamentous many of the flow features can be, as often seen in satellite images. Such filaments enhance diffusion through a process known as shear dispersion. The filaments are also of concern for the interpretation of sparse field measurements and for computing the mesoscale (10-100 km) spreading characteristics. Time histories of dye clouds and clusters of particles in laboratory simulations of ocean currents, without wind, show much larger spreading than particle spreading due to strong winds. The results demonstrate that numerical models need high resolution and/or good parametrization of the spreading characteristics, which vary both in space and time, to achieve their goals. It is proposed that the differences between numerical and laboratory simulations of dispersion, with identical forcing, be parametrized as a size-dependent, or time-dependent random walk diffusion in the numerical code.</p>
<p>The laboratory results amply show that spreading is greatly enhanced by shear dispersion, and that assessments of the consequences of accidental oil spills or releases of radionuclides, for example, must take this into account. Island communities in tidally active regions are particularly prone to the consequences of a rapid dispersion of contaminants. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000077931500006","On the use of laboratory ocean circulation models to simulate mesoscale (10-100 km) spreading","1998","
<p>The development of hydrodynamic numerical models for environmental studies depends on good benchmarks to calibrate and validate the physics and numerical codes. Laboratory models of non-linear and coupled physics in topography for which no analytical solutions are available can provide such valuable benchmarks. Although field data are necessary for a final validation, they are often of less Value for developing numerical models, since a truly synoptic coverage of a scenario is seldom found, knowledge of the forcing conditions is imperfect and average conditions of a non-linear system are seldom obtained by applying average boundary conditions.</p>
<p>The role of laboratory models and experiments for providing information on turbulence in complicated topography is indisputable. The high topographical resolution of these models reveals how narrow and filamentous many of the flow features can be, as often seen in satellite images. Such filaments enhance diffusion through a process known as shear dispersion. The filaments are also of concern for the interpretation of sparse field measurements and for computing the mesoscale (10-100 km) spreading characteristics. Time histories of dye clouds and clusters of particles in laboratory simulations of ocean currents, without wind, show much larger spreading than particle spreading due to strong winds. The results demonstrate that numerical models need high resolution and/or good parametrization of the spreading characteristics, which vary both in space and time, to achieve their goals. It is proposed that the differences between numerical and laboratory simulations of dispersion, with identical forcing, be parametrized as a size-dependent, or time-dependent random walk diffusion in the numerical code.</p>
<p>The laboratory results amply show that spreading is greatly enhanced by shear dispersion, and that assessments of the consequences of accidental oil spills or releases of radionuclides, for example, must take this into account. Island communities in tidally active regions are particularly prone to the consequences of a rapid dispersion of contaminants. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science"
"WOS:000077740100018","Standardisation of the GII - the ITU perspective","1998","
<p>The International Telecommunications Union has embarked on a standardisation programme for the Global Information Infrastructure with the fundamental objective that the GII will be a 'federation of networks' consisting of seamlessly interconnected and interoperable communication networks, information processing equipment, data bases and terminals. The presentation will describe the objectives of the various GII projects that have been established and will outline the key results so far achieved in the areas of framework architectures and scenarios. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science, Hardware & Architecture"
"WOS:000077740100018","Standardisation of the GII - the ITU perspective","1998","
<p>The International Telecommunications Union has embarked on a standardisation programme for the Global Information Infrastructure with the fundamental objective that the GII will be a 'federation of networks' consisting of seamlessly interconnected and interoperable communication networks, information processing equipment, data bases and terminals. The presentation will describe the objectives of the various GII projects that have been established and will outline the key results so far achieved in the areas of framework architectures and scenarios. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science, Software Engineering"
"WOS:000077740100018","Standardisation of the GII - the ITU perspective","1998","
<p>The International Telecommunications Union has embarked on a standardisation programme for the Global Information Infrastructure with the fundamental objective that the GII will be a 'federation of networks' consisting of seamlessly interconnected and interoperable communication networks, information processing equipment, data bases and terminals. The presentation will describe the objectives of the various GII projects that have been established and will outline the key results so far achieved in the areas of framework architectures and scenarios. (C) 1998 Elsevier Science B.V. All rights reserved.</p>","Computer Science"
"WOS:000072870500007","Stability criteria for feedback-controlled, imperfectly known, bilinear systems with time-varying delay","1998","
<p>The problem of synthesizing a class of continuous, memoryless feedback controls in order to stabilize a class of imperfectly known homogeneous-in-the-state bilinear time-delay systems is considered. In particular, bilinear systems with state time-delay in the linear term are investigated. The time-delay is assumed to be an unknown time-varying function with known upper bound on its derivative. As well as considering both matched and residual uncertainty, the uncertainty in the class of systems can be state, delayed-state and input dependent, and time-varying. Prior information on the bound of the system uncertainty is required; such bounding information allows for quadratic growth with respect to the state. For this stabilizability problem, a stability criterion, involving the upper bound on the derivative of the time-varying time-delay is obtained. (C) 1998 IMACS/Elsevier Science B.V.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000072870500007","Stability criteria for feedback-controlled, imperfectly known, bilinear systems with time-varying delay","1998","
<p>The problem of synthesizing a class of continuous, memoryless feedback controls in order to stabilize a class of imperfectly known homogeneous-in-the-state bilinear time-delay systems is considered. In particular, bilinear systems with state time-delay in the linear term are investigated. The time-delay is assumed to be an unknown time-varying function with known upper bound on its derivative. As well as considering both matched and residual uncertainty, the uncertainty in the class of systems can be state, delayed-state and input dependent, and time-varying. Prior information on the bound of the system uncertainty is required; such bounding information allows for quadratic growth with respect to the state. For this stabilizability problem, a stability criterion, involving the upper bound on the derivative of the time-varying time-delay is obtained. (C) 1998 IMACS/Elsevier Science B.V.</p>","Computer Science, Software Engineering"
"WOS:000072870500007","Stability criteria for feedback-controlled, imperfectly known, bilinear systems with time-varying delay","1998","
<p>The problem of synthesizing a class of continuous, memoryless feedback controls in order to stabilize a class of imperfectly known homogeneous-in-the-state bilinear time-delay systems is considered. In particular, bilinear systems with state time-delay in the linear term are investigated. The time-delay is assumed to be an unknown time-varying function with known upper bound on its derivative. As well as considering both matched and residual uncertainty, the uncertainty in the class of systems can be state, delayed-state and input dependent, and time-varying. Prior information on the bound of the system uncertainty is required; such bounding information allows for quadratic growth with respect to the state. For this stabilizability problem, a stability criterion, involving the upper bound on the derivative of the time-varying time-delay is obtained. (C) 1998 IMACS/Elsevier Science B.V.</p>","Computer Science"
"WOS:000075814400001","Twelve numerical symbolic and hybrid supervised classification methods","1998","
<p>Supervised classification has already been the subject of numerous studies in the fields of Statistics, Pattern Recognition and Artificial Intelligence under various appellations which include discriminant analysis, discrimination and concept learning. Many practical applications relating to this field have been developed. New methods have appeared in recent years, due to developments concerning Neural Networks and Machine Learning. These ""hybrid"" approaches share one common factor in that they combine symbolic and numerical aspects. The former are characterized by the representation of knowledge, the latter by the introduction of frequencies and probabilistic criteria. In the present study, we shall present a certain number of hybrid methods, conceived (or improved) by members of the SYMENU research group. These methods issue mainly from Machine Learning and from research on Classification Trees done in Statistics, and they may also be qualified as ""rule-based"". They shall be compared with other more classical approaches. This comparison will be based on a detailed description of each of the twelve methods envisaged, and on the results obtained concerning the ""Waveform Recognition Problem"" proposed by Breiman et al.,(4) which is difficult for rule based approaches.</p>","Computer Science, Artificial Intelligence"
"WOS:000075814400001","Twelve numerical symbolic and hybrid supervised classification methods","1998","
<p>Supervised classification has already been the subject of numerous studies in the fields of Statistics, Pattern Recognition and Artificial Intelligence under various appellations which include discriminant analysis, discrimination and concept learning. Many practical applications relating to this field have been developed. New methods have appeared in recent years, due to developments concerning Neural Networks and Machine Learning. These ""hybrid"" approaches share one common factor in that they combine symbolic and numerical aspects. The former are characterized by the representation of knowledge, the latter by the introduction of frequencies and probabilistic criteria. In the present study, we shall present a certain number of hybrid methods, conceived (or improved) by members of the SYMENU research group. These methods issue mainly from Machine Learning and from research on Classification Trees done in Statistics, and they may also be qualified as ""rule-based"". They shall be compared with other more classical approaches. This comparison will be based on a detailed description of each of the twelve methods envisaged, and on the results obtained concerning the ""Waveform Recognition Problem"" proposed by Breiman et al.,(4) which is difficult for rule based approaches.</p>","Computer Science"
"WOS:000075844600008","Parametric analysis of polyhedral iteration spaces","1998","
<p>In the area of automatic parallelization of programs, analyzing and transforming loop nests with parametric affine loop bounds requires fundamental mathematical results. The most common geometrical model of iteration spaces, called the polytope model, is based on mathematics dealing with convex and discrete geometry, linear programming, combinatorics and geometry of numbers.</p>
<p>In this paper, we present automatic methods for computing the parametric vertices and the Ehrhart polynomial, i.e., a parametric expression of the number of integer points, of a polytope defined by-a set of parametric linear constraints.</p>
<p>These methods have many applications in analysis and transformations of nested loop programs. The paper is illustrated with exact symbolic array dataflow analysis, estimation of execution time, and with the computation of the maximum available parallelism of given loop nests.</p>","Computer Science, Information Systems"
"WOS:000075844600008","Parametric analysis of polyhedral iteration spaces","1998","
<p>In the area of automatic parallelization of programs, analyzing and transforming loop nests with parametric affine loop bounds requires fundamental mathematical results. The most common geometrical model of iteration spaces, called the polytope model, is based on mathematics dealing with convex and discrete geometry, linear programming, combinatorics and geometry of numbers.</p>
<p>In this paper, we present automatic methods for computing the parametric vertices and the Ehrhart polynomial, i.e., a parametric expression of the number of integer points, of a polytope defined by-a set of parametric linear constraints.</p>
<p>These methods have many applications in analysis and transformations of nested loop programs. The paper is illustrated with exact symbolic array dataflow analysis, estimation of execution time, and with the computation of the maximum available parallelism of given loop nests.</p>","Computer Science"
"WOS:000076133800083","Noise reduction using nonlinear optimization modeling","1998","
<p>The primary objective of this study is to develop a 3D model to determine the maximum allowable noise exposure time. This model allows the presence of multiple uncorrelated sources. The model provides an optimal arrangement for locating various machinery and equipment in the plant layout and also takes into consideration the acoustic characteristics of materials used in constructing the room. A detailed description of the models and results obtained will be presented and discussed. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science, Interdisciplinary Applications"
"WOS:000076133800083","Noise reduction using nonlinear optimization modeling","1998","
<p>The primary objective of this study is to develop a 3D model to determine the maximum allowable noise exposure time. This model allows the presence of multiple uncorrelated sources. The model provides an optimal arrangement for locating various machinery and equipment in the plant layout and also takes into consideration the acoustic characteristics of materials used in constructing the room. A detailed description of the models and results obtained will be presented and discussed. (C) 1998 Elsevier Science Ltd. All rights reserved.</p>","Computer Science"
"WOS:000075513700009","Geometry and structure of Lie pseudogroups from infinitesimal defining systems","1998","
<p>An algorithm is described which uses a finite number of differentiations and linear operations to determine the Cartan structure of a transitive Lie pseudogroup from its infinitesimal defining equations. In addition, an algorithm is presented for determining from the infinitesimal defining system whether a Lie pseudogroup has essential invariants. If such invariants exist, the pseudogroup is intransitive. These methods make feasible the calculation of the Cartan structure of infinite Lie pseudogroups of symmetries of differential equations. The structure of the symmetry pseudogroup of the KP equation is presented. (C) 1998 Academic Press.</p>","Computer Science, Theory & Methods"
"WOS:000075513700009","Geometry and structure of Lie pseudogroups from infinitesimal defining systems","1998","
<p>An algorithm is described which uses a finite number of differentiations and linear operations to determine the Cartan structure of a transitive Lie pseudogroup from its infinitesimal defining equations. In addition, an algorithm is presented for determining from the infinitesimal defining system whether a Lie pseudogroup has essential invariants. If such invariants exist, the pseudogroup is intransitive. These methods make feasible the calculation of the Cartan structure of infinite Lie pseudogroups of symmetries of differential equations. The structure of the symmetry pseudogroup of the KP equation is presented. (C) 1998 Academic Press.</p>","Computer Science"
"WOS:000085483000012","Some prospects for efficient fixed parameter algorithms","1998","
<p>Recent time has seen quite some progress in the development of exponential time algorithms for NP-hard problems, where the base of the exponential term is fairly small. These developments are also tightly related to the theory of fixed parameter tractability. In this incomplete survey, we explain some basic techniques in the design of efficient fixed parameter algorithms, discuss deficiencies of parameterized complexity theory, and try to point out some future research challenges. The focus of this paper is on the design of efficient algorithms and not on a structural theory of parameterized complexity. Moreover, our emphasis will be laid on two exemplifying issues: Vertex Cover and MaxSat problems.</p>","Computer Science, Theory & Methods"
"WOS:000085483000012","Some prospects for efficient fixed parameter algorithms","1998","
<p>Recent time has seen quite some progress in the development of exponential time algorithms for NP-hard problems, where the base of the exponential term is fairly small. These developments are also tightly related to the theory of fixed parameter tractability. In this incomplete survey, we explain some basic techniques in the design of efficient fixed parameter algorithms, discuss deficiencies of parameterized complexity theory, and try to point out some future research challenges. The focus of this paper is on the design of efficient algorithms and not on a structural theory of parameterized complexity. Moreover, our emphasis will be laid on two exemplifying issues: Vertex Cover and MaxSat problems.</p>","Computer Science"
"WOS:000071830200006","Direct broadcast satellite applications","1998","
<p>One of the main reasons for the popularity of direct broadcast satellite (DBS) service is the small size of the parabolic dish antenna. The key to the small-size dish is a low-noise GaAs transistor used in the low-noise block of the DBS receiver system. One of HP's efforts in this area has been to develop an AllnAs/GalnAs device fabricated on a conventional GaAs substrate that has a lower noise figure, higher gain. and lower cost.</p>","Computer Science, Hardware & Architecture"
"WOS:000071830200006","Direct broadcast satellite applications","1998","
<p>One of the main reasons for the popularity of direct broadcast satellite (DBS) service is the small size of the parabolic dish antenna. The key to the small-size dish is a low-noise GaAs transistor used in the low-noise block of the DBS receiver system. One of HP's efforts in this area has been to develop an AllnAs/GalnAs device fabricated on a conventional GaAs substrate that has a lower noise figure, higher gain. and lower cost.</p>","Computer Science"
"WOS:000072346100005","Use of autoassociative neural networks for signal validation","1998","
<p>Recently, the use of Autoassociative Neural Networks (AANNs) to perform on-line calibration monitoring of process sensors has been shown to not only be feasible, but practical as well. This paper summarizes the results of applying AANNs to instrument surveillance and calibration monitoring at Florida Power Corporation's Crystal River #3 Nuclear Power Plant and at the Oak Ridge National Laboratory High Flux Isotope Reactor. In both cases sensor drifts are detectable at a nominal level of 0.5% of the instrument's full scale range. This paper will discuss the selection of a five layer neural network architecture, a robust training paradigm, the input selection criteria, and a retuning algorithm.</p>","Computer Science, Artificial Intelligence"
"WOS:000072346100005","Use of autoassociative neural networks for signal validation","1998","
<p>Recently, the use of Autoassociative Neural Networks (AANNs) to perform on-line calibration monitoring of process sensors has been shown to not only be feasible, but practical as well. This paper summarizes the results of applying AANNs to instrument surveillance and calibration monitoring at Florida Power Corporation's Crystal River #3 Nuclear Power Plant and at the Oak Ridge National Laboratory High Flux Isotope Reactor. In both cases sensor drifts are detectable at a nominal level of 0.5% of the instrument's full scale range. This paper will discuss the selection of a five layer neural network architecture, a robust training paradigm, the input selection criteria, and a retuning algorithm.</p>","Computer Science"
"WOS:000075188700010","Discrete-time quasi-sliding mode tracking control of uncertain systems with long sampling interval","1998","
<p>This paper presents a discrete-time quasi-sliding mode controller of SIMO systems with parameter uncertainty and long sampling interval. The discrete quasi-sliding mode control system is designed to track the desired path on the basis of a Lyapunov function, and sufficient conditions of the switching gain are given to make the discrete system stable. Each component of the switching gain can be determined separately. The switching region is variable according to the product of the bounds of uncertainty and the amplitude of the trajectory. The controller is robust to the uncertainties, and the reaching condition of the control system is satisfied for any initial condition. This control law is a generalized form of the discrete quasi-sliding mode control and reduces the chattering problem considerably. Simulation results for motion control of an autonomous underwater vehicle which acquires position information with a discrete interval show the effectiveness of the proposed technique.</p>","Computer Science, Theory & Methods"
"WOS:000075188700010","Discrete-time quasi-sliding mode tracking control of uncertain systems with long sampling interval","1998","
<p>This paper presents a discrete-time quasi-sliding mode controller of SIMO systems with parameter uncertainty and long sampling interval. The discrete quasi-sliding mode control system is designed to track the desired path on the basis of a Lyapunov function, and sufficient conditions of the switching gain are given to make the discrete system stable. Each component of the switching gain can be determined separately. The switching region is variable according to the product of the bounds of uncertainty and the amplitude of the trajectory. The controller is robust to the uncertainties, and the reaching condition of the control system is satisfied for any initial condition. This control law is a generalized form of the discrete quasi-sliding mode control and reduces the chattering problem considerably. Simulation results for motion control of an autonomous underwater vehicle which acquires position information with a discrete interval show the effectiveness of the proposed technique.</p>","Computer Science"
"WOS:000074105300011","Ultrasonic characterisation in determining elastic modulus of trabecular bone material","1998","
<p>The pulse transmission ultrasonic technique is used to characterise the actual pathway and the wavelength dependence in relation to the bone specimen and microstructural dimensions. The average velocity through individual trabecular bone is 2901 m s(-1) (SD 161), and the mean velocity through cylindrical cancellous bone specimens is 2717 m s(-1) (SD 171). Thus, the velocity through the cylindrical cancellous bone specimens is underestimated by as much as 6.4% of that through individual trabeculae. There is a statistically significant difference in the ultrasonic velocity between individual trabeculae and cylindrical cancellous bone specimens (p=0.0012).</p>","Computer Science, Interdisciplinary Applications"
