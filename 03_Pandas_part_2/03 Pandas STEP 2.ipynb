{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuation of previous notebook on PANDAS\n",
    "\n",
    "This is the continuation of previous notebook on PANDAS\n",
    "\n",
    "we will use now two dataframes\n",
    "pubs_df containing publications\n",
    "affs_df containing affiliations (for all itmes in pubs_df, thus also Bocconi co-authors)\n",
    "\n",
    "both loaded from csv files and with a common index \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pubs_df = pd.read_csv(\"wos_publications.csv\")\n",
    "affs_df = pd.read_csv(\"wos_affiliations.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching / selecting data data: \n",
    "\n",
    "Using a single colum to select data:\n",
    "df[df.A > val]\n",
    "\n",
    "to select a list of possible values use isin\n",
    "\n",
    "df[df.A.isin(['val1', 'val2'...])\n",
    "\n",
    "In examples below we show records where the number of addresses (addr_num) is above 10\n",
    "and where country of affiliations is ENGLAND or USA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "affs_df[affs_df.addr_num > 10].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affs_df[affs_df.country.isin(['ENGLAND','USA' ])].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawing an histogram\n",
    "\n",
    "Next step is to use hist() function to draw an histogram by country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "\n",
    "grouped=affs_df.groupby('country')\n",
    "#print(grouped['UID'].count())\n",
    "grouped['UID'].count().hist(xrot=90)\n",
    "\n",
    "plt.ylabel('Countries')\n",
    "plt.xlabel('N aff')\n",
    "plt.title('Countries Histogram')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One country has a value around 6K\n",
    "let's plot a reduced dataset (count >=100) \n",
    "to see which one is it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "g2=pd.DataFrame(grouped['UID'].count())\n",
    "\n",
    "g2=g2[g2.UID > 99]\n",
    "\n",
    "g2.plot()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge, join, append DFs\n",
    "\n",
    "In order to combine DFs several logics are available\n",
    "\n",
    "To apply easily on our dataset we build 2 subsets with a smaller numer of columns\n",
    "\n",
    "pubs_df_reduced\n",
    "affs_df_reduced\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affs_df_reduced = affs_df.loc[0:10 ,['UID', 'organization', 'country'] ]\n",
    "pubs_df_reduced = pubs_df.loc[0:5 ,['UID', 'itemtitle', 'source', 'pubyear'] ]\n",
    "\n",
    "pubs_df_reduced.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### concat\n",
    "Concatenating pandas objects together with concat():\n",
    "performs concatenation operations along an axis while performing optional set logic (union or intersection) of the indexes (if any) on the other axes.\n",
    "\n",
    "pd.concat(objs, axis=0, join='outer', join_axes=None, ignore_index=False,\n",
    "          keys=None, levels=None, names=None, verify_integrity=False,\n",
    "          copy=True)\n",
    "          \n",
    "join : {‘inner’, ‘outer’}, default ‘outer’\n",
    "\n",
    "axis : {0/’index’, 1/’columns’}, default 0\n",
    "The axis to concatenate along\n",
    "( 0 --> appends vertically , 1 orizzontally)\n",
    "\n",
    "join_axes : list of Index objects\n",
    "Specific indexes to use for the other n - 1 axes instead of performing inner/outer set logic\n",
    "          \n",
    "sort : boolean, default None\n",
    "Sort non-concatenation axis if it is not already aligned when join is ‘outer’.                   \n",
    "   \n",
    "ignore_index : boolean, default False\n",
    "If True, do not use the index values along the concatenation axis   \n",
    "   \n",
    "          \n",
    "let's just concat() two DFs:\n",
    "\n",
    "the result brings all columns, with NaN value where the column is missing in one DF          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frames = [affs_df_reduced, pubs_df_reduced]\n",
    "\n",
    "pd.concat(frames, sort=True)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append()\n",
    "\n",
    "append is a shortcut to concat() if you need to concatenate along axis=0, namely the index \n",
    "\n",
    "\n",
    "DataFrame.append(other, ignore_index=False, verify_integrity=False, sort=None)\n",
    "\n",
    "We try the same as above, adding the ignore_index=True clause\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affs_df_reduced.append(pubs_df_reduced, ignore_index = True, sort=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affs_df_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge\n",
    "\n",
    "pandas.merge(left, right, how='inner', on=None, left_on=None, right_on=None, left_index=False, right_index=False, sort=False, suffixes=('_x', '_y'), copy=True, indicator=False, validate=None)\n",
    "\n",
    "how : {‘left’, ‘right’, ‘outer’, ‘inner’}, default ‘inner’\n",
    "\n",
    "\n",
    "#### Merge method    \tSQL Join Name      \tDescription\n",
    "            left    LEFT OUTER JOIN\t    Use keys from left frame only\n",
    "            right\tRIGHT OUTER JOIN\tUse keys from right frame only\n",
    "            outer\tFULL OUTER JOIN\t    Use union of keys from both frames\n",
    "            inner\tINNER JOIN          Use intersection of keys from both frames\n",
    "\n",
    "\n",
    "on : label or list (columns in common - These must be found in both DataFrames)\n",
    "\n",
    "indicator : bool or str, default False\n",
    "\n",
    "We show now a DF merge between affs and pubs by UID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(affs_df_reduced, pubs_df_reduced, how= 'outer' , on = ['UID'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: for last record we have no affiliations since aff reduced dataframe did not include it; try rerunning adding indicator = 'indicator_column' to crosscheck;\n",
    "\n",
    "It could be also possible to set NaN to a default value with method\n",
    ".fillna('value') (if more columns use a dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 =pd.merge(affs_df_reduced, pubs_df_reduced, how= 'outer' , on = ['UID'], indicator='indicator_column').fillna({'organization': 'no org', 'country': 'no cy'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join\n",
    "\n",
    "Method .join() is more convenient when mergin on index.\n",
    "result = left.join(right, how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping DFs\n",
    "\n",
    "### Stack\n",
    "The stack() method “compresses” a level in the DataFrame’s columns.\n",
    "\n",
    "the inverse operation of stack() is unstack(), which by default unstacks the last level\n",
    "(needs multilevel indexed DFs)\n",
    "\n",
    "In the following example first 10 affiliations are taken (df1) and a count by uid/country is \n",
    "built (df2)\n",
    "eventually df2 has 2 keys since groupby has 2 levels; unstack creates a set of columns with country names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = affs_df[:10]\n",
    "df2 = df1.groupby(['UID', 'country'])['organization'].count()\n",
    "df2.unstack(level=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot tables\n",
    "\n",
    "pandas.pivot_table(data, values=None, index=None, columns=None, aggfunc='mean', fill_value=None, margins=False, dropna=True, margins_name='All')\n",
    "\n",
    "\n",
    "data : DataFrame\n",
    "values : column to aggregate, optional\n",
    "index : column, Grouper, array, or list of the previous\n",
    "If an array is passed, it must be the same length as the data. The list can contain any of the other types (except list). Keys to group by on the pivot table index. If an array is passed, it is being used as the same manner as column values.\n",
    "\n",
    "columns : column, Grouper, array, or list of the previous\n",
    "If an array is passed, it must be the same length as the data. The list can contain any of the other types (except list). Keys to group by on the pivot table column. If an array is passed, it is being used as the same manner as column values.\n",
    "\n",
    "aggfunc : function, list of functions, dict, default numpy.mean\n",
    "If list of functions passed, the resulting pivot table will have hierarchical columns whose top level are the function names (inferred from the function objects themselves) If dict is passed, the key is column to aggregate and value is function or list of functions\n",
    "\n",
    "fill_value : scalar, default None\n",
    "Value to replace missing values with\n",
    "\n",
    "margins : boolean, default False\n",
    "Add all row / columns (e.g. for subtotal / grand totals)\n",
    "\n",
    "dropna : boolean, default True\n",
    "Do not include columns whose entries are all NaN\n",
    "\n",
    "margins_name : string, default ‘All’\n",
    "Name of the row / column that will contain the totals when margins is True.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df1 = pubs_df.loc[:15,['source', 'pubyear', 'UID']]\n",
    "df2 = df1.groupby(['source', 'pubyear']).count()\n",
    "pd.pivot_table(df2, index = ['source'], columns = ['pubyear'],  fill_value=0, dropna = False )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## categorical data \n",
    "\n",
    "pandas can include categorical data in a DataFrame.\n",
    "A categorical variable takes on a limited, and usually fixed, number of possible values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df1 = pubs_df.loc[:15].copy()  #make a copy since we are going to modify\n",
    "series1 = pd.DataFrame({'raw_rank': ['A', 'B', 'B', 'C', 'A', 'B', 'B', 'C', 'A', 'B', 'B', 'C',\n",
    "                                     'A', 'B', 'B', 'C']})\n",
    "df1['grade'] = series1['raw_rank'].astype('category')\n",
    "df1['grade']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we rename categories (.cat.categories ) \n",
    "and we set the full range by adding missing categories (.cat.set_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"grade\"].cat.categories = [\"very good\", \"good\", \"bad\"]\n",
    "df1[\"grade\"] = df1[\"grade\"].cat.set_categories([\"very bad\", \"bad\", \"medium\",\n",
    "                                                 \"good\", \"very good\"])\n",
    "df1[\"grade\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data output: csv and HDF5\n",
    "\n",
    "Previously we imported data with read_csv() function\n",
    "\n",
    "Inverse function is to_csv(filename)\n",
    "\n",
    "Another option is HDFStore (Hierarchical Data Format), a dict-like object which reads and writes pandas using the high performance HDF5  format using the PyTables library (important : upgrade to PyTables >= 3.2 to avoid a bug; check with pip freeze).\n",
    "\n",
    "DataFrame.to_hdf(path_or_buf, key, **kwargs)\n",
    "\n",
    "pandas.read_hdf(path_or_buf, key=None, mode='r', **kwargs)\n",
    "\n",
    "\n",
    "path_or_buf : str or pandas.HDFStore\n",
    "File path or HDFStore object; default extension h5\n",
    "\n",
    "key : str\n",
    "Identifier for the group in the store.\n",
    "\n",
    "mode : {‘a’, ‘w’, ‘r+’}, default ‘a’\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_hdf('c:\\\\temp\\\\df_test.h5', 'df2')\n",
    "print('Dataframe succesfully exported!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using lambda function on dataframes:\n",
    "\n",
    "lambda functions are syntetic writing of functions that, thanks to apply method\n",
    "can run across all dataframe.\n",
    "A lambda function can take any number of arguments, but can only have one expression\n",
    "\n",
    "x = lambda a, b, c : a + b + c\n",
    "print(x(5, 6, 2))\n",
    "\n",
    "df.text = df.text.apply(lambda x: x.lower())\n",
    "\n",
    "in the example below we set lower case to organization field, then\n",
    "set a new column BOC containing B if affiliation contains the word 'bocconi'\n",
    "'O' else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affs_df['organization']= affs_df['organization'].apply(lambda x: x.lower())\n",
    "affs_df['BOC']= affs_df['organization'].apply(lambda x: 'O' if 'BOCCONI' not in x else 'B')\n",
    "affs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
