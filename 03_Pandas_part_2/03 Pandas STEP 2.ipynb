{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuation of previous notebook on PANDAS\n",
    "\n",
    "This is the continuation of previous notebook on PANDAS\n",
    "\n",
    "We use now two dataframes\n",
    "pubs_df containing publications\n",
    "affs_df containing affiliations (for all items in pubs_df, thus also Bocconi co-authors)\n",
    "\n",
    "both loaded from csv files and with a common index UID\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pubs_df = pd.read_csv(\"wos_publications.csv\")\n",
    "affs_df = pd.read_csv(\"wos_affiliations.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step let's have a peek to the new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affs_df.isna().sum()  #see how many nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affs_df.isna().mean() * 100 # to show % of nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affs_df.describe() # data composition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching / selecting data data: \n",
    "\n",
    "Using a single colum to select data:\n",
    "df[df.A > val]\n",
    "\n",
    "to select a list of possible values use isin\n",
    "\n",
    "df[df.A.isin(['val1', 'val2'...])\n",
    "\n",
    "We use isin() also when the value to search is a DF or list with one only value.\n",
    "\n",
    "In examples below we show records where the number of addresses (addr_num) is te max (99)\n",
    "and where country of affiliations is ENGLAND or USA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "a99 =affs_df[affs_df.addr_num >=99]\n",
    "\n",
    "affs_df[ affs_df.UID.isin( a99['UID'])].sort_values(by=['organization']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pubs_df[pubs_df.UID.isin( a99['UID'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affs_df[affs_df.country.isin(['ENGLAND','USA' ])].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawing an histogram\n",
    "\n",
    "Next step is to use hist() Pandas function to draw an histogram by country\n",
    "\n",
    "First one counts the number of countries by number of occurrences; it autosizes the steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "\n",
    "grouped=affs_df.groupby('country')\n",
    "# print(grouped['UID'].count())\n",
    "grouped['UID'].count().hist(xrot=90, bins=10)\n",
    "\n",
    "plt.ylabel('Countries')\n",
    "plt.xlabel('N aff')\n",
    "plt.title('Countries Histogram')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One country has a value around 6K\n",
    "let's plot a reduced dataset (count >=100) \n",
    "to see which one is it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "g2=pd.DataFrame(grouped['UID'].count())\n",
    "\n",
    "g2=g2[g2.UID > 99]\n",
    "\n",
    "g2.plot(rot=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge, join, append DFs\n",
    "\n",
    "In order to combine DFs several logics are available\n",
    "\n",
    "To apply easily on our dataset we build 2 subsets with a smaller numer of columns\n",
    "\n",
    "pubs_df_reduced\n",
    "affs_df_reduced\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affs_df_reduced = affs_df.sort_values(by=['UID']).loc[0:10 ,['UID', 'organization', 'country'] ]\n",
    "pubs_df_reduced = pubs_df.sort_values(by=['UID']).loc[0:5 ,['UID', 'itemtitle', 'source', 'pubyear'] ]\n",
    "\n",
    "pubs_df_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concat()\n",
    "Concatenating pandas objects together with concat():\n",
    "performs concatenation operations along an axis while performing optional set logic (union or intersection) of the indexes (if any) on the other axes.\n",
    "\n",
    "pd.concat(objs, axis=0, join='outer', join_axes=None, ignore_index=False,\n",
    "          keys=None, levels=None, names=None, verify_integrity=False,\n",
    "          copy=True)\n",
    "          \n",
    "join : {‘inner’, ‘outer’}, default ‘outer’\n",
    "\n",
    "axis : {0/’index’, 1/’columns’}, default 0\n",
    "The axis to concatenate along\n",
    "( 0 --> appends vertically , 1 orizzontally)\n",
    "\n",
    "join_axes : list of Index objects\n",
    "Specific indexes to use for the other n - 1 axes instead of performing inner/outer set logic\n",
    "          \n",
    "sort : boolean, default None\n",
    "Sort non-concatenation axis if it is not already aligned when join is ‘outer’.                   \n",
    "   \n",
    "ignore_index : boolean, default False\n",
    "If True, do not use the index values along the concatenation axis   \n",
    "   \n",
    "          \n",
    "let's just concat() two DFs:\n",
    "\n",
    "the result brings all columns, with NaN value where the column is missing in one DF        \n",
    "\n",
    "(note index column has overlapping values: to avoid add ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frames = [affs_df_reduced, pubs_df_reduced]\n",
    "\n",
    "pd.concat(frames, sort = True)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append()\n",
    "\n",
    "append is a shortcut to concat() if you need to concatenate along axis=0, namely the index \n",
    "\n",
    "\n",
    "DataFrame.append(other, ignore_index=False, verify_integrity=False, sort=None)\n",
    "\n",
    "We try the same as above, adding the ignore_index=True clause\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affs_df_reduced.append(pubs_df_reduced, ignore_index = True, sort=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge()\n",
    "\n",
    "pandas.merge(left, right, how='inner', on=None, left_on=None, right_on=None, left_index=False, right_index=False, sort=False, suffixes=('_x', '_y'), copy=True, indicator=False, validate=None)\n",
    "\n",
    "how : {‘left’, ‘right’, ‘outer’, ‘inner’}, default ‘inner’\n",
    "\n",
    "\n",
    "#### Merge method    \tSQL Join Name      \tDescription\n",
    "            left    LEFT OUTER JOIN\t    Uses keys from left frame only\n",
    "            right\tRIGHT OUTER JOIN\tUses keys from right frame only\n",
    "            outer\tFULL OUTER JOIN\t    Uses union of keys from both frames\n",
    "            inner\tINNER JOIN          Uses intersection of keys from both frames\n",
    "\n",
    "\n",
    "on : label or list (columns in common - These must be found in both DataFrames)\n",
    "\n",
    "indicator : bool or str, default False: If True, adds a column to output DataFrame called “_merge” with information on the source of each row\n",
    "\n",
    "We show now a DF merge between affs and pubs by UID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(affs_df_reduced, pubs_df_reduced, how= 'outer' , on = ['UID'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: for last record we have no affiliations since aff reduced dataframe did not include it; try rerunning adding indicator = 'indicator_column' to crosscheck;\n",
    "\n",
    "Try also using how= 'inner'\n",
    "\n",
    "It could be also possible to set NaN to a default value with method\n",
    ".fillna('value') (if more columns use a dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 =pd.merge(affs_df_reduced, pubs_df_reduced, how= 'outer' , on = ['UID'], indicator='indicator_column').fillna({'organization': 'no org', 'country': 'no cy'})\n",
    "df6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join\n",
    "\n",
    "Method .join() is more convenient when mergin on index.\n",
    "result = left.join(right, how='outer')\n",
    "\n",
    "in cases where we want to merge on another column we can use .set_index() method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pubs_df_reduced.set_index('UID').join(affs_df_reduced.set_index('UID'), how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping DFs\n",
    "\n",
    "### Stack\n",
    "The stack() method “compresses” a level in the DataFrame’s columns.\n",
    "\n",
    "the inverse operation of stack() is unstack(), which by default unstacks the last level\n",
    "(needs multilevel indexed DFs)\n",
    "\n",
    "In the following example first 10 affiliations are taken (df1) and a count by uid/country is \n",
    "built (df2)\n",
    "eventually df2 has 2 keys since groupby has 2 levels; unstack creates a set of columns with country names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = affs_df[:10]\n",
    "df2 = df1.groupby(['UID', 'country'])['organization'].count()\n",
    "df2.unstack(level=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we take 2 columns only and we reindex so we stack on the new index\n",
    "\n",
    "df2=df1[['organization','UID']][:10].set_index('organization')\n",
    "df2.stack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot tables\n",
    "\n",
    "pandas.pivot_table(data, values=None, index=None, columns=None, aggfunc='mean', fill_value=None, margins=False, dropna=True, margins_name='All')\n",
    "\n",
    "\n",
    "data : DataFrame\n",
    "\n",
    "values : column to aggregate, optional\n",
    "\n",
    "index : column, Grouper, array, or list of the previous\n",
    "\n",
    "If an array is passed, it must be the same length as the data. The list can contain any of the other types (except list). Keys to group by on the pivot table index. If an array is passed, it is being used as the same manner as column values.\n",
    "\n",
    "columns : column, Grouper, array, or list of the previous\n",
    "\n",
    "If an array is passed, it must be the same length as the data. The list can contain any of the other types (except list). Keys to group by on the pivot table column. If an array is passed, it is being used as the same manner as column values.\n",
    "\n",
    "aggfunc : function, list of functions, dict, default numpy.mean\n",
    "\n",
    "If list of functions passed, the resulting pivot table will have hierarchical columns whose top level are the function names (inferred from the function objects themselves) If dict is passed, the key is column to aggregate and value is function or list of functions\n",
    "\n",
    "fill_value : scalar, default None\n",
    "Value to replace missing values with\n",
    "\n",
    "margins : boolean, default False\n",
    "Add all row / columns (e.g. for subtotal / grand totals)\n",
    "\n",
    "dropna : boolean, default True\n",
    "Do not include columns whose entries are all NaN\n",
    "\n",
    "margins_name : string, default ‘All’\n",
    "Name of the row / column that will contain the totals when margins is True.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df1 = pubs_df.loc[:15,['source', 'pubyear', 'UID']]\n",
    "#df2 = df1.groupby(['source', 'pubyear']).count()\n",
    "#pd.pivot_table(df2, index = ['source'], columns = ['pubyear'],  fill_value=0, dropna = False )\n",
    "\n",
    "pd.pivot_table(df1, index = ['source'], columns = ['pubyear'],  fill_value=0, dropna = False, aggfunc='count' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last step:\n",
    "\n",
    "We create a pivot table with the range of collaboration years by insitution and type of publication.\n",
    "\n",
    "First we create a joined DF, then we slice it and eventually make a pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "dfjoined= pd.merge(affs_df, pubs_df, how= 'inner' , on = ['UID'])\n",
    "dfredux = dfjoined[['organization','pubyear', 'pubtype']].copy()\n",
    "\n",
    "pd.pivot_table(dfredux, index = ['organization'], columns = ['pubtype'],\n",
    "               values = ['pubyear'],fill_value=0, dropna = False,\n",
    "               aggfunc={'min','max'})[0:15]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical data \n",
    "\n",
    "pandas can include categorical data in a DataFrame.\n",
    "A categorical variable takes on a limited, and usually fixed, number of possible values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df1 = pubs_df.loc[:15].copy()  #make a copy since we are going to modify\n",
    "series1 = pd.DataFrame({'raw_rank': ['A', 'B', 'B', 'C', 'A', 'B', 'B', 'C', 'A', 'B', 'B', 'C',\n",
    "                                     'A', 'B', 'B', 'C']})\n",
    "df1['grade'] = series1['raw_rank'].astype('category')\n",
    "df1['grade']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we rename categories (.cat.categories ) \n",
    "and we set the full range by adding missing categories (.cat.set_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"grade\"].cat.categories = [\"very good\", \"good\", \"bad\"]\n",
    "df1[\"grade\"] = df1[\"grade\"].cat.set_categories([\"very bad\", \"bad\", \"medium\",\n",
    "                                                 \"good\", \"very good\"])\n",
    "df1[\"grade\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data output: csv and HDF5\n",
    "\n",
    "Previously we imported data with read_csv() function\n",
    "\n",
    "Inverse function is to_csv(filename)\n",
    "\n",
    "Another option is HDFStore (Hierarchical Data Format), a dict-like object which reads and writes pandas using the high performance HDF5  format using the PyTables library (important : upgrade to PyTables >= 3.2 to avoid a bug; check with pip freeze).\n",
    "\n",
    "DataFrame.to_hdf(path_or_buf, key, **kwargs)\n",
    "\n",
    "pandas.read_hdf(path_or_buf, key=None, mode='r', **kwargs)\n",
    "\n",
    "\n",
    "path_or_buf : str or pandas.HDFStore\n",
    "File path or HDFStore object; default extension h5\n",
    "\n",
    "key : str\n",
    "Identifier for the group in the store.\n",
    "\n",
    "mode : {‘a’, ‘w’, ‘r+’}, default ‘a’\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_hdf('c:\\\\temp\\\\df_test.h5', 'df2')\n",
    "print('Dataframe succesfully exported!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using lambda function on dataframes:\n",
    "\n",
    "lambda functions are syntetic writing of functions that, thanks to apply method\n",
    "can run across all dataframe.\n",
    "A lambda function can take any number of arguments, but can only have one expression\n",
    "\n",
    "x = lambda a, b, c : a + b + c\n",
    "print(x(5, 6, 2))\n",
    "\n",
    "df.text = df.text.apply(lambda x: x.lower())\n",
    "\n",
    "in the example below we set lower case to organization field, then\n",
    "set a new column BOC containing B if affiliation contains the word 'bocconi'\n",
    "'O' else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affs_df['organization']= affs_df['organization'].apply(lambda x: x.lower())\n",
    "affs_df['BOC']= affs_df['organization'].apply(lambda x: 'O' if 'bocconi' not in x else 'B')\n",
    "affs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
